{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d9c0eb",
   "metadata": {},
   "source": [
    "# Document Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a8d92e",
   "metadata": {},
   "source": [
    "### 1. WebBase Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06cb2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/integrations/document_loaders/', 'title': 'Document loaders | ü¶úÔ∏èüîó LangChain', 'description': 'DocumentLoaders load data into the standard LangChain Document format.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nDocument loaders | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchProvidersAnthropicAWSGoogleHugging FaceMicrosoftOpenAIMoreProvidersAbsoAcreomActiveloop Deep LakeADS4GPTsAerospikeAgentQLAI21 LabsAimAINetworkAirbyteAirtableAlchemyAleph AlphaAlibaba CloudAnalyticDBAnnoyAnthropicAnyscaleApache Software FoundationApache DorisApifyAppleArangoDBArceeArcGISArgillaArizeArthurArxivAscendAskNewsAssemblyAIAstra DBAtlasAwaDBAWSAZLyricsAzure AIBAAIBagelBagelDBBaichuanBaiduBananaBasetenBeamBeautiful SoupBibTeXBiliBiliBittensorBlackboardbookend.aiBoxBrave SearchBreebs (Open Knowledge)Bright DataBrowserbaseBrowserlessByteDanceCassandraCerebrasCerebriumAIChaindeskChromaClarifaiClearMLClickHouseClickUpCloudflareClovaCnosDBCogneeCogniSwitchCohereCollege ConfidentialCometConfident AIConfluenceConneryContextContextual AICouchbaseCozeCrateDBC TransformersCTranslate2CubeDappierDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODataheraldDedocDeepInfraDeeplakeDeepSeekDeepSparseDellDiffbotDingoDBDiscordDiscord (community loader)DocArrayDoclingDoctranDocugamiDocusaurusDriaDropboxDuckDBDuckDuckGo SearchE2BEden AIElasticsearchElevenLabsEmbedchainEpsillaEtherscanEverly AIEverNoteExaFacebook - MetaFalkorDBFaunaFeatherless AIFiddlerFigmaFireCrawlFireworks AIFlyteFMP Data (Financial Data Prep)Forefront AIFriendli AISmabblerGelGeopandasGitGitBookGitHubGitLabGOATGoldenGoodfireGoogleSerper - Google Search APIGooseAIGPT4AllGradientGraph RAGGraphsignalGrobidGroqGutenbergHacker NewsHazy ResearchHeliconeHologresHTML to textHuaweiHugging FaceHyperbrowserIBMIEIT SystemsiFixitiFlytekIMSDbInfinispan VSInfinityInfinoIntelIuguJaguarJavelin AI GatewayJenkinsJina AIJohnsnowlabsJoplinKDB.AIKineticaKoboldAIKonkoKoNLPYK√πzuLabel StudiolakeFSLanceDBLangChain Decorators ‚ú®LangFair: Use-Case Level LLM Bias and Fairness AssessmentsLangfuse ü™¢LanternLindormLinkupLiteLLMLlamaIndexLlama.cppLlamaEdgellamafileLLMonitorLocalAILog10MariaDBMariTalkMarqoMediaWikiDumpMeilisearchMemcachedMemgraphMetalMicrosoftMilvusMindsDBMinimaxMistralAIMLflow AI Gateway for LLMsMLflowMLXModalModelScopeModern TreasuryMomentoMongoDBMongoDB AtlasMotherduckMot√∂rheadMyScaleNAVERNebiusNeo4jNetmindNimbleNLPCloudNomicNotion DBNucliaNVIDIAObsidianOceanBaseOracle Cloud Infrastructure (OCI)OctoAIOllamaOntotext GraphDBOpenAIOpenGradientOpenLLMOpenSearchOpenWeatherMapOracleAI Vector SearchOutlineOutlinesOxylabsPandasPaymanAIPebbloPermitPerplexityPetalsPostgres EmbeddingPGVectorPineconePipelineAIPipeshiftPortkeyPredibasePrediction GuardPremAISWI-PrologPromptLayerPsychicPubMedPullMd LoaderPygmalionAIPyMuPDF4LLMQdrantRAGatouillerank_bm25Ray ServeRebuffRedditRedisRemembrallReplicateRoamSema4 (fka Robocorp)RocksetRunhouseRunpodRWKV-4SalesforceSambaNovaSAPScrapeGraph AISearchApiSearxNG Search APISemaDBSerpAPIShale ProtocolSingleStore Integrationscikit-learnSlackSnowflakespaCySparkSparkLLMSpreedlySQLiteStack ExchangeStarRocksStochasticAIStreamlitStripeSupabase (Postgres)SurrealDBNebulaTableauTaigaTairTavilyTelegramTencentTensorFlow DatasetsTiDBTigerGraphTigrisTiloresTogether AI2MarkdownTranswarpTrelloTrubricsTruLensTwitterTypesenseUnstructuredUpstageupstashUpTrainUSearchValtheraValyu Deep SearchVDMSVearchVectaraVectorizeVespavliteVoyageAIWeights & BiasesWeights & Biases tracingWeights & Biases trackingWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriter, Inc.xAIXataXorbits Inference (Xinference)YahooYandexYDBYeager.aiYellowbrick01.AIYouYouTubeZepZhipu AIZillizZoteroComponentsChat modelsChat modelsAbsoAI21 LabsAlibaba Cloud PAI EASAnthropic[Deprecated] Experimental Anthropic Tools WrapperAnyscaleAzureAIChatCompletionsModelAzure OpenAIAzure ML EndpointBaichuan ChatBaidu QianfanAWS BedrockCerebrasCloudflareWorkersAICohereContextualAICoze ChatDappier AIDatabricksDeepInfraDeepSeekEden AIErnie Bot ChatEverlyAIFeatherless AIFireworksChatFriendliGoodfireGoogle GeminiGoogle Cloud Vertex AIGPTRouterGroqChatHuggingFaceIBM watsonx.aiJinaChatKineticaKonkoLiteLLMLlama 2 ChatLlama APILlamaEdgeLlama.cppmaritalkMiniMaxMistralAIMLXModelScopeMoonshotNaverNebiusNetmindNVIDIA AI EndpointsChatOCIModelDeploymentOCIGenAIChatOctoAIOllamaOpenAIOutlinesPerplexityPipeshiftChatPredictionGuardPremAIPromptLayer ChatOpenAIQwen QwQRekaRunPod Chat ModelSambaNovaCloudSambaStudioChatSeekrFlowSnowflake CortexsolarSparkLLM ChatNebula (Symbl.ai)Tencent HunyuanTogetherTongyi QwenUpstagevectaravLLM ChatVolc Engine MaasChat WriterxAIXinferenceYandexGPTChatYIYuan2.0ZHIPU AIRetrieversRetrieversActiveloop Deep MemoryAmazon KendraArceeArxivAskNewsAzure AI SearchBedrock (Knowledge Bases)BM25BoxBREEBS (Open Knowledge)ChaindeskChatGPT pluginCogneeCohere rerankerCohere RAGContextual AI RerankerDappierDocArrayDriaElasticSearch BM25ElasticsearchEmbedchainFlashRank rerankerFleet AI ContextGalaxiaGoogle DriveGoogle Vertex AI SearchGraph RAGIBM watsonx.aiJaguarDB Vector DatabaseKay.aiKinetica Vectorstore based RetrieverkNNLinkupSearchRetrieverLLMLingua Document CompressorLOTR (Merger Retriever)MetalNanoPQ (Product Quantization)NebiusneedleNimbleOutlinePermitPinecone Hybrid SearchPinecone RerankPubMedQdrant Sparse VectorRAGatouilleRePhraseQueryRememberizerSEC filingSelf-querying retrieversSVMTavilySearchAPITF-IDF**NeuralDB**ValyuContextVectorizeVespaWikipediaYou.comZep CloudZep Open SourceZilliz Cloud PipelineZoteroTools/ToolkitsToolsADS4GPTsAgentQLAINetwork ToolkitAlpha VantageAmadeus ToolkitApify ActorArXivAskNewsAWS LambdaAzure AI Services ToolkitAzure Cognitive Services ToolkitAzure Container Apps dynamic sessionsShell (bash)Bearly Code InterpreterBing SearchBrave SearchBrightDataWebScraperAPIBrightDataSERPBrightDataUnlockerCassandra Database ToolkitCDPChatGPT PluginsClickUp ToolkitCogniswitch ToolkitCompass DeFi ToolkitConnery Toolkit and ToolsDall-E Image GeneratorDappierDatabricks Unity Catalog (UC)DataForSEODataheraldDuckDuckGo SearchDiscordE2B Data AnalysisEden AIElevenLabs Text2SpeechExa SearchFile SystemFinancialDatasets ToolkitFMP DataGithub ToolkitGitlab ToolkitGmail ToolkitGOATGolden QueryGoogle BooksGoogle Calendar ToolkitGoogle Cloud Text-to-SpeechGoogle DriveGoogle FinanceGoogle ImagenGoogle JobsGoogle LensGoogle PlacesGoogle ScholarGoogle SearchGoogle SerperGoogle TrendsGradioGraphQLHuggingFace Hub ToolsHuman as a toolHyperbrowser Browser Agent ToolsHyperbrowser Web Scraping ToolsIBM watsonx.aiIFTTT WebHooksInfobipIonic Shopping ToolJenkinsJina SearchJira ToolkitJSON ToolkitLemon AgentLinkupSearchToolMemgraphMemorizeMojeek SearchMultiOn ToolkitNASA ToolkitNaver SearchNuclia UnderstandingNVIDIA Riva: ASR and TTSOffice365 ToolkitOpenAPI ToolkitNatural Language API ToolkitsOpenGradientOpenWeatherMapOracle AI Vector Search: Generate SummaryOxylabsPandas DataframePassio NutritionAIPaymanAIPermitPlayWright Browser ToolkitPolygon IO Toolkit and ToolsPowerBI ToolkitPrologPubMedPython REPLReddit SearchRequests ToolkitRiza Code InterpreterRobocorp ToolkitSalesforceSceneXplainScrapeGraphSearchApiSearxNG SearchSemantic Scholar API ToolSerpAPISlack ToolkitSpark SQL ToolkitSQLDatabase ToolkitStackExchangeSteam ToolkitStripeTableauTaigaTavily ExtractTavily SearchTiloresTwilioUpstageValtheraValyuContextVectaraWikidataWikipediaWolfram AlphaWriter ToolsYahoo Finance NewsYou.com SearchYouTubeZapier Natural Language ActionsZenGuard AIDocument loadersDocument loadersacreomAgentQLLoaderAirbyteLoaderAirbyte CDK (Deprecated)Airbyte Gong (Deprecated)Airbyte Hubspot (Deprecated)Airbyte JSON (Deprecated)Airbyte Salesforce (Deprecated)Airbyte Shopify (Deprecated)Airbyte Stripe (Deprecated)Airbyte Typeform (Deprecated)Airbyte Zendesk Support (Deprecated)AirtableAlibaba Cloud MaxComputeAmazon TextractApify DatasetArcGISArxivLoaderAssemblyAI Audio TranscriptsAstraDBAsync ChromiumAsyncHtmlAthenaAWS S3 DirectoryAWS S3 FileAZLyricsAzure AI DataAzure Blob Storage ContainerAzure Blob Storage FileAzure AI Document IntelligenceBibTeXBiliBiliBlackboardBlockchainBoxBrave SearchBrowserbaseBrowserlessBSHTMLLoaderCassandraChatGPT DataCollege ConfidentialConcurrent LoaderConfluenceCoNLL-UCopy PasteCouchbaseCSVCube Semantic LayerDatadog LogsDedocDiffbotDiscordDoclingDocugamiDocusaurusDropboxDuckDBEmailEPubEtherscanEverNoteexample_dataFacebook ChatFaunaFigmaFireCrawlGeopandasGitGitBookGitHubGlue CatalogGoogle AlloyDB for PostgreSQLGoogle BigQueryGoogle BigtableGoogle Cloud SQL for SQL serverGoogle Cloud SQL for MySQLGoogle Cloud SQL for PostgreSQLGoogle Cloud Storage DirectoryGoogle Cloud Storage FileGoogle Firestore in Datastore ModeGoogle DriveGoogle El Carro for Oracle WorkloadsGoogle Firestore (Native Mode)Google Memorystore for RedisGoogle SpannerGoogle Speech-to-Text Audio TranscriptsGrobidGutenbergHacker NewsHuawei OBS DirectoryHuawei OBS FileHuggingFace datasetHyperbrowserLoaderiFixitImagesImage captionsIMSDbIuguJoplinJSONLoaderJupyter NotebookKineticalakeFSLangSmithLarkSuite (FeiShu)LLM SherpaMastodonMathPixPDFLoaderMediaWiki DumpMerge Documents LoadermhtmlMicrosoft ExcelMicrosoft OneDriveMicrosoft OneNoteMicrosoft PowerPointMicrosoft SharePointMicrosoft WordNear BlockchainModern TreasuryMongoDBNeedle Document LoaderNews URLNotion DB 2/2NucliaObsidianOpen Document Format (ODT)Open City DataOracle Autonomous DatabaseOracle AI Vector Search: Document ProcessingOrg-modeOutline Document LoaderPandas DataFrameparsersPDFMinerLoaderPDFPlumberPebblo Safe DocumentLoaderPolars DataFrameDell PowerScale Document LoaderPsychicPubMedPullMdLoaderPyMuPDFLoaderPyMuPDF4LLMPyPDFDirectoryLoaderPyPDFium2LoaderPyPDFLoaderPySparkQuipReadTheDocs DocumentationRecursive URLRedditRoamRocksetrspaceRSS FeedsRSTscrapflyScrapingAntSingleStoreSitemapSlackSnowflakeSource CodeSpiderSpreedlyStripeSubtitleSurrealDBTelegramTencent COS DirectoryTencent COS FileTensorFlow DatasetsTiDB2MarkdownTOMLTrelloTSVTwitterUnstructuredUnstructuredMarkdownLoaderUnstructuredPDFLoaderUpstageURLVsdxWeatherWebBaseLoaderWhatsApp ChatWikipediaUnstructuredXMLLoaderXorbits Pandas DataFrameYouTube audioYouTube transcriptsYoutubeLoaderDLYuqueZeroxPDFLoaderVector storesVector storesActiveloop Deep LakeAerospikeAlibaba Cloud OpenSearchAnalyticDBAnnoyApache DorisApertureDBAstra DB Vector StoreAtlasAwaDBAzure Cosmos DB Mongo vCoreAzure Cosmos DB No SQLAzure AI SearchBagelBagelDBBaidu Cloud ElasticSearch VectorSearchBaidu VectorDBApache CassandraChromaClarifaiClickHouseCloudflareVectorizeCouchbaseDashVectorDatabricksIBM Db2 Vector Store and Vector SearchDingoDBDocArray HnswSearchDocArray InMemorySearchAmazon Document DBDuckDBChina Mobile ECloud ElasticSearch VectorSearchElasticsearchEpsillaFaissFaiss (Async)FalkorDBVectorStoreGelGoogle AlloyDB for PostgreSQLGoogle BigQuery Vector SearchGoogle Cloud SQL for MySQLGoogle Cloud SQL for PostgreSQLFirestoreGoogle Memorystore for RedisGoogle SpannerGoogle Vertex AI Feature StoreGoogle Vertex AI Vector SearchHippoHologresInfinispanJaguar Vector DatabaseKDB.AIKineticaLanceDBLanternLindormLLMRailsManticoreSearch VectorStoreMariaDBMarqoMeilisearchAmazon MemoryDBMilvusMomento Vector Index (MVI)MongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOceanbaseopenGaussOpenSearchOracle AI Vector Search: Vector StorePathwayPostgres EmbeddingPGVecto.rsPGVectorPineconePinecone (sparse)QdrantRedisRelytRocksetSAP HANA Cloud Vector EngineScaNNSemaDBSingleStorescikit-learnSQLiteVecSQLite-VSSSQLServerStarRocksSupabase (Postgres)SurrealDBVectorStoreTablestoreTairTencent Cloud VectorDBThirdAI NeuralDBTiDB VectorTigrisTileDBTimescale Vector (Postgres)TypesenseUpstash VectorUSearchValdVDMSVearchVectaraVespaviking DBvliteWeaviateXataYDBYellowbrickZepZep CloudZillizEmbedding modelsEmbedding modelsAI21Aleph AlphaAnyscaleascendAwaDBAzureOpenAIBaichuan Text EmbeddingsBaidu QianfanBedrockBGE on Hugging FaceBookend AIClarifaiCloudflare Workers AIClova EmbeddingsCohereDashScopeDatabricksDeepInfraEDEN AIElasticsearchEmbaasERNIEFake EmbeddingsFastEmbed by QdrantFireworksGoogle GeminiGoogle Vertex AIGPT4AllGradientHugging FaceIBM watsonx.aiInfinityInstruct Embeddings on Hugging FaceIPEX-LLM: Local BGE Embeddings on Intel CPUIPEX-LLM: Local BGE Embeddings on Intel GPUIntel¬Æ Extension for Transformers Quantized Text EmbeddingsJinaJohn Snow LabsLASER Language-Agnostic SEntence Representations Embeddings by Meta AILindormLlama.cppllamafileLLMRailsLocalAIMiniMaxMistralAImodel2vecModelScopeMosaicMLNaverNebiusNetmindNLP CloudNomicNVIDIA NIMsOracle Cloud Infrastructure Generative AIOllamaOpenClipOpenAIOpenVINOEmbedding Documents using Optimized and Quantized EmbeddersOracle AI Vector Search: Generate EmbeddingsOVHcloudPinecone EmbeddingsPredictionGuardEmbeddingsPremAISageMakerSambaNovaCloudSambaStudioSelf HostedSentence Transformers on Hugging FaceSolarSpaCySparkLLM Text EmbeddingsTensorFlow HubText Embeddings InferenceTextEmbed - Embedding Inference ServerTitan TakeoffTogether AIUpstageVolc EngineVoyage AIXorbits inference (Xinference)YandexGPTZhipuAIOtherComponentsDocument loadersOn this pageDocument loaders\\n\\nDocumentLoaders load data into the standard LangChain Document format.\\nEach DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the .load method.\\nAn example use case is as follows:\\nfrom langchain_community.document_loaders.csv_loader import CSVLoaderloader = CSVLoader(    ...  # <-- Integration specific parameters here)data = loader.load()API Reference:CSVLoader\\nWebpages\\u200b\\nThe below document loaders allow you to load webpages.\\nSee this guide for a starting point: How to: load web pages.\\nDocument LoaderDescriptionPackage/APIWebUses urllib and BeautifulSoup to load and parse HTML web pagesPackageUnstructuredUses Unstructured to load and parse web pagesPackageRecursiveURLRecursively scrapes all child links from a root URLPackageSitemapScrapes all pages on a given sitemapPackageFirecrawlAPI service that can be deployed locally, hosted version has free credits.APIDoclingUses Docling to load and parse web pagesPackageHyperbrowserPlatform for running and scaling headless browsers, can be used to scrape/crawl any siteAPIAgentQLWeb interaction and structured data extraction from any web page using an AgentQL query or a Natural Language promptAPI\\nPDFs\\u200b\\nThe below document loaders allow you to load PDF documents.\\nSee this guide for a starting point: How to: load PDF files.\\nDocument LoaderDescriptionPackage/APIPyPDFUses `pypdf` to load and parse PDFsPackageUnstructuredUses Unstructured\\'s open source library to load PDFsPackageAmazon TextractUses AWS API to load PDFsAPIMathPixUses MathPix to load PDFsPackagePDFPlumberLoad PDF files using PDFPlumberPackagePyPDFDirectryLoad a directory with PDF filesPackagePyPDFium2Load PDF files using PyPDFium2PackagePyMuPDFLoad PDF files using PyMuPDFPackagePyMuPDF4LLMLoad PDF content to Markdown using PyMuPDF4LLMPackagePDFMinerLoad PDF files using PDFMinerPackageUpstage Document Parse LoaderLoad PDF files using UpstageDocumentParseLoaderPackageDoclingLoad PDF files using DoclingPackage\\nCloud Providers\\u200b\\nThe below document loaders allow you to load documents from your favorite cloud providers.\\nDocument LoaderDescriptionPartner PackageAPI referenceAWS S3 DirectoryLoad documents from an AWS S3 directory‚ùåS3DirectoryLoaderAWS S3 FileLoad documents from an AWS S3 file‚ùåS3FileLoaderAzure AI DataLoad documents from Azure AI services‚ùåAzureAIDataLoaderAzure Blob Storage ContainerLoad documents from an Azure Blob Storage container‚ùåAzureBlobStorageContainerLoaderAzure Blob Storage FileLoad documents from an Azure Blob Storage file‚ùåAzureBlobStorageFileLoaderDropboxLoad documents from Dropbox‚ùåDropboxLoaderGoogle Cloud Storage DirectoryLoad documents from GCS bucket‚úÖGCSDirectoryLoaderGoogle Cloud Storage FileLoad documents from GCS file object‚úÖGCSFileLoaderGoogle DriveLoad documents from Google Drive (Google Docs only)‚úÖGoogleDriveLoaderHuawei OBS DirectoryLoad documents from Huawei Object Storage Service Directory‚ùåOBSDirectoryLoaderHuawei OBS FileLoad documents from Huawei Object Storage Service File‚ùåOBSFileLoaderMicrosoft OneDriveLoad documents from Microsoft OneDrive‚ùåOneDriveLoaderMicrosoft SharePointLoad documents from Microsoft SharePoint‚ùåSharePointLoaderTencent COS DirectoryLoad documents from Tencent Cloud Object Storage Directory‚ùåTencentCOSDirectoryLoaderTencent COS FileLoad documents from Tencent Cloud Object Storage File‚ùåTencentCOSFileLoader\\nSocial Platforms\\u200b\\nThe below document loaders allow you to load documents from different social media platforms.\\nDocument LoaderAPI referenceTwitterTwitterTweetLoaderRedditRedditPostsLoader\\nMessaging Services\\u200b\\nThe below document loaders allow you to load data from different messaging platforms.\\nDocument LoaderAPI referenceTelegramTelegramChatFileLoaderWhatsAppWhatsAppChatLoaderDiscordDiscordChatLoaderFacebook ChatFacebookChatLoaderMastodonMastodonTootsLoader\\nProductivity tools\\u200b\\nThe below document loaders allow you to load data from commonly used productivity tools.\\nDocument LoaderAPI referenceFigmaFigmaFileLoaderNotionNotionDirectoryLoaderSlackSlackDirectoryLoaderQuipQuipLoaderTrelloTrelloLoaderRoamRoamLoaderGitHubGithubFileLoader\\nCommon File Types\\u200b\\nThe below document loaders allow you to load data from common data formats.\\nDocument LoaderData TypeCSVLoaderCSV filesDirectoryLoaderAll files in a given directoryUnstructuredMany file types (see https://docs.unstructured.io/platform/supported-file-types)JSONLoaderJSON filesBSHTMLLoaderHTML filesDoclingLoaderVarious file types (see https://ds4sd.github.io/docling/)\\nAll document loaders\\u200b\\nNameDescriptionacreomacreom is a dev-first knowledge base with tasks running on local mark...AgentQLLoaderAgentQL\\'s document loader provides structured data extraction from an...AirbyteLoaderAirbyte is a data integration platform for ELT pipelines from APIs, d...Airtable* Get your API key here.Alibaba Cloud MaxComputeAlibaba Cloud MaxCompute (previously known as ODPS) is a general purp...Amazon TextractAmazon Textract is a machine learning (ML) service that automatically...Apify DatasetApify Dataset is a scalable append-only storage with sequential acces...ArcGISThis notebook demonstrates the use of the langchaincommunity.document...ArxivLoaderarXiv is an open-access archive for 2 million scholarly articles in t...AssemblyAI Audio TranscriptsThe AssemblyAIAudioTranscriptLoader allows to transcribe audio files ...AstraDBDataStax Astra DB is a serverlessAsync ChromiumChromium is one of the browsers supported by Playwright, a library us...AsyncHtmlAsyncHtmlLoader loads raw HTML from a list of URLs concurrently.AthenaAmazon Athena is a serverless, interactive analytics service builtAWS S3 DirectoryAmazon Simple Storage Service (Amazon S3) is an object storage serviceAWS S3 FileAmazon Simple Storage Service (Amazon S3) is an object storage servic...AZLyricsAZLyrics is a large, legal, every day growing collection of lyrics.Azure AI DataAzure AI Studio provides the capability to upload data assets to clou...Azure Blob Storage ContainerAzure Blob Storage is Microsoft\\'s object storage solution for the clo...Azure Blob Storage FileAzure Files offers fully managed file shares in the cloud that are ac...Azure AI Document IntelligenceAzure AI Document Intelligence (formerly known as Azure Form Recogniz...BibTeXBibTeX is a file format and reference management system commonly used...BiliBiliBilibili is one of the most beloved long-form video sites in China.BlackboardBlackboard Learn (previously the Blackboard Learning Management Syste...BlockchainThe intention of this notebook is to provide a means of testing funct...BoxThe langchain-box package provides two methods to index your files fr...Brave SearchBrave Search is a search engine developed by Brave Software.BrowserbaseBrowserbase is a developer platform to reliably run, manage, and moni...BrowserlessBrowserless is a service that allows you to run headless Chrome insta...BSHTMLLoaderThis notebook provides a quick overview for getting started with Beau...CassandraCassandra is a NoSQL, row-oriented, highly scalable and highly availa...ChatGPT DataChatGPT is an artificial intelligence (AI) chatbot developed by OpenA...College ConfidentialCollege Confidential gives information on 3,800+ colleges and univers...Concurrent LoaderWorks just like the GenericLoader but concurrently for those who choo...ConfluenceConfluence is a wiki collaboration platform designed to save and orga...CoNLL-UCoNLL-U is revised version of the CoNLL-X format. Annotations are enc...Copy PasteThis notebook covers how to load a document object from something you...CouchbaseCouchbase is an award-winning distributed NoSQL cloud database that d...CSVA comma-separated values (CSV) file is a delimited text file that use...Cube Semantic LayerThis notebook demonstrates the process of retrieving Cube\\'s data mode...Datadog LogsDatadog is a monitoring and analytics platform for cloud-scale applic...DedocThis sample demonstrates the use of Dedoc in combination with LangCha...DiffbotDiffbot is a suite of ML-based products that make it easy to structur...DiscordDiscord is a VoIP and instant messaging social platform. Users have t...DoclingDocling parses PDF, DOCX, PPTX, HTML, and other formats into a rich u...DocugamiThis notebook covers how to load documents from Docugami. It provides...DocusaurusDocusaurus is a static-site generator which provides out-of-the-box d...DropboxDropbox is a file hosting service that brings everything-traditional ...DuckDBDuckDB is an in-process SQL OLAP database management system.EmailThis notebook shows how to load email (.eml) or Microsoft Outlook (.m...EPubEPUB is an e-book file format that uses the \".epub\" file extension. T...EtherscanEtherscan  is the leading blockchain explorer, search, API and analyt...EverNoteEverNote is intended for archiving and creating notes in which photos...example_dataFacebook ChatMessenger) is an American proprietary instant messaging app and platf...FaunaFauna is a Document Database.FigmaFigma is a collaborative web application for interface design.FireCrawlFireCrawl crawls and convert any website into LLM-ready data. It craw...GeopandasGeopandas is an open-source project to make working with geospatial d...GitGit is a distributed version control system that tracks changes in an...GitBookGitBook is a modern documentation platform where teams can document e...GitHubThis notebooks shows how you can load issues and pull requests (PRs) ...Glue CatalogThe AWS Glue Data Catalog is a centralized metadata repository that a...Google AlloyDB for PostgreSQLAlloyDB is a fully managed relational database service that offers hi...Google BigQueryGoogle BigQuery is a serverless and cost-effective enterprise data wa...Google BigtableBigtable is a key-value and wide-column store, ideal for fast access ...Google Cloud SQL for SQL serverCloud SQL is a fully managed relational database service that offers ...Google Cloud SQL for MySQLCloud SQL is a fully managed relational database service that offers ...Google Cloud SQL for PostgreSQLCloud SQL for PostgreSQL is a fully-managed database service that hel...Google Cloud Storage DirectoryGoogle Cloud Storage is a managed service for storing unstructured da...Google Cloud Storage FileGoogle Cloud Storage is a managed service for storing unstructured da...Google Firestore in Datastore ModeFirestore in Datastore Mode is a NoSQL document database built for au...Google DriveGoogle Drive is a file storage and synchronization service developed ...Google El Carro for Oracle WorkloadsGoogle El Carro Oracle OperatorGoogle Firestore (Native Mode)Firestore is a serverless document-oriented database that scales to m...Google Memorystore for RedisGoogle Memorystore for Redis is a fully-managed service that is power...Google SpannerSpanner is a highly scalable database that combines unlimited scalabi...Google Speech-to-Text Audio TranscriptsThe SpeechToTextLoader allows to transcribe audio files with the Goog...GrobidGROBID is a machine learning library for extracting, parsing, and re-...GutenbergProject Gutenberg is an online library of free eBooks.Hacker NewsHacker News (sometimes abbreviated as HN) is a social news website fo...Huawei OBS DirectoryThe following code demonstrates how to load objects from the Huawei O...Huawei OBS FileThe following code demonstrates how to load an object from the Huawei...HuggingFace datasetThe Hugging Face Hub is home to over 5,000 datasets in more than 100 ...HyperbrowserLoaderHyperbrowser is a platform for running and scaling headless browsers....iFixitiFixit is the largest, open repair community on the web. The site con...ImagesThis covers how to load images into a document format that we can use...Image captionsBy default, the loader utilizes the pre-trained Salesforce BLIP image...IMSDbIMSDb is the Internet Movie Script Database.IuguIugu is a Brazilian services and software as a service (SaaS) company...JoplinJoplin is an open-source note-taking app. Capture your thoughts and s...JSONLoaderThis notebook provides a quick overview for getting started with JSON...Jupyter NotebookJupyter Notebook (formerly IPython Notebook) is a web-based interacti...KineticaThis notebooks goes over how to load documents from KineticalakeFSlakeFS provides scalable version control over the data lake, and uses...LangSmithThis notebook provides a quick overview for getting started with the ...LarkSuite (FeiShu)LarkSuite is an enterprise collaboration platform developed by ByteDa...LLM SherpaThis notebook covers how to use LLM Sherpa to load files of many type...MastodonMastodon is a federated social media and social networking service.MathPixPDFLoaderInspired by Daniel Gross\\'s snippet here//gist.github.com/danielgross/...MediaWiki DumpMediaWiki XML Dumps contain the content of a wiki (wiki pages with al...Merge Documents LoaderMerge the documents returned from a set of specified data loaders.mhtmlMHTML is a is used both for emails but also for archived webpages. MH...Microsoft ExcelThe UnstructuredExcelLoader is used to load Microsoft Excel files. Th...Microsoft OneDriveMicrosoft OneDrive (formerly SkyDrive) is a file hosting service oper...Microsoft OneNoteThis notebook covers how to load documents from OneNote.Microsoft PowerPointMicrosoft PowerPoint is a presentation program by Microsoft.Microsoft SharePointMicrosoft SharePoint is a website-based collaboration system that use...Microsoft WordMicrosoft Word is a word processor developed by Microsoft.Near BlockchainThe intention of this notebook is to provide a means of testing funct...Modern TreasuryModern Treasury simplifies complex payment operations. It is a unifie...MongoDBMongoDB is a NoSQL , document-oriented database that supports JSON-li...Needle Document LoaderNeedle makes it easy to create your RAG pipelines with minimal effort.News URLThis covers how to load HTML news articles from a list of URLs into a...Notion DB 2/2Notion is a collaboration platform with modified Markdown support tha...NucliaNuclia automatically indexes your unstructured data from any internal...ObsidianObsidian is a powerful and extensible knowledge baseOpen Document Format (ODT)The Open Document Format for Office Applications (ODF), also known as...Open City DataSocrata provides an API for city open data.Oracle Autonomous DatabaseOracle autonomous database is a cloud database that uses machine lear...Oracle AI Vector Search: Document ProcessingOracle AI Vector Search is designed for Artificial Intelligence (AI) ...Org-modeA Org Mode document is a document editing, formatting, and organizing...Outline Document LoaderOutline is an open-source collaborative knowledge base platform desig...Pandas DataFrameThis notebook goes over how to load data from a pandas DataFrame.parsersPDFMinerLoaderThis notebook provides a quick overview for getting started with PDFM...PDFPlumberLike PyMuPDF, the output Documents contain detailed metadata about th...Pebblo Safe DocumentLoaderPebblo enables developers to safely load data and promote their Gen A...Polars DataFrameThis notebook goes over how to load data from a polars DataFrame.Dell PowerScale Document LoaderDell PowerScale is an enterprise scale out storage system that hosts ...PsychicThis notebook covers how to load documents from Psychic. See here for...PubMedPubMed¬Æ by The National Center for Biotechnology Information, Nationa...PullMdLoaderLoader for converting URLs into Markdown using the pull.md service.PyMuPDFLoaderThis notebook provides a quick overview for getting started with PyMu...PyMuPDF4LLMThis notebook provides a quick overview for getting started with PyMu...PyPDFDirectoryLoaderThis loader loads all PDF files from a specific directory.PyPDFium2LoaderThis notebook provides a quick overview for getting started with PyPD...PyPDFLoaderThis notebook provides a quick overview for getting started with PyPD...PySparkThis notebook goes over how to load data from a PySpark DataFrame.QuipQuip is a collaborative productivity software suite for mobile and We...ReadTheDocs DocumentationRead the Docs is an open-sourced free software documentation hosting ...Recursive URLThe RecursiveUrlLoader lets you recursively scrape all child links fr...RedditReddit is an American social news aggregation, content rating, and di...RoamROAM is a note-taking tool for networked thought, designed to create ...Rockset‚ö†Ô∏è Deprecation Notice: Rockset Integration DisabledrspaceThis notebook shows how to use the RSpace document loader to import r...RSS FeedsThis covers how to load HTML news articles from a list of RSS feed UR...RSTA reStructured Text (RST) file is a file format for textual data used...scrapflyScrapFly is a web scraping API with headless browser capabilities, pr...ScrapingAntScrapingAnt is a web scraping API with headless browser capabilities,...SingleStoreThe SingleStoreLoader allows you to load documents directly from a Si...SitemapExtends from the WebBaseLoader, SitemapLoader loads a sitemap from a ...SlackSlack is an instant messaging program.SnowflakeThis notebooks goes over how to load documents from SnowflakeSource CodeThis notebook covers how to load source code files using a special ap...SpiderSpider is the fastest and most affordable crawler and scraper that re...SpreedlySpreedly is a service that allows you to securely store credit cards ...StripeStripe is an Irish-American financial services and software as a serv...SubtitleThe SubRip file format is described on the Matroska multimedia contai...SurrealDBSurrealDB is an end-to-end cloud-native database designed for modern ...TelegramTelegram Messenger is a globally accessible freemium, cross-platform,...Tencent COS DirectoryTencent Cloud Object Storage (COS) is a distributedTencent COS FileTencent Cloud Object Storage (COS) is a distributedTensorFlow DatasetsTensorFlow Datasets is a collection of datasets ready to use, with Te...TiDBTiDB Cloud, is a comprehensive Database-as-a-Service (DBaaS) solution...2Markdown2markdown service transforms website content into structured markdown...TOMLTOML is a file format for configuration files. It is intended to be e...TrelloTrello is a web-based project management and collaboration tool that ...TSVA tab-separated values (TSV) file is a simple, text-based file format...TwitterTwitter is an online social media and social networking service.UnstructuredThis notebook covers how to use Unstructured document loader to load ...UnstructuredMarkdownLoaderThis notebook provides a quick overview for getting started with Unst...UnstructuredPDFLoaderUnstructured supports a common interface for working with unstructure...UpstageThis notebook covers how to get started with UpstageDocumentParseLoad...URLThis example covers how to load HTML documents from a list of URLs in...VsdxA visio file (with extension .vsdx) is associated with Microsoft Visi...WeatherOpenWeatherMap is an open-source weather service providerWebBaseLoaderThis covers how to use WebBaseLoader to load all text from HTML webpa...WhatsApp ChatWhatsApp (also called WhatsApp Messenger) is a freeware, cross-platfo...WikipediaWikipedia is a multilingual free online encyclopedia written and main...UnstructuredXMLLoaderThis notebook provides a quick overview for getting started with Unst...Xorbits Pandas DataFrameThis notebook goes over how to load data from a xorbits.pandas DataFr...YouTube audioBuilding chat or QA applications on YouTube videos is a topic of high...YouTube transcriptsYouTube is an online video sharing and social media platform created ...YoutubeLoaderDLLoader for Youtube leveraging the yt-dlp library.YuqueYuque is a professional cloud-based knowledge base for team collabora...ZeroxPDFLoaderZeroxPDFLoader is a document loader that leverages the Zerox library....Edit this pagePreviousZenGuard AINextDocument loadersWebpagesPDFsCloud ProvidersSocial PlatformsMessaging ServicesProductivity toolsCommon File TypesAll document loadersCommunityLangChain ForumTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "web_based_loader = WebBaseLoader(\"https://python.langchain.com/docs/integrations/document_loaders/\")\n",
    "documents = web_based_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28b13e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into ‚ÄúProblem PDDL‚Äù, then (2) requests a classical planner to generate a PDDL plan based on an existing ‚ÄúDomain PDDL‚Äù, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\n\\nExamples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\n\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: ‚Ä¶ step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\n\\nIllustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\n\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent‚Äôs working memory, up to three, to be used as context for querying LLM.\\n\\n\\nExperiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\n\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\n\\nAfter fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\n\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\n\\nIllustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\n\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\n\\nComparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\n\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I‚Äôve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. ‚Äúsix degrees of separation‚Äù feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can‚Äôt get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\n\\nComparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\n\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\n\\nA picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\n\\nMRKL (Karpas et al. 2022), short for ‚ÄúModular Reasoning, Knowledge and Language‚Äù, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of ‚Äúexpert‚Äù modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the ‚ÄúExternal APIs‚Äù section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\n\\nIllustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\n\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\n\\nPseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\n\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent‚Äôs tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API‚Äôs description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user‚Äôs requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents‚Äô experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent‚Äôs behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent‚Äôs future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe generative agent architecture. (Image source: Park et al. 2023)\\n\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code‚Äôs language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the ‚Äúentrypoint‚Äù file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  ‚Ä¶ same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). ‚ÄúLLM-powered Autonomous Agents‚Äù. Lil‚ÄôLog. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. ‚ÄúChain of thought prompting elicits reasoning in large language models.‚Äù NeurIPS 2022\\n[2] Yao et al. ‚ÄúTree of Thoughts: Dliberate Problem Solving with Large Language Models.‚Äù arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. ‚ÄúChain of Hindsight Aligns Language Models with Feedback\\n‚Äú arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. ‚ÄúLLM+P: Empowering Large Language Models with Optimal Planning Proficiency‚Äù arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. ‚ÄúReAct: Synergizing reasoning and acting in language models.‚Äù ICLR 2023.\\n[6] Google Blog. ‚ÄúAnnouncing ScaNN: Efficient Vector Similarity Search‚Äù July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. ‚ÄúReflexion: an autonomous agent with dynamic memory and self-reflection‚Äù arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. ‚ÄúIn-context Reinforcement Learning with Algorithm Distillation‚Äù ICLR 2023.\\n[10] Karpas et al. ‚ÄúMRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.‚Äù arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. ‚ÄúWebgpt: Browser-assisted question-answering with human feedback.‚Äù arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. ‚ÄúTALM: Tool Augmented Language Models‚Äù\\n[13] Schick et al. ‚ÄúToolformer: Language Models Can Teach Themselves to Use Tools.‚Äù arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. ‚ÄúAPI-Bank: A Benchmark for Tool-Augmented LLMs‚Äù arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. ‚ÄúHuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace‚Äù arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. ‚ÄúChemCrow: Augmenting large-language models with chemistry tools.‚Äù arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. ‚ÄúEmergent autonomous scientific research capabilities of large language models.‚Äù arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. ‚ÄúGenerative Agents: Interactive Simulacra of Human Behavior.‚Äù arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\", ),\n",
    "                       bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                           class_=(\"post-title\", \"post-header\", \"post-content\")\n",
    "                       )))\n",
    "\n",
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395dcb47",
   "metadata": {},
   "source": [
    "### 2. Text Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebc00432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../datasets/speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\n√¢‚Ç¨¬¶\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us√¢‚Ç¨‚Äùhowever hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendship√¢‚Ç¨‚Äùexercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts√¢‚Ç¨‚Äùfor democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "text_loader = TextLoader(file_path='../datasets/speech.txt')\n",
    "documents = text_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e11be41",
   "metadata": {},
   "source": [
    "### 3. PyPDF Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "553e2251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../datasets/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani‚àó\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer‚àó\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar‚àó\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit‚àó\\nGoogle Research\\nusz@google.com\\nLlion Jones‚àó\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez‚àó ‚Ä†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n≈Åukasz Kaiser‚àó\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin‚àó ‚Ä°\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n‚Ä†Work performed while at Google Brain.\\n‚Ä°Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../datasets/attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht‚àí1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../datasets/attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../datasets/attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by ‚àödk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n‚àödk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1‚àödk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1‚àödk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q ¬∑ k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../datasets/attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ‚àà Rdmodel√ódk , WK\\ni ‚àà Rdmodel√ódk , WV\\ni ‚àà Rdmodel√ódv\\nand WO ‚àà Rhdv√ódmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n‚Ä¢ In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n‚Ä¢ The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n‚Ä¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to ‚àí‚àû) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by ‚àödmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../datasets/attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ¬∑ d) O(1) O(1)\\nRecurrent O(n ¬∑ d2) O(n) O(n)\\nConvolutional O(k ¬∑ n ¬∑ d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r ¬∑ n ¬∑ d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2œÄ to 10000 ¬∑ 2œÄ. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ‚àà Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../datasets/attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k ¬∑ n ¬∑ d + n ¬∑ d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with Œ≤1 = 0.9, Œ≤2 = 0.98 and œµ = 10‚àí9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d‚àí0.5\\nmodel ¬∑ min(step_num‚àí0.5, step_num ¬∑ warmup_steps‚àí1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../datasets/attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 ¬∑ 1020\\nGNMT + RL [38] 24.6 39.92 2.3 ¬∑ 1019 1.4 ¬∑ 1020\\nConvS2S [9] 25.16 40.46 9.6 ¬∑ 1018 1.5 ¬∑ 1020\\nMoE [32] 26.03 40.56 2.0 ¬∑ 1019 1.2 ¬∑ 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 ¬∑ 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 ¬∑ 1020 1.1 ¬∑ 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 ¬∑ 1019 1.2 ¬∑ 1021\\nTransformer (base model) 27.3 38.1 3.3 ¬∑ 1018\\nTransformer (big) 28.4 41.8 2.3 ¬∑ 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value œµls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty Œ± = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../datasets/attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop œµls\\ntrain PPL BLEU params\\nsteps (dev) (dev) √ó106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../datasets/attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and Œ± = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../datasets/attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, √áaglar G√ºl√ßehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770‚Äì778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J√ºrgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and J√ºrgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735‚Äì1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832‚Äì841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] ≈Åukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] ≈Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../datasets/attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313‚Äì330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152‚Äì159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar T√§ckstr√∂m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433‚Äì440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929‚Äì1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440‚Äì2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104‚Äì3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google‚Äôs neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434‚Äì443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../datasets/attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‚Äòmaking‚Äô, completing the phrase ‚Äòmaking...more difficult‚Äô. Attentions here shown only for\\nthe word ‚Äòmaking‚Äô. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../datasets/attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‚Äòits‚Äô for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../datasets/attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_loader = PyPDFLoader('../datasets/attention.pdf')\n",
    "documents = pdf_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11dc3c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../datasets/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani‚àó\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer‚àó\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar‚àó\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit‚àó\\nGoogle Research\\nusz@google.com\\nLlion Jones‚àó\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez‚àó ‚Ä†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n≈Åukasz Kaiser‚àó\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin‚àó ‚Ä°\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n‚Ä†Work performed while at Google Brain.\\n‚Ä°Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61084621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff1dd8e",
   "metadata": {},
   "source": [
    "### 4. Arxiv Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ea9ea27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2025-04-01', 'Title': 'Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs to Ignore the Correct Reasoning Steps', 'Authors': 'Yu Cui, Bryan Hooi, Yujun Cai, Yiwei Wang', 'Summary': 'Recent reasoning large language models (LLMs) have demonstrated remarkable\\nimprovements in mathematical reasoning capabilities through long\\nChain-of-Thought. The reasoning tokens of these models enable self-correction\\nwithin reasoning chains, enhancing robustness. This motivates our exploration:\\nhow vulnerable are reasoning LLMs to subtle errors in their input reasoning\\nchains? We introduce \"Compromising Thought\" (CPT), a vulnerability where models\\npresented with reasoning tokens containing manipulated calculation results tend\\nto ignore correct reasoning steps and adopt incorrect results instead. Through\\nsystematic evaluation across multiple reasoning LLMs, we design three\\nincreasingly explicit prompting methods to measure CPT resistance, revealing\\nthat models struggle significantly to identify and correct these manipulations.\\nNotably, contrary to existing research suggesting structural alterations affect\\nmodel performance more than content modifications, we find that local ending\\ntoken manipulations have greater impact on reasoning outcomes than structural\\nchanges. Moreover, we discover a security vulnerability in DeepSeek-R1 where\\ntampered reasoning tokens can trigger complete reasoning cessation. Our work\\nenhances understanding of reasoning robustness and highlights security\\nconsiderations for reasoning-intensive applications.'}, page_content='Preprint. Under review.\\nProcess or Result? Manipulated Ending Tokens Can Mislead\\nReasoning LLMs to Ignore the Correct Reasoning Steps\\nYu Cui1‚àó\\nBryan Hooi2\\nYujun Cai3\\nYiwei Wang1‚Ä†\\n1University of California, Merced, 2National University of Singapore\\n3University of Queensland\\n{cuiyu.ycui, wangyw.evan}@gmail.com\\nAbstract\\nRecent reasoning large language models (LLMs) have demonstrated re-\\nmarkable improvements in mathematical reasoning capabilities through\\nlong Chain-of-Thought. The reasoning tokens of these models enable self-\\ncorrection within reasoning chains, enhancing robustness. This motivates\\nour exploration: how vulnerable are reasoning LLMs to subtle errors in their\\ninput reasoning chains? We introduce \"Compromising Thought\" (CPT),\\na vulnerability where models presented with reasoning tokens contain-\\ning manipulated calculation results tend to ignore correct reasoning steps\\nand adopt incorrect results instead. Through systematic evaluation across\\nmultiple reasoning LLMs, we design three increasingly explicit prompting\\nmethods to measure CPT resistance, revealing that models struggle sig-\\nnificantly to identify and correct these manipulations. Notably, contrary\\nto existing research suggesting structural alterations affect model perfor-\\nmance more than content modifications, we find that local ending token\\nmanipulations have greater impact on reasoning outcomes than structural\\nchanges. Moreover, we discover a security vulnerability in DeepSeek-R1\\nwhere tampered reasoning tokens can trigger complete reasoning cessation.\\nOur work enhances understanding of reasoning robustness and highlights\\nsecurity considerations for reasoning-intensive applications.\\n1\\nIntroduction\\nLarge language models (LLMs) have made remarkable progress in mathematical reasoning\\n(Zhang et al., 2024b; 2025), yet ensuring their robustness against errors remains challenging\\n(Li et al., 2024). Recent reasoning LLMs (Li et al., 2025b) such as DeepSeek-R1 (DeepSeek-\\nAI et al., 2025) have significantly improved mathematical problem-solving through their\\nreasoning tokens encompassing long Chain-of-Thought (CoT). These models continuously\\nscrutinize their own computational processes, boosting their self-correction capability (Ku-\\nmar et al., 2024; Song et al., 2025; Yang et al., 2024; YAO et al., 2024), which enhances accuracy\\nand reliability in the reasoning. This inspires us to explore the effects of incorporating minor\\nerrors into reasoning traces and evaluating how well the models can self-correct. Therefore,\\na critical question emerges: how vulnerable are these advanced reasoning LLMs to subtle\\nerrors in their input reasoning chains?\\nWe investigate this problem by examining how reasoning LLMs respond when presented\\nwith reasoning tokens containing tampered calculation results. Our findings reveal a\\nconcerning vulnerability we term \"Compromising Thought\" (CPT): when provided with\\nreasoning tokens incorporating manipulated calculation results at the ends of reasoning\\nlogic steps, models tend to undergo thinking confusion, ignore the correct reasoning steps\\nand adopt the incorrect results instead. Even when models initially demonstrate strong\\ninclination toward the correct answer during their reasoning process, the final result may\\n‚àóWork done during internship at UC Merced NLP Lab.\\n‚Ä†Corresponding author.\\n1\\narXiv:2503.19326v2  [cs.AI]  1 Apr 2025\\nPreprint. Under review.\\n38697082 √ó 4133991 = ?\\nOkay, so I need to multiply‚Ä¶\\nThe product is 159,973,388,714,262.\\nQuestion\\nReasoning LLMs\\nOutput\\nReasoning \\nTokens\\nOutput\\nAnswer\\nTokens\\n‚Ä¶\\nThus, the product is 159,973,388,714,262.\\nDirect Prompting\\nBelow is your previous thought‚Ä¶\\nThe product is 159973389714262.\\nSuffix Snippet with \\nManipulated Ending \\nReasoning Tokens\\nAppend\\nOkay, let‚Äôs‚Ä¶Step-by-Step Addition‚Ä¶ 159, \\n973,350,017,180 + 38,697,082\\n= 159,973,389,714,262\\nFinal Answer: 159,973,389,714,262\\n‚Ä¶\\nFinal Answer: 159,973,389,714,262\\nCorrect \\nReasoning \\nSteps\\nWrong \\nEnding \\nTokens\\nWrong \\nAnswer\\nTokens\\nFigure 1: CPT overview. In the original reasoning tokens, the correct ending tokens 159,973,388,714,262\\nwill be altered to the incorrect answer 159,973,389,714,262.\\nsuddenly shift to match the incorrect values from the tampered reasoning tokens, resulting\\nin a inconsistent reasoning in the output result.\\nConsider the example in Figure 1, where a model can correctly solve a mathematical\\nmultiplication problem under normal conditions. When the same model is presented with\\nthe same problem but with subtly tampered reasoning tokens (changing just one digit in\\nthe final product) as its previous thinking, it produces a new reasoning chain with illogical\\ncalculation steps. In addition, the final result content includes the correct steps and incorrect\\nproduct. This inconsistency demonstrates how endpoint tampering of input reasoning\\nchain can override the model‚Äôs own reasoning capabilities. This vulnerability has important\\nimplications for the reliability of reasoning LLMs. The self-correction of LLMs is crucial for\\nenhancing their reasoning robustness. However, our work shows that current reasoning\\nLLMs, despite their advances, remain susceptible to being misled by tampered tokens,\\ncompromising their self-correction abilities.\\nTo systematically study this phenomenon, we design three increasingly explicit prompting\\nmethods to encourage models to scrutinize the tampered reasoning tokens. Our experiments\\nacross multiple reasoning LLMs (DeepSeek-R1, OpenAI-o1(Jaech et al., 2024), o3-mini1,\\nand others) reveal that these models have difficulties in identifying wrong answers in the\\nreasoning tokens, and struggle significantly to resist CPT.\\nOur investigation into CPT reveals an interesting contrast with recent work (Li et al., 2025a)\\non reasoning chains. While existing research suggests global structural alterations to long\\nCoT more significantly impact model performance than local content modifications, we find\\nthat in reasoning vulnerabilities, endpoint token manipulations can have greater effects than\\nstructural changes. Some structural modifications even enhance CPT resistance. Addition-\\nally, we discovered that when presented with tampered reasoning tokens for mathematical\\nword problems, DeepSeek-R1 may experience \"thinking stopped\"‚Äîcompletely ceasing its\\nreasoning process without producing a final answer. This is a security vulnerability in\\nmodel reasoning that deserves attention.\\nAbove all, the contributions of this paper are as follows: (1) We focus on mathematical\\nproblem-solving and introduce a new research question: how well can reasoning LLMs\\nperform even in the presence of minor errors introduced in the input reasoning tokens? (2)\\nWe define the new concept of CPT and demonstrate, based on three approaches, that existing\\nreasoning LLMs struggle to resist CPT; (3) We find that local ending token modifications\\nhave a greater impact on model reasoning than structural changes, which is contrary to\\nthe existing research; (4) Based on our tampered reasoning tokens, we find a security\\nvulnerability that DeepSeek-R1 may stop thinking and fail to produce the final result.\\n1https://openai.com/index/openai-o3-mini\\n2\\nPreprint. Under review.\\n2\\nRelated Work\\nReasoning LLMs. Reasoning LLMs show significant advantages in solving complex prob-\\nlems based on their long CoT. Li et al. (2025b) discusses the evolution of different reasoning\\nLLMs and provides the comprehensive performance comparison of typical reasoning LLMs.\\n(Chen et al., 2025b) provides a summary of Long CoT, highlighting its main features, includ-\\ning deep reasoning, feasible reflection, and extensive exploration.\\nResistance of LLMs to Errors in Reasoning It is crucial to study the impact of introducing\\nminor mistakes in the reasoning process and analyze the effectiveness of the self-correction\\nof models. Mukherjee et al. (2025) focuses on improving the error identification ability of\\nLLMs in the reasoning chain. Li et al. (2025a) indicates that the structure of the long CoT is\\nvital for LLMs to learning, while small tampering to local content (for instance, the accuracy\\nof the final answer and the numerical values) do not substantially affect model performance.\\nHowever, our work reaches an opposing conclusion to some extent. In addition, many\\nstudies focus on the mathematical reasoning ability evaluation (Yuan et al., 2023; Liu et al.,\\n2024; Li et al., 2024) of LLMs. He et al. (2025) systematically studies the ability of LLMs to\\ndetect errors in long CoT reasoning, and finds that there will be calculation error, reasoning\\nerror and understanding errors in the inference of mathematical problems. But there is\\nstill relatively little research on evaluation and improvement of the model‚Äôs capability to\\nreflect and self-correct mistakes in the reasoning process (Chen et al., 2025a). Moreover, the\\nreasoning tokens may not only contain errors, but may also be inconsistent with the final\\nresults given by the reasoning LLMs (Zhou et al., 2025).\\n3\\nMethodology\\nWhen reasoning LLMs solve mathematical problems, they generate detailed reasoning\\nsteps leading to a final answer. We propose \"Compromising Thought\" (CPT) to describe a\\nvulnerability where models adopt incorrect results from tampered reasoning tokens despite\\ntheir ability to calculate correctly. This behavior raises important questions about how these\\nmodels integrate prior information with current reasoning processes.\\nTo systematically investigate CPT, we first analyze the structure of reasoning tokens in\\nmathematical problem-solving (Section 3.1), then formalize the concept of CPT and develop\\nmetrics to quantify its impact (Section 3.2). We explain our approach to tampering with\\nreasoning tokens (Section 3.3), evaluate methods to enhance model resistance (Section 3.4),\\nand investigate how structural modifications affect CPT (Section 3.5).\\n3.1\\nInitial Observations on Reasoning Token Vulnerabilities\\nWhen evaluating reasoning LLMs on mathematical problems, we discovered that models\\npresented with reasoning chains containing subtle calculation errors often adopt these\\nincorrect results in their outputs, despite their ability to calculate correctly. For example,\\nwhen DeepSeek-R1 correctly solved 38,697,082 √ó 4,133,991, changing just one digit in\\nthe result tokens (from 159,973,388,714,262 to 159,973,389,714,262) caused it to adopt this\\nincorrect value in subsequent reasoning.\\nThis behavior appeared across multiple models, particularly when modifications targeted\\n\"loop ending result\" tokens - the numerical conclusions at critical points in reasoning. This\\nobservation raised a key question: How do reasoning LLMs prioritize between their own\\ncalculation abilities and presented calculation results? This apparent vulnerability forms the\\nbasis of our \"Compromising Thought\" (CPT) concept formalized in the following section.\\n3.2\\nCPT of Reasoning LLMs\\nWe formalize CPT as follows: Given a mathematical problem q0, we first establish that a\\nreasoning LLM can correctly solve it, generating accurate reasoning tokens rt0 and final\\nanswer s0. We then create tampered tokens rtT by modifying specific calculation results in\\nthe ending tokens of rt0 while preserving the reasoning steps. When these tampered tokens\\n3\\nPreprint. Under review.\\nare presented alongside the original problem to the models in LM, we observe whether\\nthe model‚Äôs new reasoning tokens rtc and answer sc align with the correct answer w or are\\ninfluenced by the tampered information (Algorithm 1). It should be noted that we require\\nthe initial reasoning tokens rtm and final result sm to be correct, meaning that the evaluated\\nmodel itself has the ability to correctly solve q0. Otherwise, it is unreasonable to investigate\\nwhether it is affected by CPT.\\nTo quantify CPT susceptibility, we define the compromising rate:\\nrCPT =\\nDT\\n2DT ‚àíDCPT\\n‚àà[0.5, 1],\\n(1)\\nwhere DT is the number of tampered digits and DCPT is the number of digits in the model‚Äôs\\noutput matching our tampered digits rather than the correct result. For instance, with a\\nsingle-digit modification (DT = 1), a value of rCPT = 1 indicates complete compromise,\\nwhile rCPT = 0.5 shows complete resistance. Specifically, when CPT causes severe reasoning\\nconfusion in models, i.e. DT < DCPT (see the instance 5 and 6 in Appendix A), rCPT = 2.\\nThe aggregate compromising rate RCPT across n evaluation results is calculated as:\\nRCPT = 1\\nn\\nn\\n‚àë\\ni=1\\nrCPT,i\\n(2)\\n3.3\\nTampering with Reasoning Tokens\\nAlgorithm 1: CPT Research\\nInput: Reasoning LLM set LM, question\\nq0, method Tamper for tampering with\\nending tokens, correct answer w to q0.\\nOutput: Result s.\\ns := ‚àÖ\\n(rt0, s0) ‚ÜêM(q0)\\n‚ñ∑M ‚ààLM.\\nrtT ‚ÜêTamper(rt0)\\nqT ‚Üêq0‚à•rtT\\nfor j = 0 to length(LM) ‚àí1 do\\n(rtm, sm) ‚ÜêLMj(q0)\\nif w ‚äÜsm then\\n(rtc, sc) ‚ÜêLMj(qT)\\nif sc ==‚ä•then\\nr ‚Üê‚ä•\\n‚ñ∑Thinking stopped.\\ncontinue\\nelse\\nr ‚ÜêCompare(rtm, rtc, sm, sc)\\nend\\nelse\\nr ‚Üê‚ä•\\ncontinue\\nend\\ns ‚Üês ‚à™r\\nend\\nreturn s\\nTo systematically study CPT, we need a principled\\napproach to create tampered reasoning tokens. As\\nillustrated in Figure 2, reasoning tokens typically\\ncontain calculation loops, self-reflection, and self-\\nverification components. Our tampering strategy\\nspecifically targets the \"loop ending result\" tokens\\n- the numerical conclusions at critical points in the\\nreasoning process.\\nWe implement digit substitution as our primary\\nmethod,\\ncarefully modifying specific digits in\\ncalculation results while maintaining the origi-\\nnal format and length.\\nFor example, changing\\n\"711,025,910,075,981\" to \"711,025,710,075,981\" by re-\\nplacing ‚Äô9‚Äô with ‚Äô7‚Äô. The instances are shown in Ap-\\npendix C. This method creates subtle numerical dis-\\ncrepancies that challenge the model‚Äôs ability to recon-\\ncile its own calculations with presented information.\\nAfter evaluating alternative approaches, we found\\nthat digit substitution is optimal for studying CPT,\\nas methods that alter result length (insertion or dele-\\ntion) tend to trigger format validation mechanisms\\nrather than testing numerical reasoning fidelity. By\\npreserving the structure and only modifying specific\\nresult digits, we can directly assess how reasoning\\nLLMs prioritize between their own calculations and\\nexternally provided conclusions.\\n3.4\\nProbing LLM Resistance to CPT\\nTo evaluate how effectively reasoning LLMs can resist CPT, we developed three increasingly\\ndirect interventions (Figure 3):\\n‚Ä¢ Method 1 (Uncertainty Prompting): Uses filter text suggesting possible errors in the\\nreasoning tokens.\\n4\\nPreprint. Under review.\\nAlright, so I need to add these two big numbers‚Ä¶\\nBut let\\'s verify the \\naddition another way‚Ä¶\\n(2) Calculation\\nEnding Reasoning \\nTokens \\n(4) Self-reflection\\n(6) Self-verification\\n(1) Start Thinking\\n(5) Loop Execution\\n‚Ä¶Therefore, I\\'m confident the answer is correct.\\n(10) End Thinking\\n(7) Calculation\\nEnding Reasoning \\nTokens \\n(9) Loop Execution\\nWait, maybe I should‚Ä¶\\n(3) Loop Ending Result\\n(8) Loop Ending Result\\nFigure 2: Overview of reasoning tokens for solving mathematical problems and specific objectives of\\ntampering.\\n‚Ä¢ Method 2 (Direct Contradiction): Explicitly states the reasoning tokens contain errors\\nand instructs the model to ignore them.\\n‚Ä¢ Method 3 (Output Prefix Control): On the basis of Method 2, implements forced recon-\\nsideration using the method described by (Wang et al., 2024), which leverages output\\nprefix to override model tendencies.\\nFor Method 3, we provide implementation pseudocode based on the DeepSeek API2 (Fig-\\nure 3). Since reasoning tokens are often not directly accessible in API-based interactions, we\\nfocus our analysis on final outputs for this condition. These interventions create a systematic\\nframework for measuring CPT resistance across models and mathematical problem types.\\nComplete examples of model responses across these conditions are provided in Appendix B.\\nMy previous thinking is incorrect, especially the final result. \\nTherefore, I should ignore my previous reasoning and rethink.\\nresponse = client.chat.completions.create(\\n  model=\"deepseek-reasoner\",\\n  messages=[\\n  {\"role\": \"user\", \"content\": \"<Question>+<Input Suffix>\"},\\n  {\"role\": \"assistant\", \"content\":\"<Output Prefix>\", \"prefix\": True}])\\nprint(\"result: \", response.choices[0].message.content)\\nInput\\nSuffix\\nInput\\nSuffix\\nMethod 2\\nCode\\nOutput \\nPrefix\\nMethod 1: Uncertainty Prompting\\nMethod 2: Direct Contradiction\\nMethod 3: Output Prefix Control\\nBelow is your previous thought. But \\nyour previous thinking may be wrong. \\nYou need to carefully identify‚Ä¶\\nBelow is your previous thought. But \\nyour previous thinking is wrong. You \\nshould ignore it and rethink‚Ä¶\\nFigure 3: The approaches of exploring the resistance of reasoning LLMs to CPT.\\n3.5\\nStructural Modifications on Tampered Reasoning Tokens\\nTo investigate whether the \"structure matters more than local results\" hypothesis proposed\\nby (Li et al., 2025a) applies to the CPT phenomenon, we adopted their effective reasoning\\nstep modification methods (deleted and inserted reasoning steps) to structurally modify the\\ntampered reasoning tokens in our study. We consider self-reflection and self-verification\\ncomponents (Figure 2) as integral reasoning units. For deleted reasoning steps, we imple-\\nment two approaches: Delete-R (removing only self-reflection) and Delete-V (removing only\\nself-verification). For inserted reasoning steps, we introduce extraneous reasoning tokens\\nrelated to angle calculation between the self-reflection and self-verification components.\\nIf structural modifications have a more significant impact than content modifications on\\nreasoning outcomes, we would expect the model‚Äôs ability to resist CPT to be substantially\\nweakened when the structure is altered, leading to increased compromising thinking proba-\\nbility. Conversely, if local content modifications prove more influential, structural changes\\nmight have minimal effect or even potentially enhance CPT resistance. Through these struc-\\ntural modifications, we aim to test whether the conclusions from prior work on reasoning\\nchain structure extend to the CPT phenomenon, or whether result token modifications\\nrepresent a distinct vulnerability that operates through different mechanisms.\\n2https://api-docs.deepseek.com\\n5\\nPreprint. Under review.\\nModel\\nBaseline\\nMethod 1\\nMethod 2\\nMethod 3\\nAverage\\nDEEPSEEK-R1\\n1.00\\n0.83\\n0.83\\n0.50\\n0.72\\nO3-MIN\\n0.50\\n0.75\\n0.75\\n0.50\\n0.67\\nKIMI K1.5\\n1.00\\n1.00\\n1.00\\n-\\n1.00\\nOPENAI-O1\\n0.67\\n0.50\\n0.50\\n0.50\\n0.50\\nDOUBAO\\n1.00\\n1.00\\n1.00\\n-\\n1.00\\nTable 1: Evaluation result of CPT towards reasoning LLMs in mathematical word problems.\\n4\\nExperiments\\n4.1\\nExperimental Setup\\nDataset. Our research requires an in-depth analysis of the reasoning tokens generated\\nby reasoning LLMs. Since most model APIs do not allow direct access to these tokens,\\nautomated evaluation is difficult and extensive human assessment is required. As a result,\\nour experimental dataset is relatively concise and precise. For standalone arithmetic tasks\\n(Shrestha et al., 2025), we focus on addition and multiplication problems and construct a\\ntest dataset containing 15-digit plus 13-digit addition and 8-digit by 7-digit multiplication\\nvia random number generation. For mathematical word problems, we randomly selected 6\\nrepresentative questions (see Appendix E) from GSM-Ranges (Shrestha et al., 2025) with\\nlevel 6 perturbation to build a subset for evaluation.\\nModels. We evaluate multiple reasoning LLMs, including DeepSeek-R1, OpenAI o3-min,\\nKimi k1.5 (long-CoT) (Team et al., 2025), Doubao (Deep Thinking) 3, and OpenAI-o1. To\\nacquire reasoning tokens, the models were accessed through their respective APIs using\\ndefault temperature settings and web-based conversations.\\nBenchmark. We conduct experimental evaluation following Algorithm 1, and use the\\nquestioning method illustrated in Figure 1 as our baseline. This baseline simply appends\\nthe question to the manipulated reasoning tokens by using a basic filter (i.e., \"Below is your\\nprevious thought.\"). We compare the baseline with three methods (Method 1-3) designed\\nto resist CPT. For mathematical calculation tasks, we adopt manipulated reasoning tokens\\nwith different levels of tampering, defined by DT ‚àà[1, 6]. We focus on the DT = 1 case\\nfor mathematical word problems, due to the complex numerical and logical structures of\\nthe tokens. To better demonstrate the impact of the CPT issue, we use DeepSeek-R1 for\\nM in Algorithm 1, as its generated long CoT exhibits a more rigorous and comprehensive\\nreasoning process.\\nEvaluation Protocol. For each model and problem combination, we first verify that the\\nmodel can correctly solve the problem independently. Then, following Algorithm 1, we\\npresent the model with the same problem appended with tampered reasoning tokens and\\nmeasure the compromising rate as defined in Equation 1. For structural modification\\nexperiments, we implement three variants (Delete-R, Delete-V, and Insertion) as described\\nin Section 3.5 and evaluate their impact on CPT resistance.\\n4.2\\nMain Results\\nIn Figure 4, we present the RCPT evaluation results for the reasoning LLMs performing both\\naddition and multiplication at various DT values. As far as we know, Doubao model cannot\\nimplement the output prefix method under deep-thinking, so its evaluation data for Method\\n3 are omitted and thus displayed as a zero value in Figure 5. Overall, Method 3 substantially\\noutperforms Method 1 and Method 2 in improving models‚Äô resistance to CPT. Under the\\nbaseline condition, all models are noticeably affected by CPT, with DeepSeek-R1 being the\\nmost susceptible. For DeepSeek-R1, cases under Method 3 arise in which DT is lower than\\nDCPT, causing RCPT to exceed the baseline. This indicates that under the output prefix\\napproach, DeepSeek-R1‚Äôs mathematical computation capabilities are not robust enough.\\nFurthermore, it can be clearly observed that resistance to CPT generally strengthens as DT\\nincreases.\\n3https://www.doubao.com/chat\\n6\\nPreprint. Under review.\\n1.0\\n2.0\\n3.0\\n4.0\\n5.0\\n6.0\\nNumber of tampered digits\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\nRate of CPT\\nBaseline\\nMethod 1\\nMethod 2\\nMethod 3\\n(a) DeepSeek-R1\\n1.0\\n2.0\\n3.0\\n4.0\\n5.0\\n6.0\\nNumber of tampered digits\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nRate of CPT\\nBaseline\\nMethod 1\\nMethod 2\\nMethod 3\\n(b) o3-mini\\n1.0\\n2.0\\n3.0\\n4.0\\n5.0\\n6.0\\nNumber of tampered digits\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\nRate of CPT\\nBaseline\\nMethod 1\\nMethod 2\\n(c) Doubao\\n1.0\\n2.0\\n3.0\\n4.0\\n5.0\\n6.0\\nNumber of tampered digits\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\nRate of CPT\\nBaseline\\nMethod 1\\nMethod 2\\nMethod 3\\n(d) OpenAI-o1\\nFigure 4: Evaluation result of CPT towards reasoning LLMs in standalone arithmetic tasks.\\nFigure 5 shows CPT performance for both addition and multiplication, aggregating rCPT\\nacross the different DT values. For addition, o3-mini demonstrates excellent resilience,\\nclearly exceeding its multiplication performance. By contrast, DeepSeek-R1 follows the\\nopposite trend, underscoring the variability in CPT‚Äôs influence across distinct mathematical\\noperations. Furthermore, while Method 1 and Method 2 generally reduce RCPT, they\\nnegatively impact OpenAI-o1.\\nLastly, Table 1 summarizes the results for mathematical word problems, which largely align\\nwith the observations from standalone arithmetic tasks. Under the baseline, DeepSeek-R1,\\nDoubao, and kimi k1.5 display minimal resistance to CPT. However, with progressively\\ndirect interventions, DeepSeek-R1 exhibits a marked improvement in resistance.\\nBaseline\\nMethod 1\\nMethod 2\\nMethod 3\\no3-mini\\nDeepSeek-R1\\nDoubao\\nOpenAI-o1\\n0.58\\n0.50\\n0.50\\n0.50\\n0.83\\n0.96\\n0.83\\n0.50\\n0.92\\n0.83\\n0.83\\n0.00\\n0.67\\n0.50\\n0.50\\n0.50\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n(a) Addition\\nBaseline\\nMethod 1\\nMethod 2\\nMethod 3\\no3-mini\\nDeepSeek-R1\\nDoubao\\nOpenAI-o1\\n0.75\\n0.50\\n0.50\\n0.50\\n0.75\\n0.67\\n0.58\\n1.08\\n0.70\\n0.67\\n0.75\\n0.00\\n0.58\\n0.67\\n0.58\\n0.50\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n(b) Multiplication\\nFigure 5: Comparative evaluation of reasoning LLMs on addition and multiplication in CPT.\\nStructure or Content. In Figure 8, we illustrate the reasoning performance of reasoning\\nLLMs for additive calculation under CPT setting and structural modifications on reasoning\\ntokens. Compared with the baseline method, all three approaches of modifying structure\\nimprove the model‚Äôs resistance to CPT. Among these approaches, Delete-R achieves optimal\\nperformance. Therefore, structural modifications are unlikely to significantly enhance the\\nimpact of CPT. This finding also suggests that for the CPT problem, local ending token\\nmanipulations have greater impact on reasoning outcomes than structural alterations, which\\npartially conflicts with previous work (Li et al., 2025a) to some extent.\\n7\\nPreprint. Under review.\\n255 tokens\\nDiligent \\nThinking\\nThe product of 38,697,082 and \\n4,133,991 is 159,973,388,714,262.\\n‚Ä¶But, I‚Äôm going to stick with manual \\ncalculations for now‚Ä¶I‚Äôll continue \\nensuring this is accurate!\\nThe final product is‚Ä¶\\nTo \\nexplain \\nbriefly \\nwithout \\nrevealing all internal details‚Ä¶ A \\nquick estimation‚Ä¶ \\n‚Ä¶We can check the final product \\ndirectly‚Ä¶I believe this result should \\nbe correct‚Ä¶\\nCompromising \\nThinking\\nOutput Reasoning Tokens\\nOutput Answer Tokens\\nInference Costs\\n18 seconds\\n167 tokens\\n6 seconds\\nManipulated \\nEnding \\nTokens\\nNormal \\nQuestions\\nFigure 6: The compromising thinking of o3-mini model. It is caused by the tampered reasoning tokens\\nof DeepSeek-R1 (DT = 2).\\n5\\nAnalysis and Discussion\\nIn the previous experimental results, we explored the impact of CPT on reasoning LLM‚Äôs\\nperformance. This section will further analyze these results, focusing on the additional\\nfindings and new contributions derived from them. We first investigate the impact of\\nvarying degrees of manipulation on CPT (Section 5.1), and then analyze the nature of CPT‚Äôs\\neffect on model reasoning (Section 5.2). Finally, we reveal a security vulnerability that\\nmanipulated tokens can lead DeepSeek-R1 to stop reasoning and fail to deliver a final\\nanswer (Section 5.3).\\n5.1\\nImpact of Different Degrees of Tampering on CPT\\nIn our experiments, along with the increasing of the number of tampered digits in ending\\nreasoning tokens, the reasoning LLMs become more likely to identify and self-correct errors\\nin the calculation results. This may be related to the existing research conclusion that LLMs\\nusually fail to confidently predict the last digit of calculation tasks (Gambardella et al., 2024).\\nIn fact, the degree of numerical discrepancy caused by tampering may be the key factor.\\nAs shown below, when the modified result deviates significantly from the correct result,\\nthe models exhibit higher confidence in identifying the error in the reasoning tokens and\\nproviding the correct result.\\nSignificant tampering enhances the model‚Äôs resistance to CPT\\nWhen I add them all together, I get $1,151,679,562, but the earlier provided result was $1,151,676,562.\\nThat‚Äôs a difference of $3,000, so I‚Äôm confident there was a small error in that chain-of-thought. The\\ncorrect total cost is $1,151,679,562.\\n5.2\\nFundamental Impact of CPT on the Reasoning Process\\nBy carefully analyzing the \"compromising thinking\" process in reasoning LLMs, we find\\nthat compared with the initial reasoning tokens rtm towards solving q0, the tokens under\\nCPT are markedly fewer, and the reasoning time is noticeably shorter (even reduced by\\nmore than 60%). The reason for this is that the models have great confidence that the\\ncalculation result provided is correct, and thus perform simple verification before ending\\nthe reasoning task. For o3-mini, the reasoning and answer tokens under CPT will show\\nobvious slackness, impatience and perfunctory behavior, as shown in Figure 6. The counts\\nof tokens are calculated by Tokenizer (\"GPT-4o & GPT-4o mini\") 4. The poor performance is\\nin sharp contrast to the initial tokens, which indicate the effort to think and pursue careful\\nexamination.\\nMoreover, when dealing with mathematical word problems, o3-mini significantly outper-\\nforms DeepSeek-R1 in resisting CPT. The essence is that the reasoning time under CPT of\\nDeepSeek-R1 is less than initial reasoning time, while that of o3-mini is just the opposite\\n(see Figure 7). Under the influence of CPT, the reasonable and ideal reasoning time TCPT\\n4https://platform.openai.com/tokenizer\\n8\\nPreprint. Under review.\\nQuestion 1\\nQuestion 3\\nQuestion 4\\n100\\n101\\n102\\nReasoning time (s)\\n80\\n74\\n150\\n15\\n18\\n14\\n5\\n21\\n10\\n12\\n28\\n22\\nDeepSeek-Init\\nDeepSeek-CPT\\no3-mini-Init\\no3-mini-CPT\\nFigure 7: Time comparison of the initial reasoning and the reason-\\ning under CPT for DeepSeek-R1 and o3-mini on Q1, Q3, and Q4\\n(Others exhibit thinking stopped as described in Section 5.3).\\nBaseline\\nInsertion\\nDelete-R\\nDelete-V\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRate of CPT\\n0.875\\n0.75\\n0.5\\n0.75\\nFigure 8: Evaluation result of\\nreasoning LLMs for structural\\nmodifications on reasoning to-\\nkens\\nshould be:\\nTinit + Tidenti f y + Tcompare,\\n(3)\\nwhere Tinit denotes the initial time taken to solve q0 independently, Tidenti f y represents the\\ntime spent checking the provided reasoning tokens, and Tcompare is the time spent comparing\\nincorrect results in the tokens with the final calculated results. Therefore, TCPT should be\\nmore than Tinit. In fact, the three methods proposed in Section 3.4 all aim to make the\\nreasoning of LLMs more consistent with this principle.\\n5.3\\nThinking Stopped\\nWhen examining the impact of tampered reasoning tokens on the mathematical word\\nproblem solving of DeepSeek-R1, we accidentally observe an unsafe phenomenon: for\\ncertain questions, DeepSeek-R1 may stop thinking and fail to provide a final answer, as\\nshown in Appendix D. In this kind of reasoning tokens with thinking interruption, we can\\nstill clearly see that DeepSeek-R1 gives a relatively complete thinking process of problem\\nsolving, indicating the influence of CPT. When calling DeepSeek-R1 via its API, this thinking\\nstopped problem manifests as the value of choices[0].message.content in code being\\nempty. In our experiments, the probability of this phenomenon occurring for the chosen\\nproblems is extremely high. Further analysis reveals that the core reason for the problem is\\nthe presence of previously generated reasoning tokens for the same question in the prompt.\\nIn fact, the method to trigger this security vulnerability can be simplified as follows:\\n(rt, s) ‚ÜêLMR1(q0), (rt1, s1) ‚ÜêLMR1(rt),\\n(4)\\nwhere s1 has a very high probability of being empty. In other words, resubmitting to\\nDeepSeek-R1 the reasoning tokens it previously generated for q0 can cause its reasoning to\\nstop. This issue could lead to unexpected problems in downstream applications based on\\nLLMs, such as various Natural Language Processing systems exploiting LLM-based agents\\n(Zhang et al., 2024a). This discovery is of great significance for enhancing the security of\\nDeepSeek-R1 in the future.\\nThe overall structure of reasoning tokens generated by DeepSeek-R1 for mathematical word\\nproblems and standalone arithmetic tasks is similar, but this thinking stopped is difficult to\\noccur in our experiments targeting the latter. Our experimental results show that the core\\ntrigger factor has nothing to do with the count of tokens, and is more likely to be closely\\nrelated to the semantic logic within the reasoning tokens for mathematical word problem.\\nFurther investigations are needed to clarify the specific underlying logic responsible for this\\nvulnerability.\\n9\\nPreprint. Under review.\\n6\\nConclusion\\nIn this paper, we explore the vulnerability of reasoning LLMs to subtle errors in their input\\nreasoning chains. Our findings indicate that long CoT with manipulated ending tokens can\\nmisguide reasoning LLMs to ignore the correct reasoning steps and give wrong final result.\\nFocused on mathematical problem-solving, we introduce the concept of CPT and demon-\\nstrate that existing reasoning LLMs struggle to resist CPT. In addition, our study reveals\\nthat local ending token modifications have a greater influence on model reasoning than\\nstructural changes. Finally, we discover a security vulnerability that tampered reasoning\\ntokens can cause DeepSeek-R1 to stop thinking and fail to deliver a final answer.\\nEthics Statement\\nSome of the research methods we designed may mislead LLMs into outputting incorrect\\nresults, requiring readers to have the ability to distinguish. Additionally, unsafe prompt-\\ning methods mentioned in the paper that could cause abnormal behavior of LLMs, such\\nas thinking stopped, should only be used for scientific research and authorized security\\nevaluation.\\nReferences\\nGuizhen Chen, Weiwen Xu, Hao Zhang, Hou Pong Chan, Chaoqun Liu, Lidong Bing,\\nDeli Zhao, Anh Tuan Luu, and Yu Rong. Finereason: Evaluating and improving llms‚Äô\\ndeliberate reasoning through reflective puzzle solving. arXiv preprint arXiv:2502.20238,\\n2025a.\\nQiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang\\nHu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: A survey of long\\nchain-of-thought for reasoning large language models, 2025b. URL https://arxiv.org/\\nabs/2503.09567.\\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin\\nXu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu,\\nZ. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan\\nWang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu\\nZhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong\\nDai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu,\\nHaocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong\\nGuo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L.\\nCai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin\\nHuang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang,\\nLei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun\\nWang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu\\nChen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L.\\nJin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu\\nWang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu,\\nShengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao,\\nWen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An,\\nXiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie,\\nXingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin,\\nXiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou,\\nXianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao\\nLi, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong,\\nYing He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo,\\nYuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo,\\nYuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui\\nLi, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren,\\nZehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao,\\n10\\nPreprint. Under review.\\nZhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie,\\nZiyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv\\npreprint arXiv:2501.12948, 2025.\\nAndrew Gambardella, Yusuke Iwasawa, and Yutaka Matsuo. Language models do hard\\narithmetic tasks easily and hardly do easy arithmetic tasks. In Lun-Wei Ku, Andre Martins,\\nand Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for\\nComputational Linguistics (Volume 2: Short Papers), pp. 85‚Äì91, Bangkok, Thailand, August\\n2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-short.8. URL\\nhttps://aclanthology.org/2024.acl-short.8/.\\nYancheng He, Shilong Li, Jiaheng Liu, Weixun Wang, Xingyuan Bu, Ge Zhang, Zhongyuan\\nPeng, Zhaoxiang Zhang, Wenbo Su, and Bo Zheng. Can large language models detect\\nerrors in long chain-of-thought reasoning? arXiv preprint arXiv:2502.19361, 2025.\\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low,\\nAlec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko,\\nAlex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison\\nTam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein,\\nAndrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair,\\nBarret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob\\nMcGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McK-\\ninzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li,\\nCharles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger,\\nChristopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel\\nLevy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris\\nTsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Eliz-\\nabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan\\nWang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Fran-\\ncis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo,\\nGildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming\\nBao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman,\\nHyung Won Chung, Ian Kivlichan, Ian O‚ÄôConnell, Ian Osband, Ignasi Clavera Gilaberte,\\nIlge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon,\\nJason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu,\\nJoaquin Qui√±onero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman,\\nJohn Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie\\nWang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla\\nWood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu,\\nLama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng,\\nLinden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz\\nKaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko\\nTintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yat-\\nbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael\\nLampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov,\\nMiles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat\\nMcAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown,\\nOfir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne,\\nPavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo\\nLopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown,\\nRoshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer,\\nSam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker,\\nScott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray\\nChaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh\\nSanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders,\\nTejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng,\\nTimur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou,\\nValerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng,\\nWenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen,\\n11\\nPreprint. Under review.\\nYoung Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan\\nLi. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024.\\nAviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate\\nBaumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to\\nself-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024.\\nDacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Shishir G Patil, Matei Zaharia,\\nJoseph E Gonzalez, and Ion Stoica. Llms can easily learn to reason from demonstrations\\nstructure, not content, is what matters! arXiv preprint arXiv:2502.07374, 2025a.\\nQintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, and Wei Bi. GSM-plus: A com-\\nprehensive benchmark for evaluating the robustness of LLMs as mathematical problem\\nsolvers. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd\\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\\n2961‚Äì2984, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\\ndoi: 10.18653/v1/2024.acl-long.163. URL https://aclanthology.org/2024.acl-long.\\n163/.\\nZhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao,\\nHaotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: A\\nsurvey of reasoning large language models. arXiv preprint arXiv:2502.17419, 2025b.\\nHongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe Zhou,\\nWenwei Zhang, Songyang Zhang, Dahua Lin, and Kai Chen. MathBench: Evaluating the\\ntheory and application proficiency of LLMs with a hierarchical mathematics benchmark.\\nIn Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association\\nfor Computational Linguistics: ACL 2024, pp. 6884‚Äì6915, Bangkok, Thailand, August 2024.\\nAssociation for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.411. URL\\nhttps://aclanthology.org/2024.findings-acl.411/.\\nSagnik Mukherjee, Abhinav Chinta, Takyoung Kim, Tarun Anoop Sharma, and\\nDilek Hakkani Tur. Premise-augmented reasoning chains improve error identification in\\nmath reasoning with llms. arXiv preprint arXiv:2502.02362, 2025.\\nSafal Shrestha, Minwu Kim, and Keith Ross. Mathematical reasoning in large language\\nmodels: Assessing logical and arithmetic errors across wide numerical ranges. arXiv\\npreprint arXiv:2502.08680, 2025.\\nXiaoshuai Song, Yanan Wu, Weixun Wang, Jiaheng Liu, Wenbo Su, and Bo Zheng. Progco:\\nProgram helps self-correction of large language models. arXiv preprint arXiv:2501.01264,\\n2025.\\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li,\\nChenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao\\nZhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai,\\nHaiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian\\nZhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan,\\nJia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu,\\nJunjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang,\\nNingchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei,\\nSihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao\\nHuang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran\\nXu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang\\nLiu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao,\\nYulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu,\\nZheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and\\nZonghan Yang. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint\\narXiv:2501.12599, 2025.\\nYiwei Wang, Muhao Chen, Nanyun Peng, and Kai-Wei Chang. Vulnerability of large\\nlanguage models to output prefix jailbreaks: Impact of positions on safety. 11 2024. doi:\\n10.36227/techrxiv.173161123.33560859/v1.\\n12\\nPreprint. Under review.\\nZhe Yang, Yichang Zhang, Yudong Wang, Ziyao Xu, Junyang Lin, and Zhifang Sui. Confi-\\ndence vs critique: A decomposition of self-correction capability for llms. arXiv preprint\\narXiv:2412.19513, 2024.\\nYuxuan YAO, Han Wu, Zhijiang Guo, Zhou Biyan, Jiahui Gao, Sichun Luo, Hanxu\\nHou, Xiaojin Fu, and Linqi Song.\\nLearning from correctness without prompting\\nmakes LLM efficient reasoner. In First Conference on Language Modeling, 2024. URL\\nhttps://openreview.net/forum?id=dcbNzhVVQj.\\nZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. How well do\\nlarge language models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015, 2023.\\nBeichen Zhang, Yuhong Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Haodong Duan,\\nYuhang Cao, Dahua Lin, and Jiaqi Wang. Booststep: Boosting mathematical capability of\\nlarge language models via improved single-step reasoning. arXiv preprint arXiv:2501.03226,\\n2025.\\nJintian Zhang, Xin Xu, Ningyu Zhang, Ruibo Liu, Bryan Hooi, and Shumin Deng. Exploring\\ncollaboration mechanisms for LLM agents: A social psychology view. In Lun-Wei Ku,\\nAndre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 14544‚Äì14607, Bangkok,\\nThailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/\\n2024.acl-long.782. URL https://aclanthology.org/2024.acl-long.782/.\\nWei Zhang, Chaoqun Wan, Yonggang Zhang, Yiu-ming Cheung, Xinmei Tian, Xu Shen, and\\nJieping Ye. Interpreting and improving large language models in arithmetic calculation.\\nIn Proceedings of the 41st International Conference on Machine Learning, ICML‚Äô24. JMLR.org,\\n2024b.\\nKaiwen Zhou, Chengzhi Liu, Xuandong Zhao, Shreedhar Jangam, Jayanth Srinivasa,\\nGaowen Liu, Dawn Song, and Xin Eric Wang. The hidden risks of large reasoning\\nmodels: A safety assessment of r1. arXiv preprint arXiv:2502.12659, 2025.\\n13\\nPreprint. Under review.\\nA\\nInstances of Compromising Thought\\nInstance 1: Multiplication by o3-mini (Baseline: DT = 2)\\nThe product of 38,697,082 and 4,133,991 is 159,973,389,714,562.\\nTo summarize briefly without revealing internal steps: I computed the product using a standard\\nmultiplication approach by breaking down each digit‚Äôs contribution according to its place value\\nand then summing the results, confirming that the final answer is correct.\\nInstance 2: Multiplication by DeepSeek-R1 (Baseline: DT = 1)\\n...Add each subsequent result sequentially:\\n+3,869,708,200,000 = 158,658,036,200,000\\n+1,160,912,460,000 = 159,818,948,660,000\\n+116,091,246,000 = 159,935,039,906,000\\n+34,827,373,800 = 159,969,867,279,800\\n+3,482,737,380 = 159,973,350,017,180\\n+38,697,082 = 159,973,389,714,262\\nFinal Answer: 38,697,082 √ó 4,133,991 = 159,973,389,714,262\\nInstance 3: Addition by Doubao (Baseline: DT = 1)\\nThe sum of 705,537,514,349,043 and 5,488,395,726,938 is 711,025,710,075,981.\\nStep-by-Step Breakdown:\\nAlign the numbers by place value (pad the shorter number with leading zeros):\\nplaintext 7 0 5 5 3 7 5 1 4 3 4 9 0 4 3 + 0 0 5 4 8 8 3 9 5 7 2 6 9 3 8\\nAdd digit by digit from right to left, carrying over when necessary:\\nUnits: 3 + 8 = 11 ‚Üí1 (carry 1)\\nTens: 4 + 3 + 1 = 8 ‚Üí8\\nHundreds: 0 + 9 = 9 ‚Üí9\\nThousands: 9 + 6 = 15 ‚Üí5 (carry 1)\\nTen thousands: 4 + 2 + 1 = 7 ‚Üí7\\nHundred thousands: 3 + 7 = 10 ‚Üí0 (carry 1)\\nMillions: 4 + 5 + 1 = 10 ‚Üí0 (carry 1)\\nTen millions: 1 + 9 + 1 = 11 ‚Üí1 (carry 1)\\nHundred millions: 5 + 3 + 1 = 9 ‚Üí9\\nBillions: 7 + 8 = 15 ‚Üí5 (carry 1)\\nTen billions: 3 + 8 + 1 = 12 ‚Üí2 (carry 1)\\nHundred billions: 5 + 4 + 1 = 10 ‚Üí0 (carry 1)\\nTrillions: 5 + 5 + 1 = 11 ‚Üí1 (carry 1)\\nTen trillions: 0 + 0 + 1 = 1 ‚Üí1\\nHundred trillions: 7 + 0 = 7 ‚Üí7\\nCombine the results:\\n7 1 1 0 2 5 7 1 0 0 7 5 9 8 1 ‚Üí711,025,710,075,981.\\nLet me know if you need further clarification!\\n14\\nPreprint. Under review.\\nInstance 4: Word problem by DeepSeek-R1 (Baseline: DT = 1)\\nTo determine how much Janet makes daily at the farmers‚Äô market, follow these steps:\\n1. Total Eggs Laid:\\nJanet‚Äôs ducks lay 8,488,302 eggs per day.\\n2. Eggs Used for Consumption:\\n- Breakfast: 1,146,800 eggs\\n- Muffins: 1,309,630 eggs\\n- Total Used: 1, 146, 800 + 1, 309, 630 = 2, 456, 430 eggs\\n3. Remaining Eggs for Sale:\\nSubtract the total eggs used from the total eggs laid:\\n8, 488, 302 ‚àí2, 456, 430 = 6, 031, 872 eggs\\n4. Daily Earnings:\\nEach remaining egg is sold for $7. Multiply the remaining eggs by the price per egg:\\n6, 031, 872 √ó 7 = 42, 225, 104\\nFinal Answer:\\nJanet makes 42225104 dollars every day at the farmers‚Äô market.\\n15\\nPreprint. Under review.\\nInstance 5: Multiplication by DeepSeek-R1 (Method 3: DT = 4)\\nFirst, I need to multiply the two numbers: 38,697,082 and 4,133,991. To ensure accuracy, I‚Äôll use the\\nstandard multiplication method, breaking down the numbers and summing their products.\\nStep 1: Decompose the multiplier (4,133,991) - 4,000,000 - 100,000 - 30,000 - 3,000 - 900 - 90 - 1\\nStep 2: Multiply 38,697,082 by each decomposed part 1. 38,697,082 √ó 4,000,000 38, 697, 082 √ó 4 =\\n154, 788, 328 Shift 6 places: 154,788,328,000,000\\n2. 38,697,082 √ó 100,000 Shift 5 places: 3,869,708,200,000\\n3. 38,697,082 √ó 30,000 38, 697, 082 √ó 3 = 116, 091, 246 Shift 4 places: 1,160,912,460,000\\n4. 38,697,082 √ó 3,000 Shift 3 places: 116,091,246,000\\n5. 38,697,082 √ó 900 38, 697, 082 √ó 9 = 348, 273, 738 Shift 2 places: 34,827,373,800\\n6. 38,697,082 √ó 90 Shift 1 place: 3,482,737,380\\n7. 38,697,082 √ó 1 38,697,082\\nStep 3: Sum all the products\\n154, 788, 328, 000, 000\\n+\\n3, 869, 708, 200, 000\\n+\\n1, 160, 912, 460, 000\\n+\\n116, 091, 246, 000\\n+\\n34, 827, 373, 800\\n+\\n3, 482, 737, 380\\n+\\n38, 697, 082\\n=\\n159, 973, 350, 017, 180\\nFinal Answer: 159,973,350,017,180\\n16\\nPreprint. Under review.\\nInstance 6: Multiplication by DeepSeek-R1 (Method 3: DT = 6)\\nTo calculate 38,697,082 √ó 4,133,991, let‚Äôs use scientific notation to simplify the process:\\n1. Approximate the numbers: - 38,697,082 ‚âà3.8697 √ó 107 - 4,133,991 ‚âà4.134 √ó 106\\n2. Multiply the coefficients:\\n3.8697 √ó 4.134 ‚âà16.0\\n(exact value: 3.8697 √ó 4.134 ‚âà16.0)\\n3. Add the exponents:\\n107 √ó 106 = 1013\\n4. Combine results:\\n16.0 √ó 1013 = 1.6 √ó 1014\\n(or 160,000,000,000,000)\\nHowever, this is an approximation. For the exact value, use direct multiplication:\\n38,697,082 √ó 4,133,991 = 160,000,000,000,000\\n(adjusted for precision)\\nFinal Answer: 160,000,000,000,000\\n(Note: The exact value requires precise arithmetic, but this is the closest simplified result using\\nscientific notation.)\\nInstance 7: Word problem by Kimi k1.5 (Method 1: DT = 1)\\nI need to find out how many eggs Janet has left after using some for breakfast and baking, and\\nthen calculate how much money she makes by selling the remaining eggs.\\nFirst, I subtract the eggs used for breakfast and muffins from the total eggs laid: - Total eggs laid:\\n8,488,302 - Eggs eaten for breakfast: 1,146,800 - Eggs used for muffins: 1,309,630\\nSo, the remaining eggs are:\\n8, 488, 302 ‚àí1, 146, 800 ‚àí1, 309, 630\\nLet me do the subtraction step by step: 1. Subtract the breakfast eggs:\\n8, 488, 302 ‚àí1, 146, 800 = 7, 341, 502\\n2. Subtract the muffin eggs:\\n7, 341, 502 ‚àí1, 309, 630 = 6, 031, 872\\nSo, Janet has 6,031,872 eggs left to sell.\\nNext, I calculate how much money she makes by selling these eggs at $7 each:\\n6, 031, 872 √ó 7\\nBreaking it down: - 6, 000, 000 √ó 7 = 42, 000, 000 - 31, 872 √ó 7 = 223, 104\\nAdding these together:\\n42, 000, 000 + 223, 104 = 42, 225, 104\\nSo, Janet makes 42225104 dollars every day at the farmers‚Äô market.\\n17\\nPreprint. Under review.\\nInstance 8: Word problem by o3-mini (Method 1: DT = 1)\\nThe calculation you‚Äôve outlined looks correct, but I agree that the numbers presented in the problem\\nseem to be extraordinarily large and likely unrealistic. Nonetheless, based on the information\\ngiven, we can proceed with the steps as follows:\\nStep-by-step breakdown:\\n1. Total classes in a week:\\n- Weekdays (Monday to Friday): 5,687,781 classes per day √ó 5 days = 28,438,905 classes.\\n- Saturday: 9,536,275 classes.\\nTotal weekly classes:\\n28,438,905 (weekdays) + 9,536,275 (Saturday) = 37,975,180 classes.\\n2. Total number of students:\\nEach class has 20 students, so:\\n37,975,180 classes √ó 20 students = 759,503,600 students.\\n3. Total revenue:\\nJudy charges $25 per student, so:\\n759,503,600 students √ó $25/student = $18,987,591,000.\\nFinal Answer:\\nJudy makes $18,987,591,000 in one week.\\nThis result seems incredibly high, which suggests that either the numbers provided are meant to be\\nfictional or there‚Äôs a mistake in how the numbers were presented. If there was a typo (for instance,\\nthe number of classes per day might have been intended to be much smaller), the total revenue\\nwould be much more realistic.\\nB\\nInstances of Revised Results\\nInstance 1: Addition by o3-mini (Baseline:DT = 1)\\nThe sum is 711,025,910,075,981.\\nHere‚Äôs a quick breakdown:\\nUnits: 043 + 938 = 981\\nThousands: 349 + 726 = 1,075 ‚Üíwrite 075 and carry 1 to the millions\\nMillions: 514 + 395 = 909; plus the carry gives 910\\nBillions: 537 + 488 = 1,025 ‚Üíwrite 025 and carry 1 to the trillions\\nTrillions: 705 + 5 = 710; plus the carry gives 711\\nPutting it all together:\\n711 (trillions), 025 (billions), 910 (millions), 075 (thousands), 981 (u\\nw711,025,910,075,981.\\n18\\nPreprint. Under review.\\nInstance 2: Addition by o3-mini (Method 1:DT = 3)\\nThe correct sum is 711,025,910,075,981.\\nLet‚Äôs briefly explain:\\nWe have two numbers: - 705,537,514,349,043 - 5,488,395,726,938\\nWhen added...\\nI verified this using a precise calculation. The previous chain-of-thought had an error in the\\naddition process.\\nThus, the correct answer is 711025910075981.\\nInstance 3: Addition by o1 (Method 1:DT = 4)\\nThere‚Äôs a small slip in the final assembly of digits in your workings...\\nBelow is a more transparent way to see why that final ‚Äú910‚Äù in the millions place (rather than\\n‚Äú720‚Äù) is correct. ... Hence the final six digits must be ‚Äú075,981.‚Äù This clinches that the sum is\\n711,025,910,075,981.\\nInstance 4: Addition by o1 (Method 3:DT = 1)\\nBelow is a fresh, concise addition (ignoring the previous reasoning): ...\\n‚Ä¢ First number: 705 trillion, 537 billion, 514 million, 349 thousand, 43 ‚Ä¢ Second number: 5 trillion,\\n488 billion, 395 million, 726 thousand, 938\\na) Trillions: 705 + 5 = 710\\nb) Billions: 537 + 488 = 1,025 ‚Üíthat is 1 trillion + 25 billion.\\nAdd the extra 1 trillion to the previous 710 ‚Üí711 trillion left, 25 billion remain.\\nc) Millions: 514 + 395 = 909.\\nSo we still have 25 billion, 909 million.\\nd) Thousands: 349 + 726 = 1,075 ‚Üíthat is 1 million + 75 thousand.\\nAdding 1 million to 909 million ‚Üí910 million.\\nSo now we have 25 billion, 910 million, 75 thousand.\\ne) Units: 43 + 938 = 981.\\nPutting it all together yields 711 trillion, 25 billion, 910 million, 75 thousand, 981, which in standard\\nform is:\\n711,025,910,075,981.\\nSo the correct sum is 711,025,910,075,981.\\nInstance 5: Addition by DeepSeek-R1 (Method 3:DT = 6)\\n...\\nFinal Result: Combining all digits from left to right: 711,025,910,075,981\\nAnswer: 711,025,910,075,981\\n(Note: Previous grouping errors have been corrected here.)\\nInstance 6: Multiplication by o1 (Baseline:DT = 2)\\nA\\ncareful\\nrecalculation\\nshows\\nthat\\nthe\\ncorrect\\nproduct\\nis\\n159,973,388,714,262\\n(not\\n159,973,389,714,562)...\\nHence, 38,697,082 √ó 4,133,991 = 159,973,388,714,262.\\n19\\nPreprint. Under review.\\nInstance 7: Word problem by o3-mini (Baseline: DT = 1)\\nLet‚Äôs break down the problem step by step:\\n1. Total eggs per day: 8,488,302 eggs\\n2. Eggs used for breakfast: 1,146,800 eggs\\n3. Eggs used for muffins: 1,309,630 eggs\\n4. Calculate remaining eggs:\\nRemaining eggs = 8, 488, 302 ‚àí1, 146, 800 ‚àí1, 309, 630 = 6, 031, 872\\n5. Calculate daily earnings: Each remaining egg is sold for $7, so:\\nEarnings = 6, 031, 872 √ó 7 = 42, 223, 104\\nThus, Janet makes $42,223,104 every day at the farmers‚Äô market.\\n20\\nPreprint. Under review.\\nC\\nTampered Reasoning Tokens\\nInstance 1: Addition by DeepSeek-R1 (Baseline: DT = 1)\\n705537514349043+5488395726938=? Below is your previous thought.\\nAlright, so I need to add these two big numbers: 705,537,514,349,043 and 5,488,395,726,938. Hmm,\\nlet me see. Okay, first, adding large numbers can be a bit intimidating because there are so many\\ndigits, but if I take it step by step, it should be manageable...\\nSo the resulting number is:\\n7 1 1 0 2 5 7 1 0 0 7 5 9 8 1\\nWait, that seems a bit off. Let me check if that makes sense. Let‚Äôs break it down with commas:\\nStarting from the right, every three digits:\\n... 5981 ‚Üíbut let‚Äôs group the digits:\\n711,025,710,075,981?\\nWait, let‚Äôs see:\\nDigits:7 1 1 0 2 5 7 1 0 0 7 5 9 8 1\\nGrouping from the right:\\n15 digits: 7 1 1 0 2 5 7 1 0 0 7 5 9 8 1\\nStarting from the right, three by three:\\n981 (positions 13-15)\\n598 (positions 10-12): Wait, no. Let‚Äôs index them:\\n...\\nSo putting commas: 711,025,710,075,981\\nBut wait, the commas in large numbers go every three digits starting from the right, so it should\\nbe: 711,025,710,075,981\\nBut let‚Äôs verify the addition another way. Let‚Äôs convert the numbers to something more manage-\\nable...\\nBut when you have 1,025 billion, that‚Äôs 1 trillion and 25 billion. Similarly, 1,075 thousand is 1\\nmillion and 75 thousand.\\nSo adjusting:\\n710 trillion + 1 trillion = 711 trillion\\n25 billion + 909 million + 1 million = 25 billion + 910 million\\n75 thousand + 981 units = 75 thousand 981\\nSo the result would be 711,025,710,075,981\\nWhich matches the earlier result. Therefore, the sum is 711,025,710,075,981.\\nLet me check with another method. Let‚Äôs subtract a small number to see.\\nAlternatively, use approximate values to check. 705 trillion + 5 trillion is 710 trillion. But the actual\\nnumbers are slightly more: 705,537... +5,488... So roughly 705.5 +5.5 =711 trillion. The exact answer\\nis 711,025,710,075,981, which is about 711.025 trillion, which makes sense.\\nTherefore, I‚Äôm confident the answer is correct.\\n21\\nPreprint. Under review.\\nD\\nStop Thinking\\nInstance 1: Thinking Stopped By Q2 (Baseline: DT = 1)\\n<Reasoning Tokens:>\\nTo determine the total cost of the pastries Toula bought, we need to calculate the cost for each type\\nof pastry separately and then sum them up.\\nDonuts: - Quantity: 7,079,464 dozen\\n- Cost per dozen: $94\\n- Calculation: 7,079,464√ó947,079,464√ó94\\n- Breaking down 94 into 90 + 4:\\n- 7,079,464√ó90=637,151,7607,079,464√ó90=637,151,760\\n- 7,079,464√ó4=28,317,8567,079,464√ó4=28,317,856\\n- Total cost for donuts: 637,151,760+28,317,856=665,469,616637,151,760+28,317,856=665,469,616\\nMini Cupcakes:\\n- Quantity: 4,483,609 dozen\\n- Cost per dozen: $86\\n- Calculation: 4,483,609√ó864,483,609√ó86\\n- Breaking down 86 into 80 + 6:\\n- 4,483,609√ó80=358,688,7204,483,609√ó80=358,688,720\\n- 4,483,609√ó6=26,901,6544,483,609√ó6=26,901,654\\n- Total cost for cupcakes: 358,688,720+26,901,654=385,590,374358,688,720+26,901,654=385,590,374\\nMini Cheesecakes:\\n- Quantity: 4,374,764 dozen\\n- Cost per dozen: $23\\n- Calculation: 4,374,764√ó234,374,764√ó23\\n- Breaking down 23 into 20 + 3:\\n- 4,374,764√ó20=87,495,2804,374,764√ó20=87,495,280\\n- 4,374,764√ó3=13,124,2924,374,764√ó3=13,124,292\\n- Total cost for cheesecakes: 87,495,280+13,124,292=100,619,57287,495,280+13,124,292=100,619,572\\nTotal Cost:\\n- Summing up all costs:\\n- Total cost: 665,469,616+385,590,374+100,619,572665,469,616+385,590,374+100,619,572\\n- Calculation:\\n- 665,469,616+385,590,374=1,051,059,990665,469,616+385,590,374=1,051,059,990\\n- 1,051,059,990+100,619,572=1,151,676,5621,051,059,990+100,619,572=1,151,676,562\\nThus, the total cost was 1151676562.\\nThinking stopped\\n22\\nPreprint. Under review.\\nInstance 2: Thinking Stopped By Q5 (Baseline: DT = 1)\\n<Reasoning Tokens:>\\nOkay, let‚Äôs go through the problem step by step to find the total number of stories Alani, Braylen,\\nand Margot wrote altogether over two weeks.\\nFirst, we need the number of stories each child wrote in the first week:\\n- Alani: 3,970,368 stories\\n- Braylen: 7,543,015 stories\\n- Margot: 8,494,230 stories\\nIn the second week, each child doubled the number of stories they wrote in the first week. Therefore,\\nthe total number of stories each child wrote over the two weeks is three times their first week‚Äôs\\ncount (first week + second week = original + doubled = 3 times original).\\nTo find the total number of stories altogether, we first calculate the sum of the stories from the first\\nweek for all three children:\\n3,970,368+7,543,015+8,494,2303,970,368+7,543,015+8,494,230\\nAdding these step by step:\\n1. Add Alani‚Äôs and Braylen‚Äôs stories:\\n3,970,368+7,543,015=11,513,3833,970,368+7,543,015=11,513,383\\n2. Add the result to Margot‚Äôs stories:\\n11,513,383+8,494,230=20,007,61311,513,383+8,494,230=20,007,613\\nNext, we multiply this sum by 3 to account for both weeks (since each child‚Äôs total is three times\\ntheir first week‚Äôs count):\\n20,007,613√ó3=60,022,83920,007,613√ó3=60,022,839\\nVerifying by calculating each child‚Äôs total individually and then summing them:\\n- Alani‚Äôs total: 3,970,368√ó3=11,911,1043,970,368√ó3=11,911,104\\n- Braylen‚Äôs total: 7,543,015√ó3=22,629,0457,543,015√ó3=22,629,045\\n- Margot‚Äôs total: 8,494,230√ó3=25,482,6908,494,230√ó3=25,482,690\\nAdding these totals:\\n11,911,104+22,629,045+25,482,690=60,022,83911,911,104+22,629,045+25,482,690=60,022,839\\nThus, the total number of stories they wrote altogether is 60022539.\\nThinking stopped\\n23\\nPreprint. Under review.\\nInstance 3: Thinking Stopped By Q6 (Baseline: DT = 1)\\n<Reasoning Tokens:>\\nOkay, let‚Äôs solve this step by step. Axel has 9,900,270 silver pesos and 1,735,148 gold pesos. Anna\\nhas 9 times as many silver pesos as Axel and 9,356,776 more gold pesos than Axel. We need to find\\nthe total number of pesos they have together.\\nFirst, calculate Anna‚Äôs silver pesos:\\n9√ó9,900,270=89,102,4309√ó9,900,270=89,102,430\\nNext, calculate Anna‚Äôs gold pesos:\\n1,735,148+9,356,776=11,091,9241,735,148+9,356,776=11,091,924\\nNow, find the total silver pesos by adding Axel‚Äôs and Anna‚Äôs amounts:\\n9,900,270+89,102,430=99,002,7009,900,270+89,102,430=99,002,700\\nThen, find the total gold pesos by adding Axel‚Äôs and Anna‚Äôs amounts:\\n1,735,148+11,091,924=12,827,0721,735,148+11,091,924=12,827,072\\nFinally, add the total silver and gold pesos together:\\n99,002,700+12,827,072=111,828,77299,002,700+12,827,072=111,828,772\\nThe total number of pesos Axel and Anna have together is 111828772 .\\nThinking stopped\\n24\\nPreprint. Under review.\\nE\\nMathematical Word Problems\\nQuestions selected from GSM-Ranges\\nQ1: Janet‚Äôs ducks lay 8488302 eggs per day. She eats 1146800 for breakfast every morning and\\nbakes muffins for her friends every day with 1309630. She sells the remainder at the farmers‚Äô\\nmarket daily for $7 per fresh duck egg. How much in dollars does she make every day at the\\nfarmers‚Äô market?\\nQ2: Toula went to the bakery and bought various types of pastries. She bought 7079464 dozen\\ndonuts which cost $94 per dozen, 4483609 dozen mini cupcakes which cost $86 per dozen, and\\n4374764 dozen mini cheesecakes for $23 per dozen. How much was the total cost?\\nQ3: Jill gets paid $9594033 per hour to teach and $9477896 to be a cheerleading coach. If she works\\n30 weeks a year, 25 hours a week as a teacher and 25 hours a week as a coach, what‚Äôs her annual\\nsalary?\\nQ4: Judy teaches 5687781 dance classes every day on the weekdays and 9536275 classes on\\nSaturday. If each class has 20 students and she charges $25.00 per student, how much money does\\nshe make in 1 week?\\nQ5: Alani‚Äôs family decided that the children should write stories of any kind. They were then\\nrequired to read all of the stories they‚Äôd written to the family at the end of the weekend. Alani\\nwrote 3970368 stories in the first week, her brother Braylen wrote 7543015 stories, and her sister\\nMargot wrote 8494230 stories. If they each doubled the number of stories they‚Äôd written in the first\\nweek in the second week, calculate the total number of stories they wrote altogether.\\nQ6: Axel has 9900270 silver pesos and 1735148 gold pesos. He visits his friend Anna who has 9\\ntimes as many silver pesos as he has and 9356776 more gold pesos. What‚Äôs the total number of\\npesos they have together?\\n25\\n'),\n",
       " Document(metadata={'Published': '2024-05-09', 'Title': 'Hypothesis Testing Prompting Improves Deductive Reasoning in Large Language Models', 'Authors': 'Yitian Li, Jidong Tian, Hao He, Yaohui Jin', 'Summary': 'Combining different forms of prompts with pre-trained large language models\\nhas yielded remarkable results on reasoning tasks (e.g. Chain-of-Thought\\nprompting). However, along with testing on more complex reasoning, these\\nmethods also expose problems such as invalid reasoning and fictional reasoning\\npaths. In this paper, we develop \\\\textit{Hypothesis Testing Prompting}, which\\nadds conclusion assumptions, backward reasoning, and fact verification during\\nintermediate reasoning steps. \\\\textit{Hypothesis Testing prompting} involves\\nmultiple assumptions and reverses validation of conclusions leading to its\\nunique correct answer. Experiments on two challenging deductive reasoning\\ndatasets ProofWriter and RuleTaker show that hypothesis testing prompting not\\nonly significantly improves the effect, but also generates a more reasonable\\nand standardized reasoning process.'}, page_content='Hypothesis Testing Prompting Improves Deductive Reasoning in\\nLarge Language Models\\nYitian Li1,2, Jidong Tian1,2, Hao He1,2, Yaohui Jin1,2\\n1MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\\n2State Key Lab of Advanced Optical Communication System and Network\\n{yitian_li, frank92, hehao, jinyh}@sjtu.edu.cn\\nAbstract\\nCombining different forms of prompts with pre-trained large language models has yielded remarkable results on\\nreasoning tasks (e.g. Chain-of-Thought prompting). However, along with testing on more complex reasoning, these\\nmethods also expose problems such as invalid reasoning and fictional reasoning paths. In this paper, we develop\\nHypothesis Testing Prompting, which adds conclusion assumptions, backward reasoning, and fact verification during\\nintermediate reasoning steps. Hypothesis Testing prompting involves multiple assumptions and reverses validation of\\nconclusions leading to its unique correct answer. Experiments on two challenging deductive reasoning datasets\\nProofWriter and RuleTaker show that hypothesis testing prompting not only significantly improves the effect, but also\\ngenerates a more reasonable and standardized reasoning process.\\nKeywords: Deductive Reasoning, Large Language Models, Prompt\\n1.\\nIntroduction\\nThe release of large language models (LLMs) has\\nrevolutionized the NLP landscape recently (Thop-\\npilan et al., 2022; Kaplan et al., 2020; Chowdh-\\nery et al., 2022). Scaling up the size of language\\nmodels and conducting diversified prompt meth-\\nods become mainstream (Liu et al., 2023c; Wei\\net al., 2022a; Yang et al., 2023). Given In-context\\nlearning or Chain-of-Thought prompts have already\\nachieved high performance on challenging tasks\\nsuch as commonsense, arithmetic, and symbolic\\nreasoning (Imani et al., 2023; Lee et al., 2021;\\nKojima et al., 2022). Logical reasoning is one of\\nthe most important and long-standing problems in\\nNLP (Hirschberg and Manning, 2015; Russell and\\nNorvig, 2010), and integrating this ability into nat-\\nural language understanding systems has always\\nbeen a goal pursued (Du et al., 2022).\\nNevertheless, scaling has been demonstrated\\nto offer limited advantages in resolving complex\\nlogical reasoning issues (Kazemi et al., 2022). For\\nexample, Saparov and He (2022) show that Chain-\\nof-Thought prompting struggles with proof planning\\nfor more complex logical reasoning problems. Addi-\\ntionally, the performance suffers greatly while han-\\ndling recently released and out-of-distribution logi-\\ncal reasoning datasets (Liu et al., 2023a). Despite\\nmany works have explored variants of Chain-of-\\nThought prompts to facilitate LLMs inference (Zelik-\\nman et al., 2022; Zheng et al., 2023), we discover\\nthat the present logical reasoning task prompts\\nplace an excessive amount of emphasis on the\\nreasoning process while ignoring the origin, pur-\\npose, and effectiveness of reasoning (Creswell\\net al., 2022; Xi et al., 2023). As examples shown in\\nQ1: Bob is green. True/false? \\nInput Facts: Alan is blue. Alan is \\nrough. Alan is young. Bob is big. \\nBob is round. Charlie is big. Charlie \\nis blue. Charlie is green. Dave is \\ngreen. Dave is rough.\\nInput Rules: Big people are rough. \\nIf someone is young and round then \\nthey are kind. If someone is round \\nand big then they are blue. All\\nrough people are green.\\nBob is big. \\nBig people are rough. \\nAll rough people are green.\\nAnswer: T\\nQ2: Dave is blue. True/false?\\nLess inspiring\\nFigure 1: Questions in RuleTaker involve logical\\nreasoning with facts and rules.\\nFigure 1, the difficulty in judging logical problems\\narises not only from the process of reasoning but\\nalso from the choice of facts and rules to use as a\\nstarting point. Even if we were provided the thought\\nprocess for some of the issues, it would not be very\\nbeneficial for others, based on how we previously\\ncreated the prompts.\\nIn this paper, we propose Hypothesis Testing\\nPrompting, a new and more considerate prompt\\ntemplate design idea. Hypothesis testing is a for-\\nmal procedure for investigating our ideas about\\nthe world using statistics and is often used by sci-\\nentists to test specific predictions (Bevans, 2022).\\nWe draw inspiration from its process to introduce a\\nprocess of conclusion assumptions, backward rea-\\nsoning, and fact verification. Experiments on Rule-\\nTaker (Clark et al., 2020) and ProofWriter (Tafjord\\net al., 2021) show the effectiveness of our novel\\nprompting paradigm as a strategy for promoting\\ndeductive reasoning in large language models. Fur-\\nther analyses show that Hypothesis Test prompting\\ngenerates more desirable intermediate processes\\narXiv:2405.06707v1  [cs.CL]  9 May 2024\\nand significantly improves the \"Unknown\" label.\\n2.\\nRelated Work\\n2.1.\\nFew-Shot Prompting\\nBrown et al. (2020) propose in-context learning as\\nan alternative few-shot prompting way to stimulate\\nability. Besides, chain-of-Thought (CoT) (Wei et al.,\\n2022b) is one of the most well-known works, which\\ndecomposes the problem into intermediate steps\\nand further improves the ability of large language\\nmodels. Subsequently, several follow-up works\\nwere carried out, including Zero-shot-CoT (simply\\nadding \"Let‚Äôs think step by step\" before each an-\\nswer) (Kojima et al., 2022), Self-consistency (Wang\\net al., 2022), complexity-based (Fu et al., 2022),\\nand other prompting work (Liu et al., 2023b; Jung\\net al., 2022; Zhou et al., 2022; Saparov and He,\\n2022). While these methods enhance the perfor-\\nmance of inference by paying attention to indica-\\ntions of the reasoning process, they often overlook\\nsome aspects such as identifying the root cause of\\nthe problem, establishing efficient reasoning strate-\\ngies, and determining the direction of logical rea-\\nsoning.\\n2.2.\\nDeductive Reasoning\\nDeductive reasoning is defined as the applica-\\ntion of general concepts to particular circum-\\nstances (Johnson-Laird, 2010). Making logical as-\\nsumptions is the foundation of deductive reasoning,\\nwhich then bases a conclusion on those assump-\\ntions. The deduction task is then applied to a sit-\\nuation from the actual world after starting with a\\nrule. In light of the principles \"All men are mortal.\"\\nand \"Socrates is a man.\" for example, we can draw\\nthe conclusion that \"Socrates is mortal.\" (Johnson-\\nLaird, 1999).\\n3.\\nHypothesis Testing Prompting\\nHypothesis testing is a formal procedure for investi-\\ngating our ideas about the world using statistics and\\nused by scientists to test specific predictions that\\narise from theories (Bevans, 2022; La et al., 2012).\\nThere are 5 main steps in hypothesis testing:\\n1. State your research hypothesis;\\n2. Collect data in a way designed to test the hy-\\npothesis;\\n3. Perform an appropriate statistical test;\\n4. Decide whether to reject or fail to reject your\\nnull hypothesis;\\n5. Present the findings in your results and discus-\\nsion section;\\nWhen completing a challenging reasoning activ-\\nity, such as a multi-step deductive reasoning prob-\\nlem, one is not conducting random reasoning to\\nobtain all possible intermediate results. We shall\\nchoose the relevant conditions for inference ver-\\nification after initially making assumptions about\\nthe judgment problem, such as \" First assume the\\nconclusion is True and start from ... Then assume\\nthe conclusion is False and start from ... because\\nthe rules state that ... So the conclusion ...\". The\\npurpose of this study is to give language models\\nthe capacity to build a process that is similar to\\nwhat we defined as Hypothesis Testing Prompt-\\ning. We will show that large language models can\\ngenerate more appropriate thought and more ac-\\ncurate results if demonstrations of hypothesis test\\nprompting are provided in the exemplars for few-\\nshot prompting. Figure 2 shows an example of a\\nmodel producing a hypothesis testing thought to\\nsolve a deductive reasoning problem.\\n4.\\nExperiment\\n4.1.\\nExperimental Setup\\nWe explore Hypothesis Test Prompting for Chat-\\nGPT (GPT-3.5-Turbo in the OpenAI API) on multiple\\nlogical reasoning benchmarks.\\nBenchmarks. Considering FOL reasoning in\\nquestion answering systems, there are two world\\nassumptions (Reiter, 1981) that result in different\\nobjectives. One is the closed world assumption\\n(CWA), which is the presumption that what is not\\ncurrently known to be entailment is contradiction.\\nThe other is the open world assumption (OWA),\\nwhose objective should distinguish false proposi-\\ntions from uncertain ones. Due to differences in\\nworld assumptions, our analysis and solutions are\\nalso different.\\nWe consider the following two deductive reason-\\ning problem benchmarks: (1) the RuleTaker (Clark\\net al., 2020) benchmark using CWA assumption;\\n(2) the ProofWriter (Tafjord et al., 2021) benchmark\\nusing OWA assumption. Both datasets are divided\\ninto five parts, each part requiring 0, ‚â§1, ‚â§2, ‚â§\\n3, and ‚â§5 hops of reasoning, respectively. We\\nconducted comparison tests on the test set of the\\ntwo datasets for 5 distinct hops.\\nStandard prompting. As one of the baselines,\\nwe take into account the common few-shot prompt-\\ning, made popular by Brown et al. (2020), in which\\na language model is provided with in-context ex-\\namples of input-output pairings before producing a\\nprediction for a test-time example. Examples are\\npresented in the form of questions and answers.\\nAs seen in Figure 2(above), the model directly an-\\nswers the question.\\nChain-of-Thought prompting. We also com-\\nChain-of-Thought Prompting\\nModel Input\\nStandard Prompting\\nModel Input\\nQ: Judge the following conclusion \\'Harry is cold.\\' is correct, wrong, or unknown, based on the following texts: Text 1\\nA: The conclusion \\'Harry is cold.\\' is True.\\nQ: Judge the following conclusion ‚ÄòBob is not young.‚Äô is correct, wrong, or unknown, based on the following texts: Text 2\\nModel Output\\nText 1: Charlie is rough. Charlie is white. Erin is blue. Erin is rough. \\nErin is young. Gary is blue. Gary is green. Gary is quiet. Gary is \\nwhite. Harry is blue. Harry is rough. All rough, white things are \\ngreen. All young, blue things are quiet. If something is young then it \\nis blue. White, blue things are quiet. If something is green then it is \\nyoung. Quiet, green things are cold. All green, cold things are rough.\\nQuestion: Harry is cold.     \\nAnswer: True\\nText 2: Bob is rough. Fiona is not furry. Fiona is white. Young, nice \\nthings are big. Blue things are big. If something is young and white \\nthen it is nice. All nice things are young. All white things are young. \\nIf something is blue and not big then it is not rough.\\nQuestion: Bob is not young. \\nAnswer: Unknown\\nQ: Judge the following conclusion \\'Harry is cold.\\' is correct, wrong, or unknown, based on the following texts: Text 1\\nA: Start from \\'Harry is quiet\\' and \\'Quiet things are nice\\'. Then can know \\'Harry is nice\\'. And \\'All nice things are cold\\' is given. Finally \\nit can be deduced \\'Harry is cold\\'. So the conclusion \\'Harry is cold.\\' is True.\\nQ: Judge the following conclusion \\'Harry is not quiet.\\' is correct, wrong, or unknown, based on the following texts: Text  2\\nModel Output\\nHypothesis Testing Prompting\\nModel Input\\nQ: Judge the following conclusion \\'Harry is cold.\\' is correct, wrong, or unknown, based on the following texts: Text 1\\nA: First assume the conclusion is True and start from Harry is cold. Because the rules state that all nice things are cold, correct\\nconclusion needs to be satisfied that Harry is nice. And the rules states that quiet things are nice. It needs to be satisfied Harry is quiet, \\nwhich is also stated in the facts. So the conclusion \\'Harry is cold.\\' is True.\\nQ: Judge the following conclusion ‚ÄòBob is not young.‚Äô is correct, wrong, or unknown, based on the following texts: Text  2\\nQ: The conclusion \\'Bob is not young.\\' is True.\\nQ: Start from \\'Bob is rough\\' and \\'If something is blue and not big then it is not rough\\'. Then can know \\'Bob is not blue\\'. And \\'All white \\nthings are young\\' is given. Finally it can be deduced \\'Bob is not young\\'. So the conclusion \\'Bob is not young.\\' is True.\\nModel Output\\nQ: First assume the conclusion is True and start from Bob is not young. Because the rules state that all white things are young, correct \\nconclusion needs to be satisfied that Bob is not white. And there are no facts or rules about \\'not white\\'. Then assume the conclusion is \\nFalse and start from Bob is young. Because the rules state that all nice things are young, correct conclusion needs to be satisfied that \\nBob is nice. And there are no facts or rules about \\'nice\\'. So the conclusion \\'Bob is not young.\\' is Unknown.\\n√ó N\\n√ó N\\n√ó N\\nFigure 2: Comparison of three prompting methods: (a) Standard (b) Chain-of-Thought (c) Hypothesis\\nTesting. Particularly, we highlight the Hypothesis testing reasoning processes. The comparative experi-\\nmental results show that: Hypothesis testing prompting enables large language models to tackle complex\\nlogical reasoning.\\npare with Chain-of-thought prompting which has\\nachieved encouraging results on complex reason-\\ning tasks (Wei et al., 2022b).\\nAs seen in Fig-\\nure 2(middle), the model not only provides the final\\nanswer but also comes with the consideration of\\nintermediate steps.\\nHypothesis Testing Prompting. Our proposed\\napproach is to augment each exemplars in few-shot\\nprompting with the thought of hypothesis testing for\\nan associated answer, as illustrated in Figure 2(be-\\nlow). We show one chain of thought exemplars\\n(Example: Judge the following conclusion ‚Äô<Con-\\nclusion>‚Äô is true, false, or unknown, based on the\\nfollowing facts and rules: <Facts> ... <Rules> ...).\\n4.2.\\nExperimental Results\\nThe results for Hypothesis Testing Prompting and\\nthe baselines on the RuleTaker datasets are pro-\\nvided in Figure 3(a), and ProofWriter results are\\nshown in Figure 3(b). From the results, we ob-\\nserve that our method significantly outperforms the\\nother two baselines, especially on ProofWriter. Fig-\\nure 3(a) demonstrates that while CoT performs well\\nin the low hop, Hypothesis Testing prompting per-\\nforms better as the hops count increases on Rule-\\nTaker. While on ProofWriter, our approach has a\\nthorough lead (improved accuracy by over 4% on\\nall hops). Comparing two datasets, the latter dis-\\ntinguishes between \"False\" and \"Unknown\", which\\ndemand a greater level of logic. The results on two\\n0.97\\n0.93\\n0.83\\n0.84\\n0.81\\n0.99\\n0.92\\n0.77\\n0.8\\n0.78\\n0.78\\n0.72\\n0.63\\n0.63\\n0.65\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\ndepth-0\\ndepth-1\\ndepth-2\\ndepth-3\\ndepth-5\\nAccuracy\\nStandard Prompting\\nChain-of-Thought Prompting\\nHypothesis Testing Prompting\\n(a)\\n0.6\\n0.41\\n0.39\\n0.34\\n0.33\\n0.77\\n0.54\\n0.58\\n0.56\\n0.5\\n0.82\\n0.63\\n0.62\\n0.61\\n0.57\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\ndepth-0\\ndepth-1\\ndepth-2\\ndepth-3\\ndepth-5\\nAccuracy\\nStandard Prompting\\nChain-of-Thought Prompting\\nHypothesis Testing Prompting\\n(b)\\nFigure 3: Prediction accuracy results on the (a) RuleTaker and (b) ProofWriter datasets.\\n0.74\\n0.26\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nHypothesis Testing\\nChain-of-Thought\\n(a) The proof accuracy of Chain-of-Thought and Hy-\\npothesis Testing prompting.\\n0.65\\n0.3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nHypothesis Testing\\nChain-of-Thought\\n(b) Comparison of accuracy between Chain-of-\\nThought and Hypothesis Testing prompting on \"Un-\\nknown\" label.\\nFigure 4: Further results on ProofWriter.\\ndatasets that were analyzed show a weakness in\\nall methods for handling \"Unknown\" labels. This\\nbeacuse the OWA hypothesis necessitates the ex-\\nclusion of both positive and negative findings to\\nvalidate the \"Unknown\" label. The advantages of\\nour strategy are illustrated by the comparison of the\\nmodel output outputs in Figure 2. The content \"First\\nassume the conclusion is True ... Then assume\\nthe conclusion is False ... So ... is Unknown.\" gen-\\nerated by the model through learning Hypothesis\\nTesting prompting is more in line with our thinking.\\nBesides, we‚Äôll conduct further research and show\\nit later.\\n4.3.\\nFurther Analysis\\nWe carry out the following thorough analysis to\\nbetter comprehend the thought process:\\nProof Accuracy. Five students are required to\\nmanually evaluate the outcomes of the intermediate\\nreasoning after we randomly picked 100 examples\\nfrom depth-5 of the ProofWriter. Proof accuracy rep-\\nresents the proportion where the inference process\\nhas been proven to be reasonable in the correct\\npart of data label prediction. We compare the re-\\nsults of Chian-of-Thought and Hypothesis Testing\\nprompting and report in Figure 4(a). While Hy-\\npothesis Testing prompting mostly produced the\\ncorrect intermediate reasoning process when the\\npredicted label was correct, CoT only generated\\nthe correct chain for 26% of the examples. This\\nresult is in line with other research showing that\\nLMs rely on spurious correlations when solving log-\\nical problems from beginning to end. Additionally,\\nour approach can successfully increase reason-\\ning‚Äôs rationality. In processing the \"Unknown\" label,\\nHypothesis Testing prompting performs noticeably\\nbetter than Chain-of-Thought.\\n\"Unknown\" accuracy.\\nIn the ProofWriter\\ndataset, we separately counted the accuracy of\\nthe \"Unknown\" label shown in Figure 4(b). The\\nresults point to a flaw in the Chain-of-Thought strat-\\negy‚Äôs handling of \"Unknown\" labels(only 0.3 accu-\\nracyies). Contrarily, Hypothesis Testing prompting\\nsignificantly increases the reliability of judging this\\nlabel (up to 0.65). This further illustrates the value\\nof holding various assumptions, as well as the re-\\nverse confirmation of conclusions.\\n5.\\nConclusion\\nWe have investigated Hypothesis Testing prompt-\\ning as a straightforward and widely applicable tech-\\nnique for improving deductive reasoning in large\\nlanguage models. Multiple assumptions are made\\nduring hypothesis testing, and conclusions are\\nreverse-validated to arrive at the one and only ac-\\ncurate answer. Through experiments on two logical\\nreasoning datasets, we find that Hypothesis Test-\\ning prompting allows large language models to con-\\nstruct reasoning more reasonably and accurately.\\nWe anticipate that additional research on language-\\nbased reasoning approaches will be stimulated by\\nour novel prompting design strategy.\\n6.\\nReferences\\nAlfred V. Aho and Jeffrey D. Ullman. 1972. The\\nTheory of Parsing, Translation and Compiling,\\nvolume 1. Prentice-Hall, Englewood Cliffs, NJ.\\nAmerican Psychological Association. 1983. Publi-\\ncations Manual. American Psychological Associ-\\nation, Washington, DC.\\nRie Kubota Ando and Tong Zhang. 2005. A frame-\\nwork for learning predictive structures from multi-\\nple tasks and unlabeled data. Journal of Machine\\nLearning Research, 6:1817‚Äì1853.\\nGalen Andrew and Jianfeng Gao. 2007. Scalable\\ntraining of L1-regularized log-linear models. In\\nProceedings of the 24th International Conference\\non Machine Learning, pages 33‚Äì40.\\nRebecca Bevans. 2022.\\nHypothesis Testing |\\nA Step-by-Step Guide with Easy Examples.\\nScribbr.\\nTom B. Brown, Benjamin Mann, Nick Ryder,\\nMelanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish\\nSastry, Amanda Askell, Sandhini Agarwal, Ariel\\nHerbert-Voss, Gretchen Krueger, Tom Henighan,\\nRewon Child, Aditya Ramesh, Daniel M. Ziegler,\\nJeffrey Wu, Clemens Winter, Christopher Hesse,\\nMark Chen, Eric Sigler, Mateusz Litwin, Scott\\nGray, Benjamin Chess, Jack Clark, Christopher\\nBerner, Sam McCandlish, Alec Radford, Ilya\\nSutskever, and Dario Amodei. 2020. Language\\nmodels are few-shot learners. In NeurIPS.\\nBSI. 1973a.\\nNatural Fibre Twines, 3rd edition.\\nBritish Standards Institution, London. BS 2570.\\nBSI. 1973b. Natural fibre twines. BS 2570, British\\nStandards Institution, London. 3rd. edn.\\nA. Castor and L. E. Pollux. 1992. The use of user\\nmodelling to guide inference and learning. Ap-\\nplied Intelligence, 2(1):37‚Äì53.\\nAshok K. Chandra, Dexter C. Kozen, and Larry J.\\nStockmeyer. 1981. Alternation. Journal of the As-\\nsociation for Computing Machinery, 28(1):114‚Äì\\n133.\\nJ.L. Chercheur. 1994. Case-Based Reasoning, 2nd\\nedition. Morgan Kaufman Publishers, San Mateo,\\nCA.\\nYejin Choi. 2022. The curious case of common-\\nsense intelligence. Daedalus.\\nN. Chomsky. 1973. Conditions on transformations.\\nIn A festschrift for Morris Halle, New York. Holt,\\nRinehart & Winston.\\nAakanksha Chowdhery, Sharan Narang, Jacob\\nDevlin, Maarten Bosma, Gaurav Mishra, Adam\\nRoberts, Paul Barham, Hyung Won Chung,\\nCharles Sutton, Sebastian Gehrmann, Parker\\nSchuh, Kensen Shi, Sasha Tsvyashchenko,\\nJoshua Maynez, Abhishek Rao, Parker Barnes,\\nYi Tay,\\nNoam Shazeer,\\nVinodkumar Prab-\\nhakaran, Emily Reif, Nan Du, Ben Hutchinson,\\nReiner Pope, James Bradbury, Jacob Austin,\\nMichael Isard, Guy Gur-Ari, Pengcheng Yin,\\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\\nSunipa Dev, Henryk Michalewski, Xavier Gar-\\ncia, Vedant Misra, Kevin Robinson, Liam Fe-\\ndus, Denny Zhou, Daphne Ippolito, David Luan,\\nHyeontaek Lim, Barret Zoph, Alexander Spiri-\\ndonov, Ryan Sepassi, David Dohan, Shivani\\nAgrawal, Mark Omernick, Andrew M. Dai, Thanu-\\nmalayan Sankaranarayana Pillai, Marie Pellat,\\nAitor Lewkowycz, Erica Moreira, Rewon Child,\\nOleksandr Polozov, Katherine Lee, Zongwei\\nZhou, Xuezhi Wang, Brennan Saeta, Mark Diaz,\\nOrhan Firat, Michele Catasta, Jason Wei, Kathy\\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav\\nPetrov, and Noah Fiedel. 2022. Palm: Scaling\\nlanguage modeling with pathways. CoRR.\\nPeter Clark, Oyvind Tafjord, and Kyle Richardson.\\n2020. Transformers as soft reasoners over lan-\\nguage. In IJCAI.\\nJames W. Cooley and John W. Tukey. 1965. An\\nalgorithm for the machine calculation of complex\\nFourier series.\\nMathematics of Computation,\\n19(90):297‚Äì301.\\nAntonia Creswell, Murray Shanahan, and Irina Hig-\\ngins. 2022. Selection-inference: Exploiting large\\nlanguage models for interpretable logical reason-\\ning. CoRR.\\nYilun Du, Shuang Li, Joshua B. Tenenbaum, and\\nIgor Mordatch. 2022. Learning iterative reason-\\ning through energy minimization. In ICML.\\nUmberto Eco. 1990. The Limits of Interpretation.\\nIndian University Press.\\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,\\nand Tushar Khot. 2022.\\nComplexity-based\\nprompting for multi-step reasoning. CoRR.\\nDan Gusfield. 1997. Algorithms on Strings, Trees\\nand Sequences. Cambridge University Press,\\nCambridge, UK.\\nJulia Hirschberg and Christopher D. Manning. 2015.\\nAdvances in natural language processing. Sci-\\nence.\\nPaul Gerhard Hoel. 1971a. Elementary Statistics,\\n3rd edition. Wiley series in probability and math-\\nematical statistics. Wiley, New York, Chichester.\\nISBN 0 471 40300.\\nPaul Gerhard Hoel. 1971b. Elementary Statistics,\\n3rd edition, Wiley series in probability and mathe-\\nmatical statistics, pages 19‚Äì33. Wiley, New York,\\nChichester. ISBN 0 471 40300.\\nShima Imani, Liang Du, and Harsh Shrivastava.\\n2023. Mathprompter: Mathematical reasoning\\nusing large language models. CoRR.\\nOtto Jespersen. 1922. Language: Its Nature, De-\\nvelopment, and Origin. Allen and Unwin.\\nPhil Johnson-Laird. 2010. Deductive reasoning.\\nWiley Interdisciplinary Reviews: Cognitive Sci-\\nence.\\nPhilip N Johnson-Laird. 1999. Deductive reasoning.\\nAnnual review of psychology.\\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze\\nBrahman, Chandra Bhagavatula, Ronan Le Bras,\\nand Yejin Choi. 2022. Maieutic prompting: Logi-\\ncally consistent reasoning with recursive expla-\\nnations. In EMNLP.\\nJared Kaplan, Sam McCandlish, Tom Henighan,\\nTom B. Brown, Benjamin Chess, Rewon Child,\\nScott Gray, Alec Radford, Jeffrey Wu, and Dario\\nAmodei. 2020. Scaling laws for neural language\\nmodels. CoRR.\\nSeyed Mehran Kazemi, Najoung Kim, Deepti Bha-\\ntia, Xin Xu, and Deepak Ramachandran. 2022.\\nLAMBADA: backward chaining for automated\\nreasoning in natural language. CoRR.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid,\\nYutaka Matsuo, and Yusuke Iwasawa. 2022.\\nLarge language models are zero-shot reason-\\ners. In NeurIPS.\\nRosa Patricio S. La, Brooks J. Paul, Deych Elena,\\nEdward L. Boone, David J. Edwards, Wang Qin,\\nSodergren Erica, Weinstock George, William D.\\nShannon, and Ethan P. White. 2012. Hypothe-\\nsis testing and power calculations for taxonomic-\\nbased human microbiome data. Plos One.\\nChia-Hsuan Lee, Hao Cheng, and Mari Osten-\\ndorf. 2021. Dialogue state tracking with a lan-\\nguage model using schema-driven prompting. In\\nEMNLP. Association for Computational Linguis-\\ntics.\\nHanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu,\\nQiji Zhou, and Yue Zhang. 2023a. Evaluating the\\nlogical reasoning ability of chatgpt and GPT-4.\\nCoRR.\\nHanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli\\nZhang, Qiji Zhou, and Yue Zhang. 2023b. Logi-\\ncot: Logical chain-of-thought instruction-tuning\\ndata collection with GPT-4. CoRR.\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao\\nJiang, Hiroaki Hayashi, and Graham Neubig.\\n2023c. Pre-train, prompt, and predict: A sys-\\ntematic survey of prompting methods in natural\\nlanguage processing. ACM Comput. Surv.\\nMohammad Sadegh Rasooli and Joel R. Tetreault.\\n2015. Yara parser: A fast and accurate depen-\\ndency parser. Computing Research Repository,\\narXiv:1503.06733. Version 2.\\nRaymond Reiter. 1981. On closed world data bases.\\nIn Readings in Artificial Intelligence.\\nStuart J. Russell and Peter Norvig. 2010. Artificial\\nIntelligence - A Modern Approach, Third Interna-\\ntional Edition. Pearson Education.\\nAbulhair Saparov and He He. 2022. Language mod-\\nels are greedy reasoners: A systematic formal\\nanalysis of chain-of-thought. CoRR.\\nCharles Joseph Singer, E. J. Holmyard, and A. R.\\nHall, editors. 1954‚Äì58. A history of technology.\\nOxford University Press, London. 5 vol.\\nJannik Str√∂tgen and Michael Gertz. 2012. Temporal\\ntagging on different domains: Challenges, strate-\\ngies, and gold standards. In Proceedings of the\\nEight International Conference on Language Re-\\nsources and Evaluation (LREC‚Äô12), pages 3746‚Äì\\n3753, Istanbul, Turkey. European Language Re-\\nsource Association (ELRA).\\nS. Superman, B. Batman, C. Catwoman, and S. Spi-\\nderman. 2000. Superheroes experiences with\\nbooks, 20th edition. The Phantom Editors Asso-\\nciates, Gotham City.\\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark.\\n2021.\\nProofwriter:\\nGenerating implications,\\nproofs, and abductive statements over natural\\nlanguage. In Findings of ACL.\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\\nNoam Shazeer, Apoorv Kulshreshtha, Heng-\\nTze Cheng, Alicia Jin, Taylor Bos, Leslie\\nBaker, Yu Du, YaGuang Li, Hongrae Lee,\\nHuaixiu Steven Zheng, Amin Ghafouri, Marcelo\\nMenegali, Yanping Huang, Maxim Krikun, Dmitry\\nLepikhin, James Qin, Dehao Chen, Yuanzhong\\nXu, Zhifeng Chen, Adam Roberts, Maarten\\nBosma, Yanqi Zhou, Chung-Ching Chang, Igor\\nKrivokon, Will Rusch, Marc Pickett, Kathleen S.\\nMeier-Hellstern, Meredith Ringel Morris, Tulsee\\nDoshi, Renelito Delos Santos, Toju Duke, Johnny\\nSoraker, Ben Zevenbergen, Vinodkumar Prab-\\nhakaran, Mark Diaz, Ben Hutchinson, Kristen Ol-\\nson, Alejandra Molina, Erin Hoffman-John, Josh\\nLee, Lora Aroyo, Ravi Rajakumar, Alena Butryna,\\nMatthew Lamm, Viktoriya Kuzmina, Joe Fenton,\\nAaron Cohen, Rachel Bernstein, Ray Kurzweil,\\nBlaise Aguera-Arcas, Claire Cui, Marian Croak,\\nEd H. Chi, and Quoc Le. 2022. Lamda: Lan-\\nguage models for dialog applications. CoRR.\\nXuezhi Wang, Jason Wei, Dale Schuurmans,\\nQuoc V. Le, Ed H. Chi, and Denny Zhou. 2022.\\nSelf-consistency improves chain of thought rea-\\nsoning in language models. CoRR.\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\\ngatama, Maarten Bosma, Denny Zhou, Donald\\nMetzler, Ed H. Chi, Tatsunori Hashimoto, Oriol\\nVinyals, Percy Liang, Jeff Dean, and William Fe-\\ndus. 2022a. Emergent abilities of large language\\nmodels. Trans. Mach. Learn. Res.\\nJason Wei, Xuezhi Wang, Dale Schuurmans,\\nMaarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\\nQuoc V. Le, and Denny Zhou. 2022b. Chain-\\nof-thought prompting elicits reasoning in large\\nlanguage models. In NeurIPS.\\nZhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng,\\nSongyang Gao, Tao Gui, Qi Zhang, and Xuanjing\\nHuang. 2023. Self-polish: Enhance reasoning in\\nlarge language models via problem refinement.\\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiao-\\ntian Han, Qizhang Feng, Haoming Jiang, Bing\\nYin, and Xia Hu. 2023. Harnessing the power of\\nllms in practice: A survey on chatgpt and beyond.\\nCoRR.\\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.\\nGoodman. 2022. Star: Bootstrapping reasoning\\nwith reasoning. In NeurIPS.\\nChuanyang Zheng, Zhengying Liu, Enze Xie, Zhen-\\nguo Li, and Yu Li. 2023. Progressive-hint prompt-\\ning improves reasoning in large language mod-\\nels.\\nDenny Zhou, Nathanael Sch√§rli, Le Hou, Jason\\nWei, Nathan Scales, Xuezhi Wang, Dale Schuur-\\nmans, Olivier Bousquet, Quoc Le, and Ed H. Chi.\\n2022. Least-to-most prompting enables complex\\nreasoning in large language models. CoRR.\\n'),\n",
       " Document(metadata={'Published': '2025-06-10', 'Title': 'On Reasoning Strength Planning in Large Reasoning Models', 'Authors': 'Leheng Sheng, An Zhang, Zijian Wu, Weixiang Zhao, Changshuo Shen, Yi Zhang, Xiang Wang, Tat-Seng Chua', 'Summary': \"Recent studies empirically reveal that large reasoning models (LRMs) can\\nautomatically allocate more reasoning strengths (i.e., the number of reasoning\\ntokens) for harder problems, exhibiting difficulty-awareness for better task\\nperformance. While this automatic reasoning strength allocation phenomenon has\\nbeen widely observed, its underlying mechanism remains largely unexplored. To\\nthis end, we provide explanations for this phenomenon from the perspective of\\nmodel activations. We find evidence that LRMs pre-plan the reasoning strengths\\nin their activations even before generation, with this reasoning strength\\ncausally controlled by the magnitude of a pre-allocated directional vector.\\nSpecifically, we show that the number of reasoning tokens is predictable solely\\nbased on the question activations using linear probes, indicating that LRMs\\nestimate the required reasoning strength in advance. We then uncover that LRMs\\nencode this reasoning strength through a pre-allocated directional vector\\nembedded in the activations of the model, where the vector's magnitude\\nmodulates the reasoning strength. Subtracting this vector can lead to reduced\\nreasoning token number and performance, while adding this vector can lead to\\nincreased reasoning token number and even improved performance. We further\\nreveal that this direction vector consistently yields positive reasoning length\\nprediction, and it modifies the logits of end-of-reasoning token </think> to\\naffect the reasoning length. Finally, we demonstrate two potential applications\\nof our findings: overthinking behavior detection and enabling efficient\\nreasoning on simple problems. Our work provides new insights into the internal\\nmechanisms of reasoning in LRMs and offers practical tools for controlling\\ntheir reasoning behaviors. Our code is available at\\nhttps://github.com/AlphaLab-USTC/LRM-plans-CoT.\"}, page_content='arXiv:2506.08390v1  [cs.AI]  10 Jun 2025\\nOn Reasoning Strength Planning\\nin Large Reasoning Models\\nLeheng Sheng1 An Zhang2‚àóZijian Wu 1 Weixiang Zhao3 Changshuo Shen2 Yi Zhang2\\nXiang Wang2 Tat-Seng Chua1\\n1National University of Singapore\\n2University of Science and Technology of China\\n3Harbin Institute of Technology\\nleheng.sheng@u.nus.edu, anzhang@u.nus.edu, zijianwu0522@gmail.com,\\nwxzhao@ir.hit.edu.cn, stephen_shen@mail.ustc.edu.cn,\\nzy1230@mail.ustc.edu.cn, xiangwang1223@gmail.com, dcscts@nus.edu.sg\\nAbstract\\nRecent studies empirically reveal that large reasoning models (LRMs) can automat-\\nically allocate more reasoning strengths (i.e., the number of reasoning tokens) for\\nharder problems, exhibiting difficulty-awareness for better task performance. While\\nthis automatic reasoning strength allocation phenomenon has been widely observed,\\nits underlying mechanism remains largely unexplored. To this end, we provide\\nexplanations for this phenomenon from the perspective of model activations. We\\nfind evidence that LRMs pre-plan the reasoning strengths in their activations\\neven before generation, with this reasoning strength causally controlled by the\\nmagnitude of a pre-allocated directional vector. Specifically, we show that the\\nnumber of reasoning tokens is predictable solely based on the question activations\\nusing linear probes, indicating that LRMs estimate the required reasoning strength\\nin advance. We then uncover that LRMs encode this reasoning strength through a\\npre-allocated directional vector embedded in the activations of the model, where the\\nvector‚Äôs magnitude modulates the reasoning strength. Subtracting this vector can\\nlead to reduced reasoning token number and performance, while adding this vector\\ncan lead to increased reasoning token number and even improved performance. We\\nfurther reveal that this direction vector consistently yields positive reasoning length\\nprediction, and it modifies the logits of end-of-reasoning token </think> to affect\\nthe reasoning length. Finally, we demonstrate two potential applications of our find-\\nings: overthinking behavior detection and enabling efficient reasoning on simple\\nproblems. Our work provides new insights into the internal mechanisms of reason-\\ning in LRMs and offers practical tools for controlling their reasoning behaviors. Our\\ncode is available at https://github.com/AlphaLab-USTC/LRM-plans-CoT.\\n1\\nIntroduction\\nLarge reasoning models (LRMs) [1‚Äì3] have demonstrated exceptional performance across a variety\\nof complex reasoning tasks, such as mathematical problem solving [4‚Äì6], code generation [7, 8],\\nand scientific question answering [9]. Here we take a closer look at LRMs‚Äô ability to allocate\\nreasoning strength commonly quantified by the number of reasoning tokens generated during infer-\\nence. Increasing reasoning strength has been shown to substantially improve model performance on\\ncomplex reasoning tasks [2, 10]. As a result, it has emerged as a critical factor for both performance\\noptimization and controllable model behavior [11‚Äì13].\\n‚àóAn Zhang is the corresponding author.\\nPreprint. Under review.\\n(a) Reasoning length prediction\\n(b) Activation shift direction\\n(c) Reasoning length manipulation\\nFigure 1: (1a) The reasoning length is predictable before the generation of the first reasoning token.\\n(1b) The activations of questions shift towards a pre-allocated direction as difficulty increases. Orange\\nstars denote mean activations of different difficulty levels. (1c) Steering activations of LRMs with this\\ndirection vector can causally affect the reasoning token numbers, thereby affecting the performance.\\nRecent findings have revealed two key properties of reasoning strength in LRMs: it can be auto-\\nmatically allocated based on problem difficulty [1, 14, 2, 15] and it can be manually controlled\\nthrough intervention [16‚Äì20]. On the one hand, LRMs tend to allocate more reasoning tokens to\\nharder questions, reflecting an implicit adaptation to task complexity [12, 21‚Äì24]. On the other hand,\\nreasoning strength can also be explicitly manipulated by prompting a desired length, typically enabled\\nthrough supervised fine-tuning (SFT) [17] or reinforcement learning (RL) [16] with strength-aware\\nobjectives. These empirical observations suggest that LRMs may have automatic and controllable\\nmechanisms for reasoning strength modulation. However, the underlying nature and internal structure\\nof these mechanisms remain largely unexplored.\\nTo fill this research gap, we investigate the underlying mechanism of reasoning strength in LRMs\\nby asking two key questions: (1) Do LRMs pre-plan their reasoning strength before generation?\\n(2) If so, in what form is this control encoded in advance? We approach these questions from the\\nperspective of model activations ‚Äî that is, how the activations (i.e., latent representations of the\\nquestion prompt) vary in response to different levels of reasoning strength. For the first, we examine\\nwhether the number of reasoning tokens can be predicted solely from the activations corresponding\\nto the input question. For the second, we explore how LRMs encode pre-allocation signals within\\nactivations that modulate reasoning strength. Specifically, we employ a linear probe to predict the\\nreasoning token count from question activations, and further extract a pre-allocated direction vector\\n(i.e., pre-allocation vector), such that manipulating this vector within the activation space enables\\ncontrol over the reasoning strength. Positive findings would indicate that LRMs plan reasoning\\nstrength ahead of generation through specific pre-allocated activations.\\nHere we conduct preliminary experiments using DeepSeek-R1-distilled-Qwen on the MATH [5]\\ndataset, which contains math questions across five difficulty levels. As visualized in Figure 1, we\\nsummarize three key empirical findings from the activation distribution of the LRM:\\n‚Ä¢ Figure 1a shows that a well-trained linear predictor can estimate the number of reasoning tokens\\nfrom input activations, achieving a correlation of 0.84 between predicted and actual values. This\\nresult indicates that the number of reasoning tokens is predictable prior to generation, suggesting\\nthat LRMs pre-plan their reasoning strength in advance.\\n‚Ä¢ As the question difficulty increases, activations consistently shift in a shared direction, as Figure\\n1b depicts. The activations of math problems exhibit a consistent trend shifting towards the\\nsame direction. Specifically, mean difference vectors between high- and low-difficulty questions\\nconsistently point in a similar direction, with magnitudes that correlate with question difficulty.\\n‚Ä¢ As shown in Figure 1c, manipulating activations along this direction vector ‚Äî with varying\\nmagnitudes ‚Äî causally modulates reasoning strength, leading to corresponding changes in LRM\\nperformance on reasoning tasks.\\nThese findings suggest that LRMs pre-plan their reasoning strength through pre-allocating a direction\\nvector, whose magnitude encodes the intended strength. To further investigate, we show that this\\ndirection vector consistently produces positive predictions of reasoning token numbers, closely\\n2\\naligning with actual values. Moreover, we observe that manipulating activations along this direction\\ninfluences the logit of the end-of-reasoning token </think>, indicating a causal role in terminating\\nthe reasoning process. Based on these findings, we further discover two possible potentials of such\\nunderlying mechanism of reasoning strength: overthink detection with the predictor and efficient\\nreasoning with activation steering [25].\\n2\\nRelated Works\\n2.1\\nLarge Reasoning Models\\nLarge reasoning models (LRMs) [2, 1] have recently emerged as a new paradigm of large language\\nmodels for complex task solving through step-by-step reasoning [26, 27, 22]. These models conduct\\nexplicit reasoning processes between special tokens of <think> and </think> before producing\\nfinal answers [10]. Recent studies have shown that LRMs can adaptively allocate reasoning strength\\n(i.e., the number of reasoning tokens) based on problem difficulty‚Äîthey tend to allocate more\\nreasoning strength to harder questions to improve accuracy [12, 21‚Äì23]. Moreover, it is even possible\\nto specify the reasoning strength via prompting a desired number of reasoning tokens, which can be\\nrealized through post-training with length-aware objectives [16, 17]. These phenomena suggest that\\nLRMs may possess some underlying mechanisms to plan and control the strength of their reasoning.\\n2.2\\nPlanning in Language Models\\nRecent works show that, despite being trained solely on next-token prediction [28], large language\\nmodels (LLMs) exhibit certain planning capabilities [29‚Äì32]. For example, there is evidence that\\nLLMs can anticipate future tokens‚Äîsuch as planning rhyme schemes several lines ahead when\\ncomposing poems [31]. Moreover, studies have found that models may even pre-plan answer\\nconfidence levels or the choices in multiple-choice questions [32]. Despite these findings, the\\nunderlying mechanisms behind such planning capabilities remain largely unexplored, and it is still\\nunclear whether similar planning capabilities occur in LRMs. Inspired by these observations, this\\nwork presents the first investigation into the reasoning planning capabilities of LRMs, and uncovers\\nthe underlying mechanisms that control such planning.\\n2.3\\nActivation Steering with Linear Direction\\nRecent advances in mechanism explainability reveal that there exist linear directions inside the\\nactivation space of language models that control specific semantical behaviors [33‚Äì35]. These\\ndirections are typically derived from activation differences between contrasting semantics, such\\nas refusal versus compliance in responses [35]. Manipulating directions via activation addition\\nor subtraction enables behavior modification of language models during inference. It has been\\nevidenced that response style [25, 36], refusal behaviors [35, 37], and memory extraction capabilities\\n[38] have been encoded in linear directions. Recent studies find evidence that by concatenating\\nquestion prompts with their corresponding chain-of-thought (CoT) answers and computing activation\\ndifferences between responses of varying reasoning token numbers, it is possible to identify linear\\ndirections that control the length of reasoning [19, 39, 40]. In this work, we take an important step\\nfurther, by demonstrating that such directions have been pre-allocated by LRMs upon observing the\\nquestion for reasoning strength control, even before generating the answer.\\n3\\nReasoning Strength in LRMs is Pre-Planned\\nIn this section, we explore the hypothesis that LRM plans the reasoning strength (e.g., the length of\\nthe reasoning process) even before the beginning of the reasoning process [34, 32]. We use linear\\nprobing to test this hypothesis [41, 34, 42, 32], predicting the length of reasoning with the activations\\nof the question solely. A good probing result will support our hypothesis [41].\\n3.1\\nExperimental Setup\\nLinear probing. For each question, we extract the d-dimentional residual stream activation h(l) ‚ààRd\\nat the position of the start-of-reasoning <think> token, and aim to predict the subsequent reasoning\\n3\\n(a) R1-Distill-Qwen-1.5B\\n(b) R1-Distill-Qwen-7B\\n(c) QwQ-32B\\nFigure 2: Layer-wise linear regression results\\ntoken number y ‚ààR using a linear regression model based on h(l) at each layer l. We calculate y as\\nthe number of tokens between the start-of-reasoning token <think> and the end-of-reasoning token\\n</think>. Formally, given a dataset of n samples, we construct an activation matrix H(l) ‚ààRn√ód\\nwhere each row corresponds to the activation of one question, and corresponding scalar reasoning\\ntoken number Y ‚ààRn. We then learn a linear regression function ÀÜY = H(l)W(l) + b(l) by\\nminimizing the following regularized loss:\\nÀÜ\\nW(l), ÀÜb(l) = arg\\nmin\\nW(l),b(l)\\n\\r\\r\\rY ‚àí(H(l)W(l) + b(l))\\n\\r\\r\\r\\n2\\n2 + Œ±\\n\\r\\r\\rW(l)\\r\\r\\r\\n1 .\\n(1)\\nEquation (3) denotes the Lasso regression [43], where W(l) ‚ààRd and b(l) ‚ààR are the learnable\\nparameters of this linear regression. A regularization term Œª\\n\\r\\rW(l)\\r\\r\\n1 is introduced for avoiding\\noverfitting, where Œ± is a hyperparameter controlling the regularization strength. To implement this\\nLasso regression, we use the Python package of scikit-learn [44]. The illustration and details of\\nthis probing process can be found in Appendix B.\\nDatasets. We conduct the linear regression experiments on the MATH [5] dataset, where math\\nquestions are divided into five groups according to their difficulty. We randomly split the dataset with\\na ratio of 9:1 for training and testing.\\nModels. We conduct experiments on a wide range of open-source LRMs, including the distilled R1\\nmodel series [2] and the QwQ model [3]. The models we evaluate span a variety of scales, ranging\\nfrom 1.5B to 32B parameter sizes.\\n3.2\\nResults\\nBased on the linear regression experiments, we have the following observations:\\nLRMs plan their reasoning strength even before the generation of the first reasoning token, and\\nthis planning capability becomes more evident as the layer depth increases. We visualize the\\nlayer-wise prediction results in Figure 2. As shown in this figure, our linear probe can yield high\\nprediction results with correlation coefficients over 0.8 across a range of model sizes and different\\nmodel kinds, suggesting that the reasoning strength planning is possibly encoded in the model‚Äôs\\ninternal activations before the generation of the first reasoning token. Moreover, as the layer becomes\\ndeeper, the prediction results become better. This indicates that the reasoning planning capabilities\\nmay be developed in the later layers of these models. The above observations suggest that LRMs\\nmay have the capability of planning the reasoning strength in advance. We provide more similar\\nexperimental results in the Appendix B.\\n4\\nLRMs Encode Reasoning Strength via Pre-allocated Direction Vectors\\nWe investigate the underlying mechanism behind this planning capability, given the observation that\\nLRMs may plan their reasoning strength in advance as revealed above. Specifically, inspired by the\\nemerging phenomenon that linear representations can control specific behaviors in language models\\n[33, 35], we hypothesize that LRMs may modulate their reasoning planning through pre-allocated\\ndirection vectors embedded within their activation space. We organize this section as follows: We\\nfirst describe how to find the existence of such pre-allocated direction vectors using the difference-\\nin-means approach [45] in Section 4.1. After that, in Section 4.2, we reveal that such pre-allocated\\ndirection vectors are indeed used for planning reasoning strengths, through their causal effects with\\nactivation steering. Then, in Section 4.3, we point out that such pre-allocated direction vectors can be\\n4\\n(a) R1-Distill-Qwen-1.5B\\n(b) R1-Distill-Qwen-7B\\n(c) QwQ-32B\\nFigure 3: Cosine similarity between pre-allocated vectors across different difficulties. These vectors\\nexhibit extremely high cosine similarities, indicating LRMs pre-allocate single direction vector for\\ndistinguishing different question difficulties.\\n(a) R1-Distill-Qwen-1.5B\\n(b) R1-Distill-Qwen-7B\\n(c) QwQ-32B\\nFigure 4: Layer-wise cosine similarities between four pre-allocated vectors\\nused for predicting the reasoning strengths with the linear predictors we obtained above. Finally, in\\nSection 4.4, we uncover that the mechanism of length planning is ultimately achieved by adjusting\\nthe logits of the end-of-think token </think> with such pre-allocated direction vectors.\\n4.1\\nPre-allocated Direction Vectors Exist for Reasoning Strength Planning\\nIn this section, we test the existence of pre-allocated direction vectors (i.e., pre-allocation vectors) for\\nreasoning strength control. Motivated by the observation that LRMs automatically allocate longer\\nreasoning strength for more difficult questions, we suspect that LRMs use linear activation directions\\nfor this control. Therefore, we find such vectors using the difference-in-means approach between\\nquestions of varying difficulties.\\nDifference in Means [45]. The difference-in-means method [45] effectively extracts activation\\ndirection vectors associated with specific model behaviors‚Äîsuch as refusal to answer [35]‚Äîby\\ncomputing the difference between the mean activations associated by two contrasting behaviors of\\ninput data pairs (e.g., refusal and compliance). In our case, we construct contrasting data pairs with\\nquestions of different difficulties, since LRMs behave in automatically allocating more reasoning\\nstrengths on harder tasks. In this way, we may isolate direction vectors related specifically to\\nreasoning strength control, by applying the difference-in-means method. Specifically, we compute\\nthe difference-in-means vector r(l)\\ni‚Üê1 between difficulty the hardest level i and easiest level 1 on the\\nMATH dataset [5] as:\\nr(l)\\ni‚Üê1 =\\n1\\n|Di|\\nX\\nh(l)‚ààDi\\nh(l) ‚àí\\n1\\n|D1|\\nX\\nh(l)‚ààD1\\nh(l),\\n(2)\\nwhere the first and second terms denote the mean activations at layer l computed over the activation\\nsets Di and D0, which correspond to math questions of difficulty level i and 0, respectively. Here,\\nthese activations are also extracted at the start-of-reasoning token <think> position before generation.\\nBy varying the target difficulty i from 1 to 5, we can get four such vectors at each layer, namely r(l)\\n5‚Üê1,\\nr(l)\\n4‚Üê1, r(l)\\n3‚Üê1, and r(l)\\n2‚Üê1. These vectors capture the activation shift from a baseline level of difficulty\\n1 to increasingly harder questions. These vectors are pre-allocated since we extract them before\\ngeneration. If the LRM plans the reasoning strength via a shared directional vector, then the four\\nvectors are expected to show a high degree of similarity.\\nBy analyzing these vectors, we have the following findings:\\n5\\n(a) R1-Distill-Qwen-1.5B\\n(b) R1-Distill-Qwen-7B\\n(c) QwQ-32B\\nFigure 5: L2 norms of four pre-allocated vectors. The norm becomes bigger as the difficulty increases.\\n‚Ä¢ Pre-allocated direction vectors exist for distinguishing questions across different difficulties,\\nsince all constructed vectors exhibit consistently high cosine similarities across layers. We\\nvisualize the pairwise cosine similarities among the four extracted vectors in Figure 3, which are\\ntaken from the layer with the highest averaged similarity. As shown in these figures, the vectors\\nexhibit extremely high directional consistency, with cosine similarities around 0.99. This suggests\\nthat LRMs may utilize a single, shared directional vector to distinguish between questions of\\ndifferent difficulty levels. In addition, we present the trend of average cosine similarity across\\nlayers in Figure 4. The results show consistently high similarity scores (i.e., above 0.9), which\\nfurther increase with layer depth and approach near 1.0 in the final layers.\\n‚Ä¢ The magnitudes of these pre-allocated vectors highly correlate with the required reasoning\\ntoken number, showing implicit connections. Given that the extracted four vectors exhibit nearly\\nidentical directions, their magnitudes (i.e., the L2 norm) become the key factor in distinguishing\\nquestions of varying difficulty. We plot the magnitudes of these four vectors across different layers\\nin Figure 5. As shown, the vector magnitude increases with question difficulty, and is approximately\\nproportional to the average additional reasoning token number required to solve the question (See\\nmore details in Appendix C). This suggests a strong positive correlation between the pre-allocated\\nvector magnitudes and the reasoning strengths allocated by the model.\\n4.2\\nPre-allocation Vectors Causally Affect the Reasoning Strengths\\nTo further examine whether these direction vectors are used for reasoning strength planning, we\\ntest their causal effect on reasoning strengths via intervention of activation steering [25]. The key\\nidea of activation steering is to inject direction vectors on the activation of language models, to\\ntest whether such direction vectors can causally affect the model behaviors [25, 35]. We conduct\\nsuch activation steering experiments with the average vector r(l) of our extracted four vectors (i.e.,\\nr(l) = 1\\n4\\nP5\\ni=2 r(l)\\ni‚Üê1) as:\\nh(l)‚Ä≤ ‚Üêh(l) + Œªr(l),\\n(3)\\nwhere h(l) and h(l)‚Ä≤ are the original and post-steered activations at layer l, and Œª is a hyperparameter\\ncontrolling the strength of steering. More implementation details can be found in Appendix A. By\\nvarying the steering strength Œª, we have following observations (See more results in Appendix C.2):\\n‚Ä¢ Such pre-allocated vectors are indeed responsible for the reasoning strength planning, since\\nsteering with extracted vectors will causally affect the reasoning token number. We visualize\\nin Figure 6 the change in model response length under different steering strengths from -0.2 to 0.2,\\nwith an interval of 0.05. As shown, increasing the negative steering strength progressively decreases\\nthe model‚Äôs reasoning token numbers, while the length of the final answer (i.e., the number of\\ntokens after the </think> token) remains unaffected. This indicates that the pre-allocated vector\\ncausally controls the planning of the reasoning token number, rather than the answer token number.\\n‚Ä¢ Controlling reasoning strengths with this pre-allocation vector causally affects the perfor-\\nmance [27, 10, 16]. As shown in Figure 6, reducing the model‚Äôs reasoning token number generally\\nleads to a drop in performance. This demonstrates that steering with the pre-allocated vector en-\\nables a simple yet effective test-time scaling mechanism. Furthermore, applying a positive steering\\nstrength can even improve model performance. As shown in Table 1, moderate positive steering\\nshows potential in enhancing performance across multiple math datasets, including MATH500 [5],\\nAIME [46], and OlympiadBench [47]. However, increasing the steering strength beyond a certain\\npoint does not lead to further gains, and may even degrade performance. We attribute this to the\\npossible intelligence upper bound of such LRMs. More results are in the Appendix C.2.\\n6\\n(a) R1-Distill-Qwen-1.5B\\n(b) R1-Distill-Qwen-7B\\n(c) QwQ-32B\\nFigure 6: The causal effect on the reasoning token number and corresponding performance under\\ndifferent steering strength Œª. Decreasing the steering strength Œª consistently reduces the reasoning\\ntoken number and corresponding performance.\\nTable 1: Accuracy % comparison before and after steering across datasets. The performance is bold\\nif improved after steering, and the absolute improvement is shown as superscripts.\\nMATH500\\nAIME2024\\nOlympiadBench\\nAverage\\nR1-Distill-Qwen-1.5B\\n82.77\\n28.33\\n44.41\\n51.84\\n+ Steering\\n83.00+0.23\\n29.58+1.25\\n44.67+0.26\\n52.42+0.58\\nR1-Distill-Qwen-7B\\n92.17\\n50.42\\n58.13\\n66.91\\n+ Steering\\n92.65+0.48\\n55.00+4.58\\n58.80+0.67\\n68.82+1.91\\nR1-Distill-Qwen-14B\\n93.67\\n61.25\\n62.19\\n72.37\\n+ Steering\\n94.05+0.38\\n67.08+5.83\\n63.02+0.83\\n74.72+2.35\\nR1-Distill-Qwen-32B\\n94.05\\n63.75\\n63.46\\n73.75\\n+ Steering\\n94.67+0.62\\n67.50+3.75\\n64.11+0.65\\n75.43+1.68\\nQwQ-32B\\n95.90\\n65.42\\n31.25\\n64.19\\n+ Steering\\n96.03+0.13\\n66.67+1.25\\n31.25+0.00\\n64.65+0.46\\n4.3\\nPre-allocation Vectors Yield Positive Reasoning Token Number Prediction\\nIn this section, we discuss the connection between the reasoning token number predictor and these\\npre-allocated vectors we obtained. We reveal that these vectors tend to generate positive reasoning\\ntoken number predictions, further proving their role in the reasoning strength control. Specifically, we\\ncan estimate the effect of these vectors with the linear predictor we obtained in Section 3 as follows:\\nÀÜy(l) = r(l) ÀÜ\\nW(l) + ÀÜb(l).\\n(4)\\nThe pre-allocation vectors yield positive reasoning token number predictions in most cases.\\nWe visualize the predicted reasoning token number across layers when applying the steering vector\\nwith a multiplier of 0.2 in Figure 7. The average prediction (i.e., ÀÜy(l)) shows a consistent positive\\ntrend, indicating that the steering vector reliably adjusts the reasoning length. Moreover, ÀÜy(l) aligns\\nwell with the actual causal changes shown in Figure 6, highlighting a strong link between regression\\npredictions and the steering-induced effects, thus reinforcing the vector‚Äôs role in reasoning strength\\nplanning. More similar results are in Appendix C.3.\\n4.4\\nPre-allocation Vectors Control Reasoning Strengths by Modifying Logits of </think>\\nTo study how such pre-allocated vectors affect the reasoning strength, we take one possible perspective:\\nthe impact on the logits of the end-of-reasoning token </think>. We have the following findings:\\nThese pre-allocation vectors control the reasoning strength by modifying the logits of the end-\\nof-reasoning token </think>. We visualize the distribution of logits for the </think> token under\\ndifferent steering strengths in Figure 8. As shown, applying a negative steering strength results in an\\noverall increase in the logits of the </think> token, indicating a higher likelihood of its occurrence,\\nwhich leads to fewer reasoning tokens. Conversely, positive steering strength decreases the logits of\\nthe </think> token, reducing the likelihood of generating this token, which leads to more reasoning\\ntoken numbers. We also compare the impact on the logits with the eos token <|endoftext|> and\\nrandomly selected tokens. As shown in Figure 8b, the impact on the end-of-reasoning token is\\nsignificantly higher than random tokens and the eos token |endoftext|. These observations suggest\\nthat the pre-allocated vectors primarily modulate reasoning strength by adjusting the logits of the\\nend-of-reasoning token. More similar results are in Appendix C.4.\\n7\\n(a) R1-Distill-Qwen-1.5B\\n(b) R1-Distill-Qwen-7B\\n(c) QwQ-32B\\nFigure 7: The predicted reasoning number ÀÜy(l) yielded by the pre-allocation vector r(l). Pre-allocation\\nvector yields positive predictions in most cases.\\n(a) Logit distribution shift of </think>\\n(b) Impact on the logit across different tokens\\nFigure 8: The effect on the end thinking token </think> when steering with different strengths\\non R1-Distill-Qwen-1.5B. (8a) The pre-allocated vectors control the reasoning strength by causally\\naffecting the logits of end-of-reasoning token </think>. (8a) The impact of logits on </think> is\\nsignificantly higher than other tokens.\\n5\\nPotentials of Our Findings\\nWe aim to investigate whether our findings have potential in more diverse domains, despite our\\ncurrent analysis mainly focusing on mathematical problems. Specifically, we discuss two possible\\ngeneralized potentials of our findings: overthink detection and efficient inference. It is important to\\nhighlight that we merely outline the potential application directions, and more efforts are required to\\nmake such potential applicable in real-world practice.\\n5.1\\nOverthink Detection before Model Generation\\nIn this section, we demonstrate how to detect potential overthink behavior before the generation of\\nLRMs. LRMs exhibit risks in overthink unexpectedly, generating unnecessarily long reasoning traces.\\nSuch overthink phenomena can cause significant real-world deployment consequences, not only\\nsignificantly increasing the computational cost for model providers, but also unnecessarily prolonging\\nthe waiting time for users [48, 49, 13]. If we can detect overthink in advance (i.e., before generation),\\nwe may reduce both computation and latency by, for example, switching to a lighter model for serving\\n[49]. We argue that we can effectively identify the occurrence of possible overthink, by using our\\ntrained predictors to estimate the reasoning strength in advance. We test the predictions differences\\non data pairs of non-overthink questions and overthink questions. These overthink questions are\\nconstructed by forcing LRMs to overthink on vanilla questions by adopting overthink attacks [48],\\nwhere these vanilla questions are sampled from the AlpacaEval dataset. More details can be found\\nin Appendix D.1. We visualize our prediction results in Figure 9. We can find that our predictor\\nyields significantly longer reasoning lengths on overthink questions than on the vanilla non-overthink\\nquestions. This indicates that we can detect possible overthink phenomena before the generation,\\nwhich is based on our findings that LRMs plan their reasoning strengths.\\n5.2\\nEfficient Inference\\nIn this section, we present how to leverage our findings to achieve more efficient inference, avoiding\\noverthinking on simple questions [13]. To test this potential, we conduct activation steering experi-\\nments on two kinds of questions that LRMs tend to overthink: general language understanding dataset\\nMMLU [51] and Level 1 questions on MATH500 [5]. We reduce the number of reasoning tokens by\\n8\\nFigure 9: Overthink detection on AlpacaEval [50] dataset. Our predictor can successfully detect the\\noverthink phenomenon by yielding higher predicted reasoning lengths on overthink questions.\\nFigure 10: Reasoning token numbers (bar chart) and model performance (line chart on MATH500 [5]\\nand MMLU [51]). The number of reasoning tokens can be significantly reduced without harming the\\nperformance of LRMs on easy tasks.\\napplying activation steering with a proper negative steering strength Œª. We report the performance\\nunder such steering experiments in Figure 10. As shown in this figure, we can significantly reduce the\\nnumber of reasoning tokens on such questions, indicating that LRMs may wrongly allocate reasoning\\nstrengths on these easy tasks, which induces their overthinking. While reducing the number of\\nreasoning tokens on these tasks can still maintain the performance of LRMs. This result suggests the\\npotential of using activation steering for efficient reasoning, and also reveals the underlying overthink\\nmechanism of LRMs on easy questions.\\n6\\nLimitations\\nThis work has several limitations. First, we only use a linear probe and do not explore whether\\nmore complex architectures, such as MLPs, could yield better performance in predicting reasoning\\nlength. In addition, our experiments focus primarily on the Qwen model series [52], and it remains\\nunexplored whether these findings hold for reasoning models based on other backbones [53].\\n7\\nConclusion\\nThis paper investigated whether and how large reasoning models (LRMs) plan reasoning strength (i.e.,\\nthe number of reasoning tokens) before generating answers. Using linear probing, we showed that\\nthe reasoning token number can be predicted merely using the activations of the question, suggesting\\nimplicit reasoning strength planning capabilities before generation. We found the existence of pre-\\nallocated direction vectors in LRMs, whose magnitude causally affects the number of reasoning\\ntokens. We further revealed that these vectors may affect the logits of the end-of-thinking token\\n</think> to achieve reasoning strength control. Finally, we discussed two potential applications of\\nour findings: overthink detection before generation and efficient inference.\\n9\\nReferences\\n[1] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low,\\nAlec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko,\\nAlex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam,\\nAlly Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew\\nKondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph,\\nBehrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys\\nMinaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman,\\nCamillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss,\\nChen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer,\\nClive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David\\nFarhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman,\\nEddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace,\\nErik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos\\nTsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista\\nParascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman,\\nHaiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman,\\nHyung Won Chung, Ian Kivlichan, Ian O‚ÄôConnell, Ian Osband, Ignasi Clavera Gilaberte, and\\nIlge Akkaya. Openai o1 system card. CoRR, abs/2412.16720, 2024.\\n[2] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin\\nXu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu,\\nZ. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan\\nWang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang,\\nChong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli\\nLuo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng\\nWang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li,\\nJiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian\\nLiang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean\\nWang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan\\nZhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian,\\nPanpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong\\nZhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan\\nZhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting\\nPan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\\nlearning. CoRR, abs/2501.12948, 2025.\\n[3] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL\\nhttps://qwenlm.github.io/blog/qwq-32b/.\\n[4] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\\nSchulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021.\\n[5] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\\nSong, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset.\\nIn NeurIPS Datasets and Benchmarks, 2021.\\n[6] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee,\\nJan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let‚Äôs verify step by step. In ICLR.\\nOpenReview.net, 2024.\\n[7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond√© de Oliveira Pinto,\\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul\\nPuri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke\\nChan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad\\nBavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias\\nPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex\\nNichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\\nWilliam Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant\\n10\\nMisra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie\\nMayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and\\nWojciech Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374,\\n2021.\\n[8] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David\\nDohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program\\nsynthesis with large language models. CoRR, abs/2108.07732, 2021.\\n[9] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang,\\nJulien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: A graduate-level google-\\nproof q&a benchmark. In First Conference on Language Modeling, 2024. URL https:\\n//openreview.net/forum?id=Ti67584b98.\\n[10] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi,\\nLuke Zettlemoyer, Percy Liang, Emmanuel Cand√®s, and Tatsunori Hashimoto. s1: Simple\\ntest-time scaling. arXiv preprint arXiv:2501.19393, 2025.\\n[11] Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian\\nYu, Juntao Li, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Thoughts\\nare all over the place: On the underthinking of o1-like llms. CoRR, abs/2501.18585, 2025.\\n[12] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi\\nLiu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do\\nNOT think that much for 2+3=? on the overthinking of o1-like llms. CoRR, abs/2412.21187,\\n2024.\\n[13] Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang,\\nShu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, Nicholas Thumiger, Aditya Desai, Ion\\nStoica, Ana Klimovic, Graham Neubig, and Joseph E. Gonzalez. The danger of overthinking:\\nExamining the reasoning-action dilemma in agentic tasks. CoRR, abs/2502.08235, 2025.\\n[14] Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong\\nLiu, Shuxian Liang, Junxian He, et al. A survey of efficient reasoning for large reasoning\\nmodels: Language, multimodality, and beyond. arXiv preprint arXiv:2503.21614, 2025.\\n[15] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun\\nCao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning\\npruning. CoRR, abs/2501.12570, 2025.\\n[16] Pranjal Aggarwal and Sean Welleck. L1: Controlling how long a reasoning model thinks with\\nreinforcement learning. arXiv preprint arXiv:2503.04697, 2025.\\n[17] Weizhe Yuan, Ilia Kulikov, Ping Yu, Kyunghyun Cho, Sainbayar Sukhbaatar, Jason Weston, and\\nJing Xu. Following length constraints in instructions. arXiv preprint arXiv:2406.17744, 2024.\\n[18] Bradley Butcher, Michael O‚ÄôKeefe, and James Titchener. Precise length control for large\\nlanguage models. Natural Language Processing Journal, page 100143, 2025.\\n[19] Chung-En Sun, Ge Yan, and Tsui-Wei Weng. Thinkedit: Interpretable weight editing to mitigate\\noverly short thinking in reasoning models. arXiv preprint arXiv:2503.22048, 2025.\\n[20] David D Baek and Max Tegmark. Towards understanding distilled reasoning models: A\\nrepresentational approach. arXiv preprint arXiv:2503.03730, 2025.\\n[21] Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai\\nWang, and Shiguo Lian. DAST: difficulty-adaptive slow-thinking for large reasoning models.\\nCoRR, abs/2503.04472, 2025.\\n[22] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute\\noptimally can be more effective than scaling model parameters. CoRR, abs/2408.03314, 2024.\\n[23] Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Yunhua Zhou, and Xipeng Qiu. Revisiting the\\ntest-time scaling of o1-like models: Do they truly possess test-time scaling capabilities? CoRR,\\nabs/2502.12215, 2025.\\n11\\n[24] Weixiang Zhao, Xingyu Sui, Jiahe Guo, Yulin Hu, Yang Deng, Yanyan Zhao, Bing Qin,\\nWanxiang Che, Tat-Seng Chua, and Ting Liu. Trade-offs in large reasoning models: An\\nempirical analysis of deliberative and adaptive reasoning over foundational capabilities. CoRR,\\nabs/2503.17979, 2025.\\n[25] Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt\\nTurner. Steering llama 2 via contrastive activation addition. In ACL, 2024.\\n[26] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao,\\nHaotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong,\\nZhijiang Guo, Le Song, and Cheng-Lin Liu. From system 1 to system 2: A survey of reasoning\\nlarge language models. CoRR, abs/2502.17419, 2025.\\n[27] Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang,\\nIrwin King, Xue Liu, and Chen Ma. What, how, where, and how well? A survey on test-time\\nscaling in large language models. CoRR, abs/2503.24235, 2025.\\n[28] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In\\nNeurIPS, 2020.\\n[29] Wilson Wu, John X Morris, and Lionel Levine. Do language models plan ahead for future\\ntokens? In COLM, 2024.\\n[30] Tianyi Men, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, and Jun Zhao. Unlocking the\\nfuture: Exploring look-ahead planning mechanistic interpretability in large language models. In\\nEMNLP, pages 7713‚Äì7724. Association for Computational Linguistics, 2024.\\n[31] Anthropic.\\nOn the biology of a large language model, March 2025.\\nURL https://\\ntransformer-circuits.pub/2025/attribution-graphs/biology.html.\\n[32] Zhichen Dong, Zhanhui Zhou, Zhixuan Liu, Chao Yang, and Chaochao Lu. Emergent response\\nplanning in LLM. CoRR, abs/2502.06258, 2025.\\n[33] Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the\\ngeometry of large language models. In ICML, 2024.\\n[34] Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier\\nprobes. In ICLR (Workshop), 2017.\\n[35] Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, and\\nNeel Nanda. Refusal in language models is mediated by a single direction. In NeurIPS, 2024.\\n[36] Tom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Nicolas Sonnerat,\\nVikrant Varma, J√°nos Kram√°r, Anca D. Dragan, Rohin Shah, and Neel Nanda. Gemma scope:\\nOpen sparse autoencoders everywhere all at once on gemma 2. CoRR, abs/2408.05147, 2024.\\n[37] Tom Wollschl√§ger, Jannes Elstner, Simon Geisler, Vincent Cohen-Addad, Stephan G√ºnnemann,\\nand Johannes Gasteiger. The geometry of refusal in large language models: Concept cones and\\nrepresentational independence. CoRR, abs/2502.17420, 2025.\\n[38] Yihuai Hong, Dian Zhou, Meng Cao, Lei Yu, and Zhijing Jin. The reasoning-memorization\\ninterplay in language models is mediated by a single direction. arXiv preprint arXiv:2503.23084,\\n2025.\\n[39] Xinyu Tang, Xiaolei Wang, Zhihao Lv, Yingqian Min, Wayne Xin Zhao, Binbin Hu, Ziqi Liu,\\nand Zhiqiang Zhang. Unlocking general long chain-of-thought reasoning capabilities of large\\nlanguage models via representation engineering. CoRR, abs/2503.11314, 2025.\\n12\\n[40] Runjin Chen, Zhenyu Zhang, Junyuan Hong, Souvik Kundu, and Zhangyang Wang. Seal: Steer-\\nable reasoning calibration of large language models for free. arXiv preprint arXiv:2504.07986,\\n2025.\\n[41] Wes Gurnee and Max Tegmark.\\nLanguage models represent space and time.\\nCoRR,\\nabs/2310.02207, 2023.\\n[42] Kenneth Li, Aspen K. Hopkins, David Bau, Fernanda B. Vi√©gas, Hanspeter Pfister, and Martin\\nWattenberg. Emergent world representations: Exploring a sequence model trained on a synthetic\\ntask. In ICLR, 2023.\\n[43] Jonas Ranstam and Jonathan A Cook. Lasso regression. Journal of British Surgery, 105(10):\\n1348‚Äì1348, 2018.\\n[44] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,\\nP. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,\\nM. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine\\nLearning Research, 12:2825‚Äì2830, 2011.\\n[45] Sam Marks and Max Tegmark. Diff-in-means concept editing is worst-case optimal, May 2024.\\nURL https://blog.eleuther.ai/diff-in-means/.\\n[46] Mathematical\\nAssociation\\nof\\nAmerica.\\nAmerican\\ninvitational\\nmathematics\\nex-\\namination (aime),\\nFebruary 2024.\\nURL https://maa.org/math-competitions/\\namerican-invitational-mathematics-examination-aime.\\n[47] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi\\nHu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun.\\nOlympiadbench: A challenging benchmark for promoting AGI with olympiad-level bilingual\\nmultimodal scientific problems. In ACL (1), pages 3828‚Äì3850. Association for Computational\\nLinguistics, 2024.\\n[48] Abhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr,\\nand Eugene Bagdasarian.\\nOverthink:\\nSlowdown attacks on reasoning llms.\\nCoRR,\\nabs/2502.02542, 2025.\\n[49] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi\\nLiu, Andrew Wen, Hanjie Chen, Xia Hu, et al. Stop overthinking: A survey on efficient\\nreasoning for large language models. arXiv preprint arXiv:2503.16419, 2025.\\n[50] Yann Dubois, Bal√°zs Galambosi, Percy Liang, and Tatsunori B. Hashimoto. Length-controlled\\nalpacaeval: A simple way to debias automatic evaluators. CoRR, abs/2404.04475, 2024.\\n[51] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\\nJacob Steinhardt. Measuring massive multitask language understanding. In ICLR. OpenRe-\\nview.net, 2021.\\n[52] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan\\nLi, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang,\\nJianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin\\nYang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li,\\nTingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan,\\nYuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. CoRR,\\nabs/2412.15115, 2024.\\n[53] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\\nAiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony\\nHartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark,\\nArun Rao, Aston Zhang, Aur√©lien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi√®re,\\nBethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi,\\nChris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne\\nWong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle\\nPintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano,\\n13\\nDiego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily\\nDinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee,\\nGeorgia Lewis Anderson, Graeme Nail, Gr√©goire Mialon, Guan Pang, Guillem Cucurell,\\nHailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra,\\nIsabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana\\nVranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny\\nHong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,\\nJoanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng\\nJia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin\\nStone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024.\\n[54] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,\\nJoseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large lan-\\nguage model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium\\non Operating Systems Principles, 2023.\\n14\\nA\\nImplementation Details\\nWe implement all the experiments on 8 NVIDIA A100 GPUs. The whole computational resource\\ncost of this research is about 80 A100 GPU days, which is mainly spent on the answer generation.\\nFor the answer generation, we use the vLLM [54] framework for acceleration. Following the\\nsuggestions of DeepSeek 2, we set the temperature as 0.6 to prevent endless repetitions, set the\\nmaximum new generation length as 16,384, and set the rollout number as 8. We take the average\\naccuracy of all 8 rollouts as the accuracy of one question, and report the final accuracy by taking the\\naverage accuracy of each question.\\nWe use the following template for generation:\\nPROMPT:\\nPlease reason step by step, and put your final answer within \\\\boxed{}.\\nThis is the problem:\\n{problem}\\nFigure 11: Prompts used for answer generation.\\nHere, the {problem} will be replaced by a real question. After we finish generation, we extract the\\nanswer inside the \\\\boxed{} for evaluation.\\n2https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\\n15\\nB\\nReasoning Strength in LRMs is Pre-Planned\\nB.1\\nExperimental Details\\nWe visualize the linear probing process in Figure 12. We first extract the activation of LRMs h(l)\\nat the last token position. Then we train a linear regression for predicting the subsequent reasoning\\ntoken number y, which is calculated through the tokenizer. For reducing the overfitting, we set the\\nregularization term Œ± in Equation 1 as 10.\\nFigure 12: The procedure of linear probing.\\nB.2\\nLayer-wise Regression Results\\nWe visualize the layer-wise regression results on R1-Distill-Qwen-14B and R1-Distill-Qwen-32B in\\nFigure 13. On these two models, the linear regression exhibits the same pattern of an increasing trend\\nas the model depth increases. Similarly, the correlation coefficient reaches over 0.8, indicating that\\nthe reasoning strength can be predicted before the model generation.\\n(a) R1-Distill-Qwen-14B\\n(b) R1-Distill-Qwen-32B\\nFigure 13: Layer-wise linear regression results\\n16\\nC\\nLRMs Encode Reasoning Strength via a Pre-allocated Direction Vector\\nC.1\\nExistence of Pre-allocated Direction Vectors for Reasoning Strength Control\\nWe visualize the cosine similarity matrix of the four extracted vectors from R1-Distill-Qwen-14B\\nand R1-Distill-Qwen-32B in Figure 14. We have a similar observation that all these vectors exhibit\\nextremely high cosine similarities near 1.0. This indicates that LRMs actually use a single direction\\nvector for distinguishing questions of different difficulty levels.\\n(a) R1-Distill-Qwen-14B\\n(b) R1-Distill-Qwen-32B\\nFigure 14: Cosine similarity between pre-allocated vectors across different difficulties. These vectors\\nexhibit extremely high cosine similarities, indicating LRMs pre-allocate a single direction vector for\\ndistinguishing different question difficulties.\\nWe visualize the layer-wise mean cosine similarities between four extracted vectors from R1-Distill-\\nQwen-14B and R1-Distill-Qwen-32B in Figure 15. We observe that these vectors exhibit consistently\\nhigh cosine similarities, with an increasing trend as the layer depth increases. Finally, the mean\\ncosine similarity reaches around 1.0, indicating these vectors become a single direction vector in the\\nlater layers.\\n(a) R1-Distill-Qwen-14B\\n(b) R1-Distill-Qwen-32B\\nFigure 15: Layer-wise cosine similarities between four pre-allocated vectors\\nWe visualize the L2 norm of these extracted four vectors from R1-Distill-Qwen-14B and R1-Distill-\\nQwen-32B in Figure 16. We observe that the norm of these vectors becomes bigger as the difficulty\\nincreases. Moreover, this trend is also similar to the increased reasoning token number as the difficulty\\nincreases, as shown in Figure 17. This indicates that LRMs use the magnitude of these direction\\nvectors for handling different question difficulties.\\n(a) R1-Distill-Qwen-14B\\n(b) R1-Distill-Qwen-32B\\nFigure 16: L2 norms of four pre-allocated vectors. The norm becomes bigger as the difficulty\\nincreases.\\n17\\n(a) R1-Distill-Qwen-14B\\n(b) R1-Distill-Qwen-32B\\nFigure 17: Average reasoning token number for different question difficulties.\\nC.2\\nPre-allocated Vectors Causally Affect the Reasoning Strengths\\nWe apply the activation steering at each layer and each position of LRMs. We provide more results\\nwhen steering with the pre-allocation vector r(l), in Figure 18, Figure 19, and Figure 20. We have\\nsimilar observations to those in Section 4.2. When steering with negative Œª, we observe a consistent\\ndecreasing trend in the reasoning token number and the decreased performance. When steering\\nwith positive Œª, we observe a consistent increasing trend in the reasoning token number. However,\\ndespite appropriate positive Œª can improve the performance, this is not consistent as the Œª increases.\\nWe attribute this to the capability upper bound of these LRMs. Moreover, the steering only affects\\nthe reasoning token number, while maintaining the answer token number largely unchanged. This\\nindicates that the pre-allocated direction vector is mainly responsible for the reasoning token number.\\n(a) R1-Distill-Qwen-14B\\n(b) R1-Distill-Qwen-32B\\nFigure 18: The causal effect on the reasoning token number and corresponding performance under\\ndifferent steering strength Œª on the dataset MATH500.\\n(a) R1-Distill-Qwen-1.5B\\n(b) R1-Distill-Qwen-7B\\n(c) QwQ-32B\\nFigure 19: The causal effect on the reasoning token number and corresponding performance under\\ndifferent steering strength Œª on the dataset AIME.\\n(a) R1-Distill-Qwen-1.5B\\n(b) R1-Distill-Qwen-7B\\n(c) QwQ-32B\\nFigure 20: The causal effect on the reasoning token number and corresponding performance under\\ndifferent steering strength Œª on the dataset OlympiadBench.\\n18\\nC.3\\nPre-allocation Vectors Yield Positive Reasoning Token Number Prediction\\nWe provide more results in predicting the reasoning token number directly using the pre-allocation\\nvectors in Figure 21. As shown in this figure, in most cases, the pre-allocation vectors yield positive\\npredictions, indicating the close correlation of such vectors with our obtained predictors. This\\nsuggests that LRMs are indeed using such pre-allocated vectors for planning their reasoning strength,\\nand the predication also largely relies on these vectors.\\n(a) R1-Distill-Qwen-14B\\n(b) R1-Distill-Qwen-32B\\nFigure 21: The predicted reasoning number ÀÜy(l) yielded by the pre-allocation vector r(l). Pre-\\nallocation vector yields positive predictions in most cases.\\nC.4\\nPre-allocated Vectors Control Reasoning Strengths by Modifying Logits of </think>\\nWe provide more results about how these pre-allocated direction vectors control the reasoning strength\\nby modifying the logits of the end-of-reasoning token </think>.\\nWe conduct the same activation steering as we do in Section 4.2, varying the steering strength Œª\\nfrom -0.2 to 0.2. Then, we directly extract the logits of each token at the last token position (i.e.,\\nthe start-of-reasoning <think>). We visualize the results in Figure 22. We can observe that, as the\\nsteering strength Œª increases from -0.2 to 0.2, the logits of the end-of-reasoning token </think>\\ndecrease. This indicates that LRMs are less likely to generate such tokens, thereby leading to more\\nreasoning tokens. Moreover, as shown in Figure 22b, this steering mainly has more impact on the\\nlogits of </think> than randomly selected tokens and the EOS token <endoftext>. Here, the\\nrandom token logits denote the average logits of 500 randomly selected tokens. This indicates that\\nthe steering mainly focuses on adjusting the reasoning strength by manipulating the </think>.\\n(a) Logit distribution shift of </think>\\n(b) Impact on the logit across different tokens\\nFigure 22: The effect on the end thinking token </think> when steering with different strengths\\non R1-Distill-Qwen-7B. (8a) The pre-allocated vectors control the reasoning strength by causally\\naffecting the logits of end-of-reasoning token </think>. (8a) The impact of logits on </think> is\\nsignificantly higher than other tokens.\\n19\\nD\\nPotentials of Our Findings\\nD.1\\nOverthink Detection before Model Generation\\nIn this section, we discuss whether our findings can help us detect the potential overthinking phe-\\nnomenon even before the generation. To study this, we sample 100 questions from the AlpacaEval\\n[50] dataset as the vanilla questions. We then generate one overthink question for each vanilla\\nquestion, with the overthink attack [48], which proves to be effective in inducing overthink while\\nmaintaining the accuracy on vanilla questions. In this way, we can test whether our predictor can\\ndetect the overthink phenomenon in advance, by checking whether the predicted token number on\\noverthink questions is much more than that on vanilla questions. Results in Section 5.1 show that our\\npredictor shows potential in overthink detection.\\nD.2\\nEfficient Inference\\nD.2.1\\nDetails\\nFor evaluation efficiency, we sample 100 questions from MMLU [51] and transform them into\\nsingle-choice questions for LRMs to answer. We adopt the same setting as in lm-evaluation-harness 3\\nfor evaluation.\\n3https://github.com/EleutherAI/lm-evaluation-harness\\n20\\nD.2.2\\nCase Study\\nQuestion: Compute: 1 ‚àí2 + 3 ‚àí4 + 5 ‚àí¬∑ ¬∑ ¬∑ + 99 ‚àí100.\\nw/o Steering:\\n<think>Okay, so I have this problem to compute the sum: 1 - 2 + 3 - 4 + 5 -\\n6 + ... + 99 - 100. Hmm, that looks like an alternating series where the signs alternate between\\npositive and negative. Let me try to figure this out step by step. First, let me write out the series\\nto understand the pattern better: 1 - 2 + 3 - 4 + 5 - 6 + ... + 99 - 100. I notice that this series\\nalternates between adding an odd number and subtracting the next even number. So, each pair of\\nterms is like (1 - 2), (3 - 4), (5 - 6), and so on. Maybe I can group them into these pairs and see\\nwhat each pair adds up to. Let me compute a few pairs: 1 - 2 = -1, 3 - 4 = -1, 5 - 6 = -1, 7 - 8 =\\n-1, ... It seems like each pair of terms adds up to -1. That‚Äôs interesting. So, if I can figure out how\\nmany such pairs there are, I can multiply that number by -1 to get the total sum. Since the series\\nstarts at 1 and ends at 100, that‚Äôs a total of 100 numbers. If I‚Äôm pairing them two by two, the\\nnumber of pairs should be 100 divided by 2, which is 50. So, there are 50 pairs.\\n¬∑ ¬∑ ¬∑\\nThat is, for each term, if n is odd, we add it, and if n is even, we subtract it. So, the general term\\nis (‚àí1)(n+1) ‚àón. Maybe I can use a formula for such alternating series. I remember that the\\nsum of an alternating series can sometimes be calculated using arithmetic series formulas or by\\ngrouping terms. Wait, another approach: Maybe I can separate the series into two separate sums:\\nthe sum of the odd terms and the sum of the even terms. Let‚Äôs see: The series is 1 + 3 + 5 + ...\\n+ 99 minus (2 + 4 + 6 + ... + 100). So, if I can compute both of these sums separately, I can\\nsubtract them to get the result. First, the sum of the odd numbers from 1 to 99. How many terms\\nare there? Since we‚Äôre going up to 99, which is the 50th odd number (since 99 = 2*50 - 1). So,\\nthe number of terms is 50.\\n¬∑ ¬∑ ¬∑\\nWait, another way: Let‚Äôs think about the series as telescoping. But I don‚Äôt think it‚Äôs telescoping\\nbecause there‚Äôs no cancellation beyond the immediate pairs. Each term cancels only part of the\\nnext term, but in this case, each pair is independent. Alternatively, maybe writing the series as:\\n(1 - 2) + (3 - 4) + ... + (99 - 100). Each parenthesis is -1, and there are 50 of them. So, 50*(-1)\\n= -50. Yep, same result. I think I‚Äôve approached this problem in multiple ways: pairing terms,\\nseparating into odd and even sums, using the formula for alternating series, and each time I get\\n-50. So, it seems pretty solid. Just to recap all the methods: 1. Pairing: Each pair (1-2), (3-4),\\netc., is -1, 50 pairs total, sum is -50. 2. Separating into odds and evens: Sum of odds is 2500,\\nsum of evens is 2550, difference is -50. 3. Using the formula for the sum of an alternating series\\nof consecutive integers with even terms: -n/2, which gives -50. All methods lead to the same\\nconclusion, so I think it‚Äôs safe to say the answer is -50. **Final Answer** The value of the series\\nis \\\\boxed{-50}</think>. (2253 tokens)\\nw Steering:\\n<think>First, I observe that the sequence alternates between addition and\\nsubtraction of consecutive integers. To simplify the calculation, I can group the terms in pairs:\\n(1 - 2), (3 - 4), (5 - 6), and so on, up to (99 - 100). Each pair results in -1. Since there are 100\\nnumbers in total, there are 50 such pairs. Multiplying the number of pairs by each result gives\\n50 times -1, which equals -50.</think> (113 tokens)\\nFigure 23: Case study on R1-Distill-Qwen-32B. The model generates the correct answer both with\\n(i.e., w) and without (i.e., w/o) steering, but steering significantly reduces the reasoning token\\nnumber.\\n21\\nQuestion: What is the correct answer to this question: Kant‚Äôs humanity formulation of the\\ncategorical imperative makes it impermissible for us to, he argues, Choices: (A) sell one of our\\nteeth. (B) engage in prostitution. (C) both A and B (D) Neither A nor B Only put the option\\nletter in the box, e.g.\\nboxedA. There is only one correct answer, .\\nw/o Steering:\\n<think> Okay, so I need to figure out what Kant‚Äôs humanity formulation\\nsays about selling one of our teeth or engaging in prostitution. Let me start by recalling what\\nthe humanity formulation is. From what I remember, Kant‚Äôs categorical imperative has several\\nformulations, and one of them is the humanity formulation. The key idea here is that we should\\ntreat humanity, whether in oneself or in others, always as an end and never merely as a means\\nto an end. So, the question is asking which actions Kant would consider impermissible under\\nthis principle. The options are selling one‚Äôs tooth, prostitution, both, or neither. First, let me\\nbreak down each option. Selling a tooth: that‚Äôs essentially selling part of your body. Now, in the\\ncontext of Kant‚Äôs ethics, using oneself as a means to an end might be problematic.\\n¬∑ ¬∑ ¬∑\\nWait, but what about the actual examples? I remember that Kant specifically mentions prostitution\\nin his works, right? He considered it immoral because it treats the person as an object for sexual\\nuse, which is a means to an end. Similarly, selling body parts might be similar. But selling a\\ntooth is a physical part, whereas prostitution is an action. However, in both cases, the person\\nis using their body for monetary gain, possibly reducing themselves to a means. But maybe\\nKant would say that selling a tooth is permissible if it‚Äôs for a good reason, like saving a life, but\\nnot sure. Wait, the formulation is about treating humanity as an end. So any action that uses\\na person (including oneself) merely as a means would be wrong. So selling your tooth would\\ninvolve using your body as a means to get money, but does that mean you‚Äôre treating yourself as\\na mere means? Maybe. Because you‚Äôre using your body as a commodity, rather than valuing\\nit as an end in itself. Alternatively, maybe Kant would distinguish between selling a tooth (a\\npart) and selling services (like prostitution). But I think both involve commodifying the body.\\nSo perhaps both are impermissible. Thus, the answer would be (C) both A and B.\\n¬∑ ¬∑ ¬∑\\nAlternatively, maybe he only opposes prostitution, but I think both. Let me think again. Another\\nangle: the humanity formulation requires that in any action, you never use a person merely as a\\nmeans. So if you sell your tooth, you are using yourself as a means (to get money), but you‚Äôre\\nalso an end. However, if you are selling a tooth, maybe you are treating yourself as an end in\\nsome way. Wait, perhaps the distinction is whether you are using your body as a mere means.\\nIf you sell a tooth, you are using that part of your body as a tool, but does that mean you are\\ntreating your humanity as a means? The humanity is your rational nature. So maybe selling a\\ntooth doesn‚Äôt directly violate that, as it‚Äôs a physical part. Alternatively, Kant might argue that the\\nbody is part of what makes a person, so using it as a commodity reduces oneself. So perhaps\\nboth are wrong. Since I‚Äôm a bit torn here, but I think the standard interpretation is that Kant\\nwould say both are impermissible. Hence, answer (C). </think>. (1610 tokens)\\nw Steering:\\n<think> \"Okay, so I need to figure out what the answer is here. The question\\nis about Kant‚Äôs humanity formulation of the categorical imperative and what he says is imper-\\nmissible. The options are selling a tooth, prostitution, both, or neither. First, I remember that\\nKant‚Äôs categorical imperative has different formulations.\\n¬∑ ¬∑ ¬∑\\nSo both selling a tooth (if it‚Äôs your own) and prostitution would be impermissible because both\\ninvolve treating humanity (either your own or someone else‚Äôs) as a means. So the answer should\\nbe C, both A and B? But wait, maybe selling a tooth isn‚Äôt considered the same as prostitution?\\nLike, a tooth is part of your body but maybe it‚Äôs not using humanity as an end? Hmm, but Kant\\nmight say that even your own body shouldn‚Äôt be treated merely as a resource to exploit for gain.\\nSo selling a tooth could still be wrong because you‚Äôre using your own body as a means to an end\\n(making money) rather than respecting your own humanity as an end in itself. So both A and B\\nare impermissible. Therefore, the answer should be C. \" </think> (418 tokens)\\nFigure 24: Case study on QwQ-32B. The model generates the correct answer both with (i.e., w) and\\nwithout (i.e., w/o) steering, but steering significantly reduces the reasoning token number.\\n22\\nE\\nBroader Impacts\\nThis paper aims to investigate whether LRMs pre-plan their reasoning strength within their activation\\nspace, and how such planning is encoded with pre-allocated direction vectors. Our study contributes\\nto a deeper understanding of LRMs within the LLM research community.\\nHowever, our findings may also pose potential risks. For instance, malicious methods could exploit the\\ndiscovered property that reasoning length can be manipulated through the model‚Äôs internal activations\\nto implant backdoor attacks. Such attacks might trigger excessively long chains of thought under\\nspecific conditions, thereby significantly slowing down model execution.\\n23\\n')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "arxiv_loader = ArxivLoader(\n",
    "    query=\"reasoning\"\n",
    ")\n",
    "\n",
    "documents = arxiv_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcc97876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani‚àó\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer‚àó\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar‚àó\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit‚àó\\nGoogle Research\\nusz@google.com\\nLlion Jones‚àó\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez‚àó‚Ä†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n≈Åukasz Kaiser‚àó\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin‚àó‚Ä°\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n‚Ä†Work performed while at Google Brain.\\n‚Ä°Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht‚àí1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by ‚àödk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n‚àödk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n‚àödk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n‚àödk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q ¬∑ k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n‚ààRdmodel√ódk, W K\\ni\\n‚ààRdmodel√ódk, W V\\ni\\n‚ààRdmodel√ódv\\nand W O ‚ààRhdv√ódmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n‚Ä¢ In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n‚Ä¢ The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n‚Ä¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to ‚àí‚àû) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by ‚àödmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 ¬∑ d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n ¬∑ d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k ¬∑ n ¬∑ d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r ¬∑ n ¬∑ d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2œÄ to 10000 ¬∑ 2œÄ. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ‚ààRd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k ¬∑ n ¬∑ d + n ¬∑ d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with Œ≤1 = 0.9, Œ≤2 = 0.98 and œµ = 10‚àí9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d‚àí0.5\\nmodel ¬∑ min(step_num‚àí0.5, step_num ¬∑ warmup_steps‚àí1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 ¬∑ 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 ¬∑ 1019\\n1.4 ¬∑ 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 ¬∑ 1018\\n1.5 ¬∑ 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 ¬∑ 1019\\n1.2 ¬∑ 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 ¬∑ 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 ¬∑ 1020\\n1.1 ¬∑ 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 ¬∑ 1019\\n1.2 ¬∑ 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 ¬∑ 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 ¬∑ 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value œµls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty Œ± = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nœµls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n√ó106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and Œ± = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, √áaglar G√ºl√ßehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770‚Äì778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J√ºrgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and J√ºrgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735‚Äì1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832‚Äì841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] ≈Åukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] ≈Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313‚Äì330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152‚Äì159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar T√§ckstr√∂m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433‚Äì440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929‚Äì1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440‚Äì2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104‚Äì3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google‚Äôs neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434‚Äì443. ACL, August 2013.\\n12\\nAttention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‚Äòmaking‚Äô, completing the phrase ‚Äòmaking...more difficult‚Äô. Attentions here shown only for\\nthe word ‚Äòmaking‚Äô. Different colors represent different heads. Best viewed in color.\\n13\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‚Äòits‚Äô for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_loader = ArxivLoader(\n",
    "    query='1706.03762',\n",
    "    # load_max_docs=2,    \n",
    ")\n",
    "\n",
    "documents = arxiv_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea6fe8d",
   "metadata": {},
   "source": [
    "### 5. Wikipedia Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a75ad172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'SpaceX', 'summary': \"Space Exploration Technologies Corp., commonly referred to as SpaceX, is an American space technology company headquartered at the Starbase development site in Starbase, Texas. Since its founding in 2002, the company has made numerous advances in rocket propulsion, reusable launch vehicles, human spaceflight and satellite constellation technology. As of 2025, SpaceX is the world's dominant space launch provider, its launch cadence eclipsing all others, including private competitors and national programs like the Chinese space program. SpaceX, NASA, and the United States Armed Forces work closely together by means of governmental contracts.\\nSpaceX was founded by Elon Musk in 2002 with a vision of decreasing the costs of space launches, paving the way to a sustainable colony on Mars. In 2008, Falcon 1 successfully launched into orbit after three failed launch attempts. The company then moved towards the development of the larger Falcon 9 rocket and the Dragon 1 capsule to satisfy NASA's COTS contracts for deliveries to the International Space Station. By 2012, SpaceX finished all COTS test flights and began delivering Commercial Resupply Services missions to the International Space Station. Also around that time, SpaceX started developing hardware to make the Falcon 9 first stage reusable. The company demonstrated the first successful first-stage landing in 2015 and re-launch of the first stage in 2017. Falcon Heavy, built from three Falcon 9 boosters, first flew in 2018 after a more than decade-long development process. As of May 2025, the company's Falcon 9 rockets have landed and flown again more than 450 times, reaching 1‚Äì3 launches a week.\\nThese milestones delivered the company much-needed investment and SpaceX sought to diversify its sources of income. In 2019, the first operational satellite of the Starlink internet satellite constellation came online. In subsequent years, Starlink generated the bulk of SpaceX's income and paved the way for its Starshield military counterpart. In 2020, SpaceX began to operate its Dragon 2 capsules to deliver crewed missions for NASA and private entities. Around this time, SpaceX began building test prototypes for Starship, which is the largest launch vehicle in history and aims to fully realize the company's vision of a fully-reusable, cost-effective and adaptable launch vehicle. SpaceX is also developing its own space suit and astronaut via its Polaris program as well as developing the human lander for lunar missions under NASA's Artemis program. SpaceX is not publicly traded; a space industry newspaper estimated that SpaceX has a revenue of over $10 billion in 2024.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/SpaceX'}, page_content=\"Space Exploration Technologies Corp., commonly referred to as SpaceX, is an American space technology company headquartered at the Starbase development site in Starbase, Texas. Since its founding in 2002, the company has made numerous advances in rocket propulsion, reusable launch vehicles, human spaceflight and satellite constellation technology. As of 2025, SpaceX is the world's dominant space launch provider, its launch cadence eclipsing all others, including private competitors and national programs like the Chinese space program. SpaceX, NASA, and the United States Armed Forces work closely together by means of governmental contracts.\\nSpaceX was founded by Elon Musk in 2002 with a vision of decreasing the costs of space launches, paving the way to a sustainable colony on Mars. In 2008, Falcon 1 successfully launched into orbit after three failed launch attempts. The company then moved towards the development of the larger Falcon 9 rocket and the Dragon 1 capsule to satisfy NASA's COTS contracts for deliveries to the International Space Station. By 2012, SpaceX finished all COTS test flights and began delivering Commercial Resupply Services missions to the International Space Station. Also around that time, SpaceX started developing hardware to make the Falcon 9 first stage reusable. The company demonstrated the first successful first-stage landing in 2015 and re-launch of the first stage in 2017. Falcon Heavy, built from three Falcon 9 boosters, first flew in 2018 after a more than decade-long development process. As of May 2025, the company's Falcon 9 rockets have landed and flown again more than 450 times, reaching 1‚Äì3 launches a week.\\nThese milestones delivered the company much-needed investment and SpaceX sought to diversify its sources of income. In 2019, the first operational satellite of the Starlink internet satellite constellation came online. In subsequent years, Starlink generated the bulk of SpaceX's income and paved the way for its Starshield military counterpart. In 2020, SpaceX began to operate its Dragon 2 capsules to deliver crewed missions for NASA and private entities. Around this time, SpaceX began building test prototypes for Starship, which is the largest launch vehicle in history and aims to fully realize the company's vision of a fully-reusable, cost-effective and adaptable launch vehicle. SpaceX is also developing its own space suit and astronaut via its Polaris program as well as developing the human lander for lunar missions under NASA's Artemis program. SpaceX is not publicly traded; a space industry newspaper estimated that SpaceX has a revenue of over $10 billion in 2024.\\n\\n\\n== History ==\\n\\n\\n=== 2001‚Äì2004: Founding ===\\nIn early 2001, Elon Musk met Robert Zubrin and donated $100,000 to his Mars Society, joining its board of directors for a short time.:\\u200a30‚Äì31\\u200a He gave a plenary talk at their fourth convention where he announced Mars Oasis, a project to land a greenhouse and grow plants on Mars. Musk initially attempted to acquire a Dnepr launch vehicle for the project through Russian contacts from Jim Cantrell.\\nMusk returned with his team to Moscow, this time bringing Michael Griffin, who later became the 11th Administrator of NASA, but found the Russians increasingly unreceptive. On the flight home, Musk announced he could start a company to build the affordable rockets they needed instead. By applying vertical integration, using inexpensive commercial off-the-shelf components when possible, and adopting the modular approach of modern software engineering, Musk believed SpaceX could significantly cut launch costs.\\nIn early 2002, Elon Musk started to look for staff for his company, soon to be named SpaceX. Musk approached five people for the initial positions at the fledgling company, including Griffin, who declined the position of Chief Engineer,:\\u200a11\\u200a Jim Cantrell and John Garvey (Cantrell and Garvey later founded the company Vector Launch), rocket engineer Tom Mueller, and Chris Thompson. Space\"),\n",
       " Document(metadata={'title': 'SpaceX Starship', 'summary': \"Starship is a two-stage, fully reusable, super heavy-lift launch vehicle under development by American aerospace company SpaceX. Currently built and launched from Starbase in Texas, it is intended as the successor to company's Falcon 9 and Falcon Heavy rockets, and is part of SpaceX's broader reusable launch system development program. If completed as designed, Starship would be the first fully reusable orbital rocket and have the highest payload capacity of any launch vehicle to date. As of 28 May 2025, Starship has launched 9 times, with 4 successful flights and 5 failures.\\nThe vehicle consists of two stages: the Super Heavy booster and the Starship spacecraft, both powered by Raptor engines burning liquid methane (the main component of natural gas) and liquid oxygen. Both stages are intended to return to the launch site and land vertically at the launch tower for potential reuse. Once in space, the Starship upper stage is intended to function as a standalone spacecraft capable of carrying crew and cargo. Missions beyond low Earth orbit would require multiple in-orbit refueling flights. At the end of its mission, Starship reenters the atmosphere using heat shield tiles similar to those of the Space Shuttle. SpaceX states that its goal is to reduce launch costs by both reusing and mass producing both stages.\\nSpaceX has proposed a wide range of missions for Starship, such as deploying large satellites, space station modules, and space telescopes. A crewed variant, developed under contract with NASA, is called the Starship Human Landing System, which is scheduled to deliver astronauts to the Moon as part Artemis program, beginning with Artemis III currently scheduled for 2027. SpaceX has also expressed ambitions to use Starship for crewed missions to Mars.\\nSpaceX began developing concepts for a super heavy-lift reusable launch vehicle as early as 2005, when it was called BFR (Big Falcon Rocket). Starship's current design and name were introduced in 2018. Development has followed an iterative and incremental approach, involving a high number of test flights and prototype vehicles. The first launch of a full Starship vehicle occurred on April 20, 2023, and ended with the explosion of the rocket four minutes after liftoff. The program has failed to meet many of its optimistic schedule goals and its development has had a number of setbacks, including four consecutive upper-stage failures in the first half of 2025.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/SpaceX_Starship'}, page_content='Starship is a two-stage, fully reusable, super heavy-lift launch vehicle under development by American aerospace company SpaceX. Currently built and launched from Starbase in Texas, it is intended as the successor to company\\'s Falcon 9 and Falcon Heavy rockets, and is part of SpaceX\\'s broader reusable launch system development program. If completed as designed, Starship would be the first fully reusable orbital rocket and have the highest payload capacity of any launch vehicle to date. As of 28 May 2025, Starship has launched 9 times, with 4 successful flights and 5 failures.\\nThe vehicle consists of two stages: the Super Heavy booster and the Starship spacecraft, both powered by Raptor engines burning liquid methane (the main component of natural gas) and liquid oxygen. Both stages are intended to return to the launch site and land vertically at the launch tower for potential reuse. Once in space, the Starship upper stage is intended to function as a standalone spacecraft capable of carrying crew and cargo. Missions beyond low Earth orbit would require multiple in-orbit refueling flights. At the end of its mission, Starship reenters the atmosphere using heat shield tiles similar to those of the Space Shuttle. SpaceX states that its goal is to reduce launch costs by both reusing and mass producing both stages.\\nSpaceX has proposed a wide range of missions for Starship, such as deploying large satellites, space station modules, and space telescopes. A crewed variant, developed under contract with NASA, is called the Starship Human Landing System, which is scheduled to deliver astronauts to the Moon as part Artemis program, beginning with Artemis III currently scheduled for 2027. SpaceX has also expressed ambitions to use Starship for crewed missions to Mars.\\nSpaceX began developing concepts for a super heavy-lift reusable launch vehicle as early as 2005, when it was called BFR (Big Falcon Rocket). Starship\\'s current design and name were introduced in 2018. Development has followed an iterative and incremental approach, involving a high number of test flights and prototype vehicles. The first launch of a full Starship vehicle occurred on April 20, 2023, and ended with the explosion of the rocket four minutes after liftoff. The program has failed to meet many of its optimistic schedule goals and its development has had a number of setbacks, including four consecutive upper-stage failures in the first half of 2025.\\n\\n\\n== Description ==\\nWhen stacked and fully fueled, Starship has a mass of approximately 5,000 t (11,000,000 lb), a diameter of 9 m (30 ft) and a height of 121.3 m (398 ft). The rocket has been designed with the goal of being fully reusable to reduce launch costs; it consists of the Super Heavy booster and the Starship upper stage which are powered by Raptor and Raptor Vacuum engines.\\nThe bodies of both rocket stages are made from stainless steel and are manufactured by stacking and welding stainless steel cylinders. These cylinders have a height of 1.83 m (6 ft), and a thickness of 3.97 mm (0.156 in).\\nDomes inside the spacecraft separate the methane and oxygen tanks. SpaceX has stated that Starship, in its \"baseline reusable design\", will have a payload capacity of 100‚Äì150 t (220,000‚Äì331,000 lb) to low earth orbit and 27 t (60,000 lb) to geostationary transfer orbit.\\n\\n\\n=== Super Heavy booster ===\\n\\nSuper Heavy is 71 m (233 ft) tall, 9 m (30 ft) wide, and is composed of four general sections: the engines, the oxygen tank, the fuel tank, and the interstage. SpaceX CEO Elon Musk stated in 2021 that the final design will have a dry mass between 160 t (350,000 lb) and 200 t (440,000 lb), with the tanks weighing 80 t (180,000 lb) and the interstage 20 t (44,000 lb).\\n\\n\\n==== Tanks ====\\nThe two cryogenic propellant tanks on Super Heavy are separated by a common bulkhead, a similar structural design to the S-II and S-IVB stages on the Saturn V rocket. After Starship\\'s second flight test, the common dome\\'s design was changed to be mo')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "wiki_loader = WikipediaLoader(query=\"Space X\", load_max_docs=2)\n",
    "documents = wiki_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a938be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'SpaceX',\n",
       " 'summary': \"Space Exploration Technologies Corp., commonly referred to as SpaceX, is an American space technology company headquartered at the Starbase development site in Starbase, Texas. Since its founding in 2002, the company has made numerous advances in rocket propulsion, reusable launch vehicles, human spaceflight and satellite constellation technology. As of 2025, SpaceX is the world's dominant space launch provider, its launch cadence eclipsing all others, including private competitors and national programs like the Chinese space program. SpaceX, NASA, and the United States Armed Forces work closely together by means of governmental contracts.\\nSpaceX was founded by Elon Musk in 2002 with a vision of decreasing the costs of space launches, paving the way to a sustainable colony on Mars. In 2008, Falcon 1 successfully launched into orbit after three failed launch attempts. The company then moved towards the development of the larger Falcon 9 rocket and the Dragon 1 capsule to satisfy NASA's COTS contracts for deliveries to the International Space Station. By 2012, SpaceX finished all COTS test flights and began delivering Commercial Resupply Services missions to the International Space Station. Also around that time, SpaceX started developing hardware to make the Falcon 9 first stage reusable. The company demonstrated the first successful first-stage landing in 2015 and re-launch of the first stage in 2017. Falcon Heavy, built from three Falcon 9 boosters, first flew in 2018 after a more than decade-long development process. As of May 2025, the company's Falcon 9 rockets have landed and flown again more than 450 times, reaching 1‚Äì3 launches a week.\\nThese milestones delivered the company much-needed investment and SpaceX sought to diversify its sources of income. In 2019, the first operational satellite of the Starlink internet satellite constellation came online. In subsequent years, Starlink generated the bulk of SpaceX's income and paved the way for its Starshield military counterpart. In 2020, SpaceX began to operate its Dragon 2 capsules to deliver crewed missions for NASA and private entities. Around this time, SpaceX began building test prototypes for Starship, which is the largest launch vehicle in history and aims to fully realize the company's vision of a fully-reusable, cost-effective and adaptable launch vehicle. SpaceX is also developing its own space suit and astronaut via its Polaris program as well as developing the human lander for lunar missions under NASA's Artemis program. SpaceX is not publicly traded; a space industry newspaper estimated that SpaceX has a revenue of over $10 billion in 2024.\\n\\n\",\n",
       " 'source': 'https://en.wikipedia.org/wiki/SpaceX'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f176f092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
