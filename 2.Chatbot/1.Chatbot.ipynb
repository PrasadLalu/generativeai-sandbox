{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c99feed3",
   "metadata": {},
   "source": [
    "### Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f6d4a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Access Groq API Key\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdb308d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Generative AI is a type of artificial intelligence that focuses on creating new content. \\n\\nThink of it like this:\\n\\n* **Traditional AI** is great at analyzing existing data and finding patterns. It can tell you what's in a picture, translate languages, or predict the weather.\\n* **Generative AI** takes it a step further. It learns the underlying structure of data and uses that knowledge to generate brand new content.\\n\\nThis new content can take many forms:\\n\\n* **Text:** Writing stories, poems, articles, code, scripts, musical pieces, email, letters, etc.\\n* **Images:** Creating paintings, photographs, illustrations, logos, and other visuals.\\n* **Audio:** Composing music, generating sound effects, synthesizing speech.\\n* **Video:** Generating short video clips or even entire films.\\n* **3D Models:** Designing objects and environments.\\n\\n**How does it work?**\\n\\nGenerative AI models are typically trained on massive datasets of existing content. They learn the patterns and relationships within that data, and then use that knowledge to generate new content that follows a similar structure.\\n\\nSome popular generative AI models include:\\n\\n* **GPT-3 (text):** Can write incredibly human-like text in response to a wide range of prompts.\\n* **DALL-E 2 (images):** Creates realistic and imaginative images from textual descriptions.\\n* **Midjourney (images):** Another powerful text-to-image generator known for its artistic style.\\n* **Jukebox (audio):** Generates music in various styles.\\n\\n**Applications of Generative AI:**\\n\\nThe possibilities are vast and constantly expanding:\\n\\n* **Creative industries:** Writing assistance, generating art, composing music, creating video game assets.\\n* **Marketing and advertising:** Creating personalized content, generating ad copy, designing visuals.\\n* **Education:** Providing interactive learning experiences, generating custom exercises.\\n* **Healthcare:** Assisting in medical imaging analysis, generating synthetic patient data for research.\\n* **Research:** Exploring new scientific concepts, accelerating drug discovery.\\n\\n**Ethical considerations:**\\n\\nGenerative AI raises important ethical questions, such as:\\n\\n* **Bias and fairness:** AI models can inherit biases from the data they are trained on, leading to unfair or discriminatory outputs.\\n* **Misinformation and deepfakes:** The ability to generate realistic fake content can be misused for malicious purposes.\\n* **Copyright and intellectual property:** Who owns the copyright to AI-generated content?\\n\\nIt's crucial to develop and use generative AI responsibly, addressing these challenges and ensuring its benefits are widely shared.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 541, 'prompt_tokens': 15, 'total_tokens': 556, 'completion_time': 0.983636364, 'prompt_time': 0.001908598, 'queue_time': 0.242422793, 'total_time': 0.985544962}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--1ace670d-26cb-4a42-9ec3-e55c9703c352-0', usage_metadata={'input_tokens': 15, 'output_tokens': 541, 'total_tokens': 556})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model='gemma2-9b-it')\n",
    "model.invoke('What is Generative AI?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "030eebbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Lalu Mahato! \\n\\nIt's nice to meet you.  \\n\\nBeing a Technical Lead is a great role. What kind of technology do you specialize in?  What are some of the challenges and rewards you find in your work? \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 22, 'total_tokens': 77, 'completion_time': 0.1, 'prompt_time': 0.002120928, 'queue_time': 0.24280701200000002, 'total_time': 0.102120928}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--3a4c5b8f-2a37-46c7-a4f4-605625b36b92-0', usage_metadata={'input_tokens': 22, 'output_tokens': 55, 'total_tokens': 77})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content='Hi, My name is Lalu Mahato and Working as Technical Lead')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d811dc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"According to our conversation, your name is Lalu Mahato and you work as a Technical Lead. üòä \\n\\nIs there anything else you'd like to tell me about your work?  \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 100, 'total_tokens': 141, 'completion_time': 0.074545455, 'prompt_time': 0.006633203, 'queue_time': 0.24387587800000002, 'total_time': 0.081178658}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--1748a02c-6a98-4881-b65f-531fba48b414-0', usage_metadata={'input_tokens': 100, 'output_tokens': 41, 'total_tokens': 141})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi, My name is Lalu Mahato and Working as Technical Lead\"),\n",
    "        AIMessage(content=\"Hi Lalu Mahato! It's nice to meet you.\\n\\nThat's great that you're working as a Technical Lead. What kind of technologies do you specialize in? \\n\\nI'm always interested in learning more about different technical roles and the work people do.\\n\"),\n",
    "        HumanMessage(content='What is my name and what do I do?')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa1c1632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableLambda(_enter_history), kwargs={}, config={'run_name': 'load_history'}, config_factories=[])\n",
       "| RunnableBinding(bound=RunnableLambda(_call_runnable_sync), kwargs={}, config={'run_name': 'check_sync_or_async'}, config_factories=[]), kwargs={}, config={'run_name': 'RunnableWithMessageHistory'}, config_factories=[]), kwargs={}, config={}, config_factories=[], get_session_history=<function get_session_history at 0x0000020EDC744900>, history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "        \n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)\n",
    "with_message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfa564c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"That's great to know, Lalu Mahato! \\n\\nIt's nice to meet you. \\n\\nBeing a Technical Lead is a challenging and rewarding role. What technologies are you currently working with? Are you leading a team on any exciting projects? \\n\\nI'd love to hear more about your work!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 68, 'prompt_tokens': 22, 'total_tokens': 90, 'completion_time': 0.123636364, 'prompt_time': 0.002094348, 'queue_time': 0.245485853, 'total_time': 0.125730712}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--b698bf86-3ed5-4f00-bf26-c7ef77628895-0', usage_metadata={'input_tokens': 22, 'output_tokens': 68, 'total_tokens': 90})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config1 = {\"configurable\": {\"session_id\": \"John\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content='My name is Lalu Mahato and I am working as Technical Lead')],\n",
    "    config=config1\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30c03285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Lalu Mahato.  I remember that from our earlier conversation! üòä  \\n\\nIs there anything else I can help you with?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 103, 'total_tokens': 135, 'completion_time': 0.058181818, 'prompt_time': 0.009995991, 'queue_time': 0.242994368, 'total_time': 0.068177809}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--2186b7da-5f10-463e-a951-2148234279a0-0', usage_metadata={'input_tokens': 103, 'output_tokens': 32, 'total_tokens': 135})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content='What is my name?')],\n",
    "    config=config1\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "145205a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me your name, I'd be happy to use it!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 14, 'total_tokens': 55, 'completion_time': 0.074545455, 'prompt_time': 0.002023218, 'queue_time': 0.241894112, 'total_time': 0.076568673}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--ce886f15-9a89-49cf-b268-b41010dfb2a0-0', usage_metadata={'input_tokens': 14, 'output_tokens': 41, 'total_tokens': 55})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config2 = {\"configurable\": {\"session_id\": \"Max\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content='What is my name?')],\n",
    "    config=config2\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ae50265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='My name is Lalu Mahato and I am working as Technical Lead', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's great to know, Lalu Mahato! \\n\\nIt's nice to meet you. \\n\\nBeing a Technical Lead is a challenging and rewarding role. What technologies are you currently working with? Are you leading a team on any exciting projects? \\n\\nI'd love to hear more about your work!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 68, 'prompt_tokens': 22, 'total_tokens': 90, 'completion_time': 0.123636364, 'prompt_time': 0.002094348, 'queue_time': 0.245485853, 'total_time': 0.125730712}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--b698bf86-3ed5-4f00-bf26-c7ef77628895-0', usage_metadata={'input_tokens': 22, 'output_tokens': 68, 'total_tokens': 90}), HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Lalu Mahato.  I remember that from our earlier conversation! üòä  \\n\\nIs there anything else I can help you with?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 103, 'total_tokens': 135, 'completion_time': 0.058181818, 'prompt_time': 0.009995991, 'queue_time': 0.242994368, 'total_time': 0.068177809}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--2186b7da-5f10-463e-a951-2148234279a0-0', usage_metadata={'input_tokens': 103, 'output_tokens': 32, 'total_tokens': 135})])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_session_history('John')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89bf41d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You told me your name is Lalu Mahato!  \\n\\nDo you need me to remind you of something else? Perhaps you're testing me? üòâ  \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 148, 'total_tokens': 183, 'completion_time': 0.063636364, 'prompt_time': 0.005898365, 'queue_time': 0.24436928500000002, 'total_time': 0.069534729}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--2f295327-519d-458e-beed-78c8942b99e1-0', usage_metadata={'input_tokens': 148, 'output_tokens': 35, 'total_tokens': 183})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content='What is my name?')],\n",
    "    config=config1\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add1e82a",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91f7db4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['messages'], input_types={'messages': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000020EDD0839C0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant.Amnswer all the question to the best of your ability'), additional_kwargs={}), MessagesPlaceholder(variable_name='messages')])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000020EDD54D6A0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020EDD54E270>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.Amnswer all the question to the best of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d092ad32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Lalu Mahato, it's nice to meet you! \\n\\nI'm ready to answer your questions to the best of my ability. Just ask away! üòä  \\n\\nWhat can I help you with today?  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 32, 'total_tokens': 82, 'completion_time': 0.090909091, 'prompt_time': 0.002300508, 'queue_time': 0.245477574, 'total_time': 0.093209599}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--57e29956-5bd9-4aa2-ae9e-ae8e518efb04-0', usage_metadata={'input_tokens': 32, 'output_tokens': 50, 'total_tokens': 82})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({ \"messages\": [HumanMessage(content=\"My name is Lalu Mahato\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9832e676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableLambda(_enter_history), kwargs={}, config={'run_name': 'load_history'}, config_factories=[])\n",
       "| RunnableBinding(bound=RunnableLambda(_call_runnable_sync), kwargs={}, config={'run_name': 'check_sync_or_async'}, config_factories=[]), kwargs={}, config={'run_name': 'RunnableWithMessageHistory'}, config_factories=[]), kwargs={}, config={}, config_factories=[], get_session_history=<function get_session_history at 0x0000020EDC744900>, history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)\n",
    "with_message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e2baf7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Lalu Mahato! \\n\\nIt's nice to meet you. I'm here to help.\\n\\nWhat can I do for you? Ask away, I'll do my best to answer your questions! üòä  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 32, 'total_tokens': 82, 'completion_time': 0.090909091, 'prompt_time': 0.002311088, 'queue_time': 0.241963863, 'total_time': 0.093220179}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--55bc5228-f7ef-4af3-810a-0398701ff24a-0', usage_metadata={'input_tokens': 32, 'output_tokens': 50, 'total_tokens': 82})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config3 = {\"configurable\": {\"session_id\": \"Smith\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My name is Lalu Mahato\")],\n",
    "    config=config3\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fa9e412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Lalu Mahato.  üòÑ \\n\\nI remember that!  Is there anything else I can help you with?\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name?\")],\n",
    "    config=config3\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "482e8aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['language', 'messages'], input_types={'messages': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000020EDD0839C0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are a helpful assistant. Answer all questions to the best of your ability in {language}.'), additional_kwargs={}), MessagesPlaceholder(variable_name='messages')])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000020EDD54D6A0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020EDD54E270>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt template with custom language\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4f17bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á Lalu Mahato! \\n\\n‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•Ç‡§Å‡•§ ‡§Ü‡§™‡§ï‡•á ‡§ï‡•ã‡§à ‡§∏‡§µ‡§æ‡§≤ ‡§π‡•à‡§Ç ‡§§‡•ã ‡§¨‡•á‡§ù‡§ø‡§ù‡§ï ‡§™‡•Ç‡§õ‡§ø‡§è‡•§ ‡§Æ‡•à‡§Ç ‡§Ö‡§™‡§®‡•Ä ‡§™‡•Ç‡§∞‡•Ä ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞ ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•Ç‡§Ç‡§ó‡§æ‡•§  \\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({ \"messages\": [HumanMessage(content=\"My name is Lalu Mahato\")], \"language\": \"Hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67799c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  messages: RunnableBinding(bound=RunnableLambda(_enter_history), kwargs={}, config={'run_name': 'load_history'}, config_factories=[])\n",
       "}), kwargs={}, config={'run_name': 'insert_history'}, config_factories=[])\n",
       "| RunnableBinding(bound=RunnableLambda(_call_runnable_sync), kwargs={}, config={'run_name': 'check_sync_or_async'}, config_factories=[]), kwargs={}, config={'run_name': 'RunnableWithMessageHistory'}, config_factories=[]), kwargs={}, config={}, config_factories=[], get_session_history=<function get_session_history at 0x0000020EDC744900>, input_messages_key='messages', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")\n",
    "\n",
    "with_message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab094b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§ú‡•à‡§ï! üëã \\n\\n‡§Æ‡•Å‡§ù‡•á ‡§ñ‡•Å‡§∂‡•Ä ‡§π‡•à ‡§ï‡§ø ‡§§‡•Å‡§Æ ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•ã‡•§ ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ø‡§π‡§æ‡§Å ‡§π‡•Ç‡§Å‡•§ \\n\\n‡§Ü‡§™ ‡§Æ‡•á‡§∞‡•á ‡§∏‡•á ‡§ï‡•ç‡§Ø‡§æ ‡§™‡•Ç‡§õ‡§®‡§æ ‡§ö‡§æ‡§π‡•á‡§Ç‡§ó‡•á? üòä  \\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config4 = { \"configurable\": {\"session_id\": \"Jack\"} }\n",
    "\n",
    "repsonse = with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"Hi, I am Jack\")], \"language\":\"Hindi\"},\n",
    "    config=config4\n",
    ")\n",
    "\n",
    "repsonse.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "164bc4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ú‡•à‡§ï ‡§π‡•à‡•§  \\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repsonse = with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"What is my name?\")], \"language\":\"Hindi\"},\n",
    "    config=config4\n",
    ")\n",
    "\n",
    "repsonse.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00e95f8",
   "metadata": {},
   "source": [
    "### Managing the Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5330d6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=60,  # max_tokens=40\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e1efb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You said you like vanilla ice cream!  üç¶  üòÑ'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer) | prompt | model)\n",
    "\n",
    "response = chain.invoke(\n",
    "    { \n",
    "        \"messages\": messages + [HumanMessage(content=\"What ice cream do i like\")], \n",
    "        \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc469dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked \"what is 2 + 2\".  \\n\\n\\n\\nIs there anything else I can help you with?\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714d955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
